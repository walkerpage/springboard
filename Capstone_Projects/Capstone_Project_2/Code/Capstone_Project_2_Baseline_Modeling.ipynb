{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modeling #\n",
    "\n",
    "We will be using an unsupervised approach to construct a topic model using Latent Dirichlet Allocation (LDA).\n",
    "In the present notebook, we will complete the following steps:\n",
    "1. Preprocess the review data\n",
    "2. Construct vector representation of the preprocessed data\n",
    "3. Construct multiple initial LDA topic models using various sample sizes from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "import scipy\n",
    "import string\n",
    "\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data as DataFrame\n",
    "r_df = pd.read_csv('/Users/dwalkerpage/Documents/Data_Science/Springboard/Projects/springboard/Capstone_Projects/Capstone_Project_2/Code/cleaned_restaurants_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing ##\n",
    "\n",
    "**Steps:**\n",
    "* Remove punctuation\n",
    "* Convert words to lowercase\n",
    "* Remove stopwords\n",
    "* Lemmatize words\n",
    "    * As noted [here](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#sphx-glr-auto-examples-tutorials-run-lda-py), we use lemmatization and not stemming because lemmatization produces more readable words, and readability is a desirable property for the output in topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove punctuation from a string\n",
    "# From here: https://stackoverflow.com/questions/33047818/remove-punctuation-for-each-row-in-a-pandas-data-frame?noredirect=1&lq=1\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    '''Removes punctuation from a string'''\n",
    "    s = ''.join([i for i in s if i not in set(string.punctuation)])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove stop words, using NLTK's list of stop words\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    '''Removes stopwords from a string'''\n",
    "    s = ' '.join([word for word in s.split() if word not in stop_words])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_tag(nltk_tag):\n",
    "    '''Return relevant tag for WordNetLemmatizer'''\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:                    \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(s):\n",
    "    '''Lemmatizes words in a string'''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_words = word_tokenize(s)\n",
    "    tagged_words = nltk.pos_tag(tokenized_words)\n",
    "    wordnet_tagged_words = map(lambda x: (x[0], wordnet_tag(x[1])), tagged_words)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in wordnet_tagged_words:\n",
    "        if tag is None:\n",
    "            lemmatized_words.append(word)\n",
    "        else:\n",
    "            word = lemmatizer.lemmatize(word, tag)\n",
    "            lemmatized_words.append(word)\n",
    "    output = ' '.join(lemmatized_words)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of lemmatized sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"John 's big idea be n't all that bad .\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"John's big idea isn't all that bad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The striped bat be hang on their foot for best'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"The striped bats are hanging on their feet for best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare a review from the data set with its lemmatized counterpart:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We've always been there on a Sunday so we were hoping that Saturday dim sum would be less busy. No such luck. We were surprised that some of the dishes were cold because it was so packed; I could understand if it was empty and the carts weren't circulating but every table was full. It took a while to get drinks and other items (napkins). The dishes were not of the same quality as they had been on other visits, but they were acceptable.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_df.text.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatized Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We 've always be there on a Sunday so we be hop that Saturday dim sum would be less busy . No such luck . We be surprise that some of the dish be cold because it be so packed ; I could understand if it be empty and the cart be n't circulate but every table be full . It take a while to get drink and other item ( napkin ) . The dish be not of the same quality as they have be on other visit , but they be acceptable .\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(r_df.text.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    tqdm.pandas(desc='remove_punctuation')\n",
    "    df['text'] = df['text'].progress_apply(remove_punctuation)\n",
    "    tqdm.pandas(desc='remove_stopwords')\n",
    "    df['text'] = df['text'].progress_apply(remove_stopwords)\n",
    "    tqdm.pandas(desc='lemmatize')\n",
    "    df['text'] = df['text'].progress_apply(lemmatize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c18eb123d544999b48be25b2072c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='remove_punctuation', max=4167461, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a58560b67e478fabf57023164f1d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='remove_stopwords', max=4167461, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965ba31631f749f08d4e7c52389554ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='lemmatize', max=4167461, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement preprocessing functions\n",
    "proc_r_df = r_df.copy()\n",
    "proc_r_df = preprocess(r_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare some (arbitrarily chosen) original reviews with their respective processed versions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I would say Emeralds has worsen over a period of few months, honestly, the service here was never the greatest, but now it has gotten to the point where the employees barely know any english! \\n\\nAlso, they served us with dumplings that have gone bad, and were very sour in taste.  Meh, I wouldn't really care however, they charged us for it?!? So I have to pay to eat dumplings that may prove to be a health concern?  \\n\\n...At least the washrooms were clean... :P\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_df.text.loc[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'would say emerald worsen period month honestly service never great gotten point employee barely know english also serve us dumplings go bad sour taste meh wouldnt really care however charge us pay eat dumpling may prove health concern least washrooms clean p'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_r_df.text.loc[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After wanting to try Ravi soups out for months it did not disappoint. The curried lentil and apricot soup was the clear winner of everything we tried. We also tried the veggie wrap, pork wrap, and corn bisque.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_df.text.loc[579]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'want try ravi soups month disappoint curry lentil apricot soup clear winner everything try also try veggie wrap pork wrap corn bisque'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_r_df.text.loc[579]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "pd.isna(proc_r_df['text']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>...</th>\n",
       "      <th>stars_x</th>\n",
       "      <th>state</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023206</th>\n",
       "      <td>4000 W Flamingo Rd</td>\n",
       "      <td>{'BusinessParking': \"{'garage': True, 'street'...</td>\n",
       "      <td>UXFLCGw1yOCajlMONYQM1w</td>\n",
       "      <td>Buffets, Restaurants, Chinese</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{'Monday': '16:0-21:0', 'Tuesday': '16:0-21:0'...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.116635</td>\n",
       "      <td>-115.192905</td>\n",
       "      <td>Ports O' Call Buffet</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-02-14 14:10:24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ZY2DqyaJBzW3pUa9F0N5-Q</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yctYv9E1rK6AYTx1FHU3gw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    address  \\\n",
       "2023206  4000 W Flamingo Rd   \n",
       "\n",
       "                                                attributes  \\\n",
       "2023206  {'BusinessParking': \"{'garage': True, 'street'...   \n",
       "\n",
       "                    business_id                     categories       city  \\\n",
       "2023206  UXFLCGw1yOCajlMONYQM1w  Buffets, Restaurants, Chinese  Las Vegas   \n",
       "\n",
       "                                                     hours  is_open  \\\n",
       "2023206  {'Monday': '16:0-21:0', 'Tuesday': '16:0-21:0'...      1.0   \n",
       "\n",
       "          latitude   longitude                  name  ... stars_x  state  \\\n",
       "2023206  36.116635 -115.192905  Ports O' Call Buffet  ...     3.0     NV   \n",
       "\n",
       "         cool                 date  funny               review_id  stars_y  \\\n",
       "2023206   0.0  2015-02-14 14:10:24    0.0  ZY2DqyaJBzW3pUa9F0N5-Q      1.0   \n",
       "\n",
       "        text  useful                 user_id  \n",
       "2023206  NaN     1.0  yctYv9E1rK6AYTx1FHU3gw  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify null value\n",
    "proc_r_df[proc_r_df.text.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null value\n",
    "proc_r_df = proc_r_df.loc[~pd.isna(proc_r_df['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "pd.isna(proc_r_df['text']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to csv file\n",
    "proc_r_df.to_csv('processed_cleaned_restaurants_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vectorize Documents ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data as DataFrame\n",
    "proc_r_df = pd.read_csv('/Users/dwalkerpage/Documents/Data_Science/Springboard/Projects/springboard/Capstone_Projects/Capstone_Project_2/Code/processed_cleaned_restaurants_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416746.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_r_df.shape[0]/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preserve time and computational efficiency, we will work with various sample sizes from our dataset. Determining the extent to which our results extend to the larger dataset could be a fruitful direction for future developments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting sample\n",
    "def get_sample(df, sample_size=1000, random_state=7):\n",
    "    return df.sample(n=sample_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for tokenizing strings in an iterable\n",
    "def get_tokens(iterable):\n",
    "    return [string.split() for string in tqdm(iterable, desc='get_tokens')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get feature dictionary where the key is the word and value is the number of times that word occurs in the entire corpus of documents\n",
    "def get_feature_dict(word_tokens):\n",
    "    return gensim.corpora.Dictionary(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct Bag of Words representation of text\n",
    "def get_bow(feature_dict, word_tokens):\n",
    "    return [feature_dict.doc2bow(token) for token in tqdm(word_tokens, desc='get_bow')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get feature_id to feature mapping (needed to inspect topics later)\n",
    "def get_id2token(feature_dict):\n",
    "    temp = feature_dict[0] # initialize feature_dict in memory\n",
    "    return feature_dict.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sample, vectorize text data, and split vector representation into train/test sets\n",
    "def lda_input(df, sample_size=1000, test_size=0.3, random_state=7):\n",
    "    \n",
    "    df_sample = get_sample(df, sample_size=sample_size, random_state=random_state)\n",
    "    \n",
    "    tokens = get_tokens(df_sample['text'])\n",
    "    \n",
    "    feature_dict = get_feature_dict(tokens)\n",
    "    \n",
    "    bow = get_bow(feature_dict, tokens)\n",
    "    \n",
    "    id2token_mapping = get_id2token(feature_dict)\n",
    "    \n",
    "    xtrain, xtest = train_test_split(bow, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return df_sample, tokens, feature_dict, bow, id2token_mapping, xtrain, xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f150a70c14ad41ac8c7c11ec4abfdd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='get_tokens', max=1000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516f2cc4dc184b828701d6742a381278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='get_bow', max=1000, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE SIZE 1_000\n",
    "\n",
    "(proc_r_df_sample1_000,\n",
    " review_tokens1_000,\n",
    " feature_dict1_000,\n",
    " bow1_000,\n",
    " id2word1_000,\n",
    " xtrain1_000,\n",
    " xtest1_000) = lda_input(proc_r_df,\n",
    "                         sample_size=1_000,\n",
    "                         test_size=0.3,\n",
    "                         random_state=7\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366ce73f6a674a30bbb65ed7092f7378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='get_tokens', max=10000, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664b70bed6d54143b4188ad59b58aa46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='get_bow', max=10000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE SIZE 10_000\n",
    "\n",
    "(proc_r_df_sample10_000,\n",
    " review_tokens10_000,\n",
    " feature_dict10_000,\n",
    " bow10_000,\n",
    " id2word10_000,\n",
    " xtrain10_000,\n",
    " xtest10_000) = lda_input(proc_r_df,\n",
    "                         sample_size=10_000,\n",
    "                         test_size=0.3,\n",
    "                         random_state=7\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='get_tokens', max=100000, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='get_bow', max=100000, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE SIZE 100_000\n",
    "\n",
    "(proc_r_df_sample100_000,\n",
    " review_tokens100_000,\n",
    " feature_dict100_000,\n",
    " bow100_000,\n",
    " id2word100_000,\n",
    " xtrain100_000,\n",
    " xtest100_000) = lda_input(proc_r_df,\n",
    "                         sample_size=100_000,\n",
    "                         test_size=0.3,\n",
    "                         random_state=7\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b2423f156a40b28b56756533254af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='get_tokens', max=500000, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444266220ddf46169423b2650f220cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='get_bow', max=500000, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE SIZE 500_000\n",
    "\n",
    "(proc_r_df_sample500_000,\n",
    " review_tokens500_000,\n",
    " feature_dict500_000,\n",
    " bow500_000,\n",
    " id2word500_000,\n",
    " xtrain500_000,\n",
    " xtest500_000) = lda_input(proc_r_df,\n",
    "                         sample_size=500_000,\n",
    "                         test_size=0.3,\n",
    "                         random_state=7\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LDA Topic Models ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LdaMulticore in module gensim.models.ldamulticore:\n",
      "\n",
      "class LdaMulticore(gensim.models.ldamodel.LdaModel)\n",
      " |  LdaMulticore(corpus=None, num_topics=100, id2word=None, workers=None, chunksize=2000, passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, random_state=None, minimum_probability=0.01, minimum_phi_value=0.01, per_word_topics=False, dtype=<class 'numpy.float32'>)\n",
      " |  \n",
      " |  An optimized implementation of the LDA algorithm, able to harness the power of multicore CPUs.\n",
      " |  Follows the similar API as the parent class :class:`~gensim.models.ldamodel.LdaModel`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaMulticore\n",
      " |      gensim.models.ldamodel.LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      gensim.models.basemodel.BaseTopicModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, workers=None, chunksize=2000, passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, random_state=None, minimum_probability=0.01, minimum_phi_value=0.01, per_word_topics=False, dtype=<class 'numpy.float32'>)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\n",
      " |          If not given, the model is left untrained (presumably because you want to call\n",
      " |          :meth:`~gensim.models.ldamodel.LdaModel.update` manually).\n",
      " |      num_topics : int, optional\n",
      " |          The number of requested latent topics to be extracted from the training corpus.\n",
      " |      id2word : {dict of (int, str),  :class:`gensim.corpora.dictionary.Dictionary`}\n",
      " |          Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\n",
      " |          debugging and topic printing.\n",
      " |      workers : int, optional\n",
      " |          Number of workers processes to be used for parallelization. If None all available cores\n",
      " |          (as estimated by `workers=cpu_count()-1` will be used. **Note** however that for\n",
      " |          hyper-threaded CPUs, this estimation returns a too high number -- set `workers`\n",
      " |          directly to the number of your **real** cores (not hyperthreads) minus one, for optimal performance.\n",
      " |      chunksize :  int, optional\n",
      " |          Number of documents to be used in each training chunk.\n",
      " |      passes : int, optional\n",
      " |          Number of passes through the corpus during training.\n",
      " |      alpha : {np.ndarray, str}, optional\n",
      " |          Can be set to an 1D array of length equal to the number of expected topics that expresses\n",
      " |          our a-priori belief for the each topics' probability.\n",
      " |          Alternatively default prior selecting strategies can be employed by supplying a string:\n",
      " |      \n",
      " |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n",
      " |      eta : {float, np.array, str}, optional\n",
      " |          A-priori belief on word probability, this can be:\n",
      " |      \n",
      " |              * scalar for a symmetric prior over topic/word probability,\n",
      " |              * vector of length num_words to denote an asymmetric user defined probability for each word,\n",
      " |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
      " |              * the string 'auto' to learn the asymmetric prior from the data.\n",
      " |      decay : float, optional\n",
      " |          A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n",
      " |          when each new document is examined. Corresponds to Kappa from\n",
      " |          `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      offset : float, optional\n",
      " |          Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n",
      " |          Corresponds to Tau_0 from `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      eval_every : int, optional\n",
      " |          Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
      " |      iterations : int, optional\n",
      " |          Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
      " |      gamma_threshold : float, optional\n",
      " |          Minimum change in the value of the gamma parameters to continue iterating.\n",
      " |      minimum_probability : float, optional\n",
      " |          Topics with a probability lower than this threshold will be filtered out.\n",
      " |      random_state : {np.random.RandomState, int}, optional\n",
      " |          Either a randomState object or a seed to generate one. Useful for reproducibility.\n",
      " |      minimum_phi_value : float, optional\n",
      " |          if `per_word_topics` is True, this represents a lower bound on the term probabilities.\n",
      " |      per_word_topics : bool\n",
      " |          If True, the model also computes a list of topics, sorted in descending order of most likely topics for\n",
      " |          each word, along with their phi values multiplied by the feature length (i.e. word count).\n",
      " |      dtype : {numpy.float16, numpy.float32, numpy.float64}, optional\n",
      " |          Data-type to use during calculations inside model. All inputs are also converted.\n",
      " |  \n",
      " |  update(self, corpus, chunks_as_numpy=False)\n",
      " |      Train the model with new documents, by EM-iterating over `corpus` until the topics converge\n",
      " |      (or until the maximum number of allowed iterations is reached).\n",
      " |      \n",
      " |      Train the model with new documents, by EM-iterating over the corpus until the topics converge, or until\n",
      " |      the maximum number of allowed iterations is reached. `corpus` must be an iterable. The E step is distributed\n",
      " |      into the several processes.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This update also supports updating an already trained model (`self`)\n",
      " |      with new documents from `corpus`; the two models are then merged in\n",
      " |      proportion to the number of old vs. new documents. This feature is still\n",
      " |      experimental for non-stationary input streams.\n",
      " |      \n",
      " |      For stationary input (no topic drift in new documents), on the other hand,\n",
      " |      this equals the online update of Hoffman et al. and is guaranteed to\n",
      " |      converge for any `decay` in (0.5, 1.0>.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`) used to update the\n",
      " |          model.\n",
      " |      chunks_as_numpy : bool\n",
      " |          Whether each chunk passed to the inference step should be a np.ndarray or not. Numpy can in some settings\n",
      " |          turn the term IDs into floats, these will be converted back into integers in inference, which incurs a\n",
      " |          performance hit. For distributed computing it may be desirable to keep the chunks as `numpy.ndarray`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.ldamodel.LdaModel:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=None)\n",
      " |      Get the topic distribution for the given document.\n",
      " |      \n",
      " |      Wraps :meth:`~gensim.models.ldamodel.LdaModel.get_document_topics` to support an operator style call.\n",
      " |      Uses the model's current state (set using constructor arguments) to fill in the additional arguments of the\n",
      " |      wrapper method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ---------\n",
      " |      bow : list of (int, float)\n",
      " |          The document in BOW format.\n",
      " |      eps : float, optional\n",
      " |          Topics with an assigned probability lower than this threshold will be discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Topic distribution for the given document. Each topic is represented as a pair of its ID and the probability\n",
      " |          assigned to it.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Get a string representation of the current object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the most important model parameters.\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from the corpus as E_q[log p(corpus)] - E_q[log q(corpus)].\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`) used to estimate the\n",
      " |          variational bounds.\n",
      " |      gamma : numpy.ndarray, optional\n",
      " |          Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\n",
      " |      subsample_ratio : float, optional\n",
      " |          Percentage of the whole corpus represented by the passed `corpus` argument (in case this was a sample).\n",
      " |          Set to 1.0 if the whole corpus was passed.This is used as a multiplicative factor to scale the likelihood\n",
      " |          appropriately.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The variational bound score calculated for each document.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear the model's state to free some memory. Used in the distributed implementation.\n",
      " |  \n",
      " |  diff(self, other, distance='kullback_leibler', num_words=100, n_ann_terms=10, diagonal=False, annotation=True, normed=True)\n",
      " |      Calculate the difference in topic distributions between two models: `self` and `other`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`~gensim.models.ldamodel.LdaModel`\n",
      " |          The model which will be compared against the current object.\n",
      " |      distance : {'kullback_leibler', 'hellinger', 'jaccard', 'jensen_shannon'}\n",
      " |          The distance metric to calculate the difference with.\n",
      " |      num_words : int, optional\n",
      " |          The number of most relevant words used if `distance == 'jaccard'`. Also used for annotating topics.\n",
      " |      n_ann_terms : int, optional\n",
      " |          Max number of words in intersection/symmetric difference between topics. Used for annotation.\n",
      " |      diagonal : bool, optional\n",
      " |          Whether we need the difference between identical topics (the diagonal of the difference matrix).\n",
      " |      annotation : bool, optional\n",
      " |          Whether the intersection or difference of words between two topics should be returned.\n",
      " |      normed : bool, optional\n",
      " |          Whether the matrix should be normalized or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          A difference matrix. Each element corresponds to the difference between the two topics,\n",
      " |          shape (`self.num_topics`, `other.num_topics`)\n",
      " |      numpy.ndarray, optional\n",
      " |          Annotation matrix where for each pair we include the word from the intersection of the two topics,\n",
      " |          and the word from the symmetric difference of the two topics. Only included if `annotation == True`.\n",
      " |          Shape (`self.num_topics`, `other_model.num_topics`, 2).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Get the differences between each pair of topics inferred by two models\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models.ldamulticore import LdaMulticore\n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> m1 = LdaMulticore.load(datapath(\"lda_3_0_1_model\"))\n",
      " |          >>> m2 = LdaMulticore.load(datapath(\"ldamodel_python_3_5\"))\n",
      " |          >>> mdiff, annotation = m1.diff(m2)\n",
      " |          >>> topic_diff = mdiff  # get matrix with difference for each topic pair from `m1` and `m2`\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected sufficient statistics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      state : :class:`~gensim.models.ldamodel.LdaState`, optional\n",
      " |          The state to be updated with the newly accumulated sufficient statistics. If none, the models\n",
      " |          `self.state` is updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Gamma parameters controlling the topic weights, shape (`len(chunk)`, `self.num_topics`).\n",
      " |  \n",
      " |  do_mstep(self, rho, other, extra_pass=False)\n",
      " |      Maximization step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      other : :class:`~gensim.models.ldamodel.LdaModel`\n",
      " |          The model whose sufficient statistics will be used to update the topics.\n",
      " |      extra_pass : bool, optional\n",
      " |          Whether this step required an additional pass over the corpus.\n",
      " |  \n",
      " |  get_document_topics(self, bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
      " |      Get the topic distribution for the given document.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bow : corpus : list of (int, float)\n",
      " |          The document in BOW format.\n",
      " |      minimum_probability : float\n",
      " |          Topics with an assigned probability lower than this threshold will be discarded.\n",
      " |      minimum_phi_value : float\n",
      " |          If `per_word_topics` is True, this represents a lower bound on the term probabilities that are included.\n",
      " |           If set to None, a value of 1e-8 is used to prevent 0s.\n",
      " |      per_word_topics : bool\n",
      " |          If True, this function will also return two extra lists as explained in the \"Returns\" section.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Topic distribution for the whole document. Each element in the list is a pair of a topic's id, and\n",
      " |          the probability that was assigned to it.\n",
      " |      list of (int, list of (int, float), optional\n",
      " |          Most probable topics per word. Each element in the list is a pair of a word's id, and a list of\n",
      " |          topics sorted by their relevance to this word. Only returned if `per_word_topics` was set to True.\n",
      " |      list of (int, list of float), optional\n",
      " |          Phi relevance values, multiplied by the feature length, for each word-topic combination.\n",
      " |          Each element in the list is a pair of a word's id and a list of the phi values between this word and\n",
      " |          each topic. Only returned if `per_word_topics` was set to True.\n",
      " |  \n",
      " |  get_term_topics(self, word_id, minimum_probability=None)\n",
      " |      Get the most relevant topics to the given word.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_id : int\n",
      " |          The word for which the topic distribution will be computed.\n",
      " |      minimum_probability : float, optional\n",
      " |          Topics with an assigned probability below this threshold will be discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          The relevant topics represented as pairs of their ID and their assigned probability, sorted\n",
      " |          by relevance to the given word.\n",
      " |  \n",
      " |  get_topic_terms(self, topicid, topn=10)\n",
      " |      Get the representation for a single topic. Words the integer IDs, in constrast to\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          The ID of the topic to be returned\n",
      " |      topn : int, optional\n",
      " |          Number of the most significant words that are associated with the topic.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Word ID - probability pairs for the most relevant words generated by the topic.\n",
      " |  \n",
      " |  get_topics(self)\n",
      " |      Get the term-topic matrix learned during inference.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The probability for each word in each topic, shape (`num_topics`, `vocabulary_size`).\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters controlling the topic weights)\n",
      " |      for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model The whole input chunk of document is assumed to fit in RAM;\n",
      " |      chunking of a large corpus must be done earlier in the pipeline. Avoids computing the `phi` variational\n",
      " |      parameter directly using the optimization presented in\n",
      " |      `Lee, Seung: Algorithms for non-negative matrix factorization\"\n",
      " |      <https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      collect_sstats : bool, optional\n",
      " |          If set to True, also collect (and return) sufficient statistics needed to update the model's topic-word\n",
      " |          distributions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      (numpy.ndarray, {numpy.ndarray, None})\n",
      " |          The first element is always returned and it corresponds to the states gamma matrix. The second element is\n",
      " |          only returned if `collect_sstats` == True and corresponds to the sufficient statistics for the M step.\n",
      " |  \n",
      " |  init_dir_prior(self, prior, name)\n",
      " |      Initialize priors for the Dirichlet distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prior : {str, list of float, numpy.ndarray of float, float}\n",
      " |          A-priori belief on word probability. If `name` == 'eta' then the prior can be:\n",
      " |      \n",
      " |              * scalar for a symmetric prior over topic/word probability,\n",
      " |              * vector of length num_words to denote an asymmetric user defined probability for each word,\n",
      " |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
      " |              * the string 'auto' to learn the asymmetric prior from the data.\n",
      " |      \n",
      " |          If `name` == 'alpha', then the prior can be:\n",
      " |      \n",
      " |              * an 1D array of length equal to the number of expected topics,\n",
      " |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n",
      " |              * 'auto': Learns an asymmetric prior from the corpus.\n",
      " |      name : {'alpha', 'eta'}\n",
      " |          Whether the `prior` is parameterized by the alpha vector (1 parameter per topic)\n",
      " |          or by the eta (1 parameter per unique term in the vocabulary).\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using a chunk of documents as evaluation corpus.\n",
      " |      \n",
      " |      Also output the calculated statistics, including the perplexity=2^(-bound), to log at INFO level.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      total_docs : int, optional\n",
      " |          Number of docs used for evaluation of the perplexity.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The variational bound score calculated for each word.\n",
      " |  \n",
      " |  save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you intend to use models across Python 2/3 versions there are a few things to\n",
      " |      keep in mind:\n",
      " |      \n",
      " |        1. The pickled Python dictionaries will not work across Python versions\n",
      " |        2. The `save` method does not automatically save all numpy arrays separately, only\n",
      " |           those ones that exceed `sep_limit` set in :meth:`~gensim.utils.SaveLoad.save`. The main\n",
      " |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      " |      \n",
      " |      Please refer to the `wiki recipes section\n",
      " |      <https://github.com/RaRe-Technologies/gensim/wiki/\n",
      " |      Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2>`_\n",
      " |      for an example on how to work around these issues.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.load`\n",
      " |          Load model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the system file where the model will be persisted.\n",
      " |      ignore : tuple of str, optional\n",
      " |          The named attributes in the tuple will be left out of the pickled model. The reason why\n",
      " |          the internal `state` is ignored by default is that it uses its own serialisation rather than the one\n",
      " |          provided by this method.\n",
      " |      separately : {list of str, None}, optional\n",
      " |          If None -  automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This avoids pickle memory errors and allows `mmap`'ing large arrays\n",
      " |          back on load efficiently. If list of str - this attributes will be stored in separate files,\n",
      " |          the automatic check is not performed in this case.\n",
      " |      *args\n",
      " |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |      **kwargs\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Get the representation for a single topic. Words here are the actual strings, in constrast to\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.get_topic_terms` that represents words by their vocabulary ID.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          The ID of the topic to be returned\n",
      " |      topn : int, optional\n",
      " |          Number of the most significant words that are associated with the topic.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          Word - probability pairs for the most relevant words generated by the topic.\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      Get a representation for selected topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          Number of topics to be returned. Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |          The returned topics subset of all topics is therefore arbitrary and may change between two LDA\n",
      " |          training runs.\n",
      " |      num_words : int, optional\n",
      " |          Number of words to be presented for each topic. These will be the most relevant words (assigned the highest\n",
      " |          probability for each topic).\n",
      " |      log : bool, optional\n",
      " |          Whether the output is also logged, besides being returned.\n",
      " |      formatted : bool, optional\n",
      " |          Whether the topic representations should be formatted as strings. If False, they are returned as\n",
      " |          2 tuples of (word, probability).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of {str, tuple of (str, float)}\n",
      " |          a list of topics, each represented either as a string (when `formatted` == True) or word-probability\n",
      " |          pairs.\n",
      " |  \n",
      " |  sync_state(self, current_Elogbeta=None)\n",
      " |      Propagate the states topic probabilities to the inner object's attribute.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      current_Elogbeta: numpy.ndarray\n",
      " |          Posterior probabilities for each topic, optional.\n",
      " |          If omitted, it will get Elogbeta from state.\n",
      " |  \n",
      " |  top_topics(self, corpus=None, texts=None, dictionary=None, window_size=None, coherence='u_mass', topn=20, processes=-1)\n",
      " |      Get the topics with the highest coherence score the coherence for each topic.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of list of (int, float), optional\n",
      " |          Corpus in BoW format.\n",
      " |      texts : list of list of str, optional\n",
      " |          Tokenized texts, needed for coherence models that use sliding window based (i.e. coherence=`c_something`)\n",
      " |          probability estimator .\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
      " |          Gensim dictionary mapping of id word to create corpus.\n",
      " |          If `model.id2word` is present, this is not needed. If both are provided, passed `dictionary` will be used.\n",
      " |      window_size : int, optional\n",
      " |          Is the size of the window to be used for coherence measures using boolean sliding window as their\n",
      " |          probability estimator. For 'u_mass' this doesn't matter.\n",
      " |          If None - the default window sizes are used which are: 'c_v' - 110, 'c_uci' - 10, 'c_npmi' - 10.\n",
      " |      coherence : {'u_mass', 'c_v', 'c_uci', 'c_npmi'}, optional\n",
      " |          Coherence measure to be used.\n",
      " |          Fastest method - 'u_mass', 'c_uci' also known as `c_pmi`.\n",
      " |          For 'u_mass' corpus should be provided, if texts is provided, it will be converted to corpus\n",
      " |          using the dictionary. For 'c_v', 'c_uci' and 'c_npmi' `texts` should be provided (`corpus` isn't needed)\n",
      " |      topn : int, optional\n",
      " |          Integer corresponding to the number of top words to be extracted from each topic.\n",
      " |      processes : int, optional\n",
      " |          Number of processes to use for probability estimation phase, any value less than 1 will be interpreted as\n",
      " |          num_cpus - 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (list of (int, str), float)\n",
      " |          Each element in the list is a pair of a topic representation and its coherence score. Topic representations\n",
      " |          are distributions of words, represented as a list of pairs of word IDs and their probabilities.\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document topic weights.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      gammat : numpy.ndarray\n",
      " |          Previous topic weight parameters.\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Sequence of alpha parameters.\n",
      " |  \n",
      " |  update_eta(self, lambdat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-topic word weights.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lambdat : numpy.ndarray\n",
      " |          Previous lambda parameters.\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The updated eta parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.ldamodel.LdaModel:\n",
      " |  \n",
      " |  load(fname, *args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`gensim.models.ldamodel.LdaModel` from file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file where the model is stored.\n",
      " |      *args\n",
      " |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n",
      " |      **kwargs\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> fname = datapath(\"lda_3_0_1_model\")\n",
      " |          >>> lda = LdaModel.load(fname, mmap='r')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n",
      " |  \n",
      " |  print_topic(self, topicno, topn=10)\n",
      " |      Get a single topic as a formatted string.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicno : int\n",
      " |          Topic id.\n",
      " |      topn : int\n",
      " |          Number of words from topic that will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          String representation of topic, like '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + ... '.\n",
      " |  \n",
      " |  print_topics(self, num_topics=20, num_words=10)\n",
      " |      Get the most significant topics (alias for `show_topics()` method).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          The number of topics to be selected, if -1 - all topics will be in result (ordered by significance).\n",
      " |      num_words : int, optional\n",
      " |          The number of words to be included per topics (ordered by significance).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, list of (str, float))\n",
      " |          Sequence with (topic_id, [(word, value), ... ]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LdaMulticore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model with Sample Size 1,000 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 261 ms, sys: 126 ms, total: 387 ms\n",
      "Wall time: 796 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lda1_000 = LdaMulticore(corpus=bow1_000,\n",
    "                        id2word=id2word1_000,\n",
    "                        num_topics=5,\n",
    "                        workers=5,\n",
    "                        chunksize=2000,\n",
    "                        passes=1,\n",
    "                        batch=False,\n",
    "                        alpha='symmetric',\n",
    "                        eta=None,\n",
    "                        decay=0.5,\n",
    "                        offset=1.0,\n",
    "                        eval_every=None,\n",
    "                        iterations=50,\n",
    "                        gamma_threshold=0.001,\n",
    "                        minimum_probability=0.01,\n",
    "                        random_state=7\n",
    "#                         minimum_phi_value=0.01,\n",
    "#                         per_word_topics=False,\n",
    "#                         dtype=<class 'numpy.float32'>,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('food', 0.012662879),\n",
       "   ('place', 0.011021428),\n",
       "   ('great', 0.009415503),\n",
       "   ('go', 0.0091834515),\n",
       "   ('good', 0.008783108),\n",
       "   ('order', 0.007936001),\n",
       "   ('well', 0.007903132),\n",
       "   ('time', 0.0076403674),\n",
       "   ('like', 0.0065482096),\n",
       "   ('make', 0.0065188073)]),\n",
       " (1,\n",
       "  [('food', 0.01756897),\n",
       "   ('get', 0.013004785),\n",
       "   ('good', 0.012911806),\n",
       "   ('go', 0.010328119),\n",
       "   ('place', 0.009638864),\n",
       "   ('service', 0.008195439),\n",
       "   ('great', 0.007493089),\n",
       "   ('like', 0.0061502405),\n",
       "   ('come', 0.0056369337),\n",
       "   ('try', 0.004802929)]),\n",
       " (2,\n",
       "  [('good', 0.015943957),\n",
       "   ('place', 0.012444894),\n",
       "   ('food', 0.010193529),\n",
       "   ('go', 0.009639071),\n",
       "   ('get', 0.00842312),\n",
       "   ('order', 0.0074743675),\n",
       "   ('great', 0.007362948),\n",
       "   ('time', 0.006679303),\n",
       "   ('one', 0.006175509),\n",
       "   ('also', 0.0061264876)]),\n",
       " (3,\n",
       "  [('good', 0.011204761),\n",
       "   ('get', 0.011032292),\n",
       "   ('food', 0.008898975),\n",
       "   ('come', 0.008082554),\n",
       "   ('place', 0.0079306085),\n",
       "   ('time', 0.0078652715),\n",
       "   ('go', 0.007531253),\n",
       "   ('service', 0.007052512),\n",
       "   ('one', 0.006443688),\n",
       "   ('order', 0.005591475)]),\n",
       " (4,\n",
       "  [('food', 0.0133226095),\n",
       "   ('order', 0.010665506),\n",
       "   ('come', 0.009389623),\n",
       "   ('service', 0.008450539),\n",
       "   ('go', 0.007129157),\n",
       "   ('make', 0.006922094),\n",
       "   ('us', 0.0068933587),\n",
       "   ('get', 0.0067910873),\n",
       "   ('good', 0.0066748764),\n",
       "   ('back', 0.006551741)])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda1_000.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.011204761, 'good'),\n",
       "   (0.011032292, 'get'),\n",
       "   (0.008898975, 'food'),\n",
       "   (0.008082554, 'come'),\n",
       "   (0.0079306085, 'place'),\n",
       "   (0.0078652715, 'time'),\n",
       "   (0.007531253, 'go'),\n",
       "   (0.007052512, 'service'),\n",
       "   (0.006443688, 'one'),\n",
       "   (0.005591475, 'order'),\n",
       "   (0.00520837, 'try'),\n",
       "   (0.0050734486, 'great'),\n",
       "   (0.0049117235, 'like'),\n",
       "   (0.0045495923, 'would'),\n",
       "   (0.004541352, 'meal'),\n",
       "   (0.004498734, 'back'),\n",
       "   (0.0042737816, 'say'),\n",
       "   (0.0042591286, 'restaurant'),\n",
       "   (0.0041963435, 'love'),\n",
       "   (0.003724203, 'well')],\n",
       "  -1.3023760981077013),\n",
       " ([(0.012662879, 'food'),\n",
       "   (0.011021428, 'place'),\n",
       "   (0.009415503, 'great'),\n",
       "   (0.0091834515, 'go'),\n",
       "   (0.008783108, 'good'),\n",
       "   (0.007936001, 'order'),\n",
       "   (0.007903132, 'well'),\n",
       "   (0.0076403674, 'time'),\n",
       "   (0.0065482096, 'like'),\n",
       "   (0.0065188073, 'make'),\n",
       "   (0.005822111, 'get'),\n",
       "   (0.0056278403, 'try'),\n",
       "   (0.0055285655, 'restaurant'),\n",
       "   (0.005366005, 'come'),\n",
       "   (0.0052771415, 'one'),\n",
       "   (0.005037206, 'back'),\n",
       "   (0.0049580415, 'would'),\n",
       "   (0.004695662, 'best'),\n",
       "   (0.004641422, 'also'),\n",
       "   (0.0042750807, 'chicken')],\n",
       "  -1.3027790429372612),\n",
       " ([(0.0133226095, 'food'),\n",
       "   (0.010665506, 'order'),\n",
       "   (0.009389623, 'come'),\n",
       "   (0.008450539, 'service'),\n",
       "   (0.007129157, 'go'),\n",
       "   (0.006922094, 'make'),\n",
       "   (0.0068933587, 'us'),\n",
       "   (0.0067910873, 'get'),\n",
       "   (0.0066748764, 'good'),\n",
       "   (0.006551741, 'back'),\n",
       "   (0.0063267956, 'time'),\n",
       "   (0.0061823623, 'place'),\n",
       "   (0.0058368207, 'wait'),\n",
       "   (0.00522443, 'great'),\n",
       "   (0.004722989, 'like'),\n",
       "   (0.0043554325, 'well'),\n",
       "   (0.0040812064, 'restaurant'),\n",
       "   (0.0037900247, 'nice'),\n",
       "   (0.0036595375, 'love'),\n",
       "   (0.0034909048, 'table')],\n",
       "  -1.3191503459696925),\n",
       " ([(0.015943957, 'good'),\n",
       "   (0.012444894, 'place'),\n",
       "   (0.010193529, 'food'),\n",
       "   (0.009639071, 'go'),\n",
       "   (0.00842312, 'get'),\n",
       "   (0.0074743675, 'order'),\n",
       "   (0.007362948, 'great'),\n",
       "   (0.006679303, 'time'),\n",
       "   (0.006175509, 'one'),\n",
       "   (0.0061264876, 'also'),\n",
       "   (0.005832799, 'come'),\n",
       "   (0.0056966366, 'like'),\n",
       "   (0.005588703, 'chicken'),\n",
       "   (0.0049370504, 'really'),\n",
       "   (0.004890621, 'service'),\n",
       "   (0.004161736, 'make'),\n",
       "   (0.004010489, 'love'),\n",
       "   (0.0038506447, 'would'),\n",
       "   (0.0038477366, 'best'),\n",
       "   (0.0037456488, 'take')],\n",
       "  -1.3529367963426224),\n",
       " ([(0.01756897, 'food'),\n",
       "   (0.013004785, 'get'),\n",
       "   (0.012911806, 'good'),\n",
       "   (0.010328119, 'go'),\n",
       "   (0.009638864, 'place'),\n",
       "   (0.008195439, 'service'),\n",
       "   (0.007493089, 'great'),\n",
       "   (0.0061502405, 'like'),\n",
       "   (0.0056369337, 'come'),\n",
       "   (0.004802929, 'try'),\n",
       "   (0.0046517854, 'would'),\n",
       "   (0.0046381033, 'back'),\n",
       "   (0.0045417985, 'order'),\n",
       "   (0.0043977904, 'pizza'),\n",
       "   (0.00424192, 'one'),\n",
       "   (0.0041934736, 'restaurant'),\n",
       "   (0.0040491456, 'give'),\n",
       "   (0.0036996761, 'want'),\n",
       "   (0.0036115232, 'fry'),\n",
       "   (0.0035920313, 'im')],\n",
       "  -1.4694816009252971)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.top_topics\n",
    "lda1_000.top_topics(bow1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.68 s, sys: 400 ms, total: 6.08 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vis_data1_000 = pyLDAvis.gensim.prepare(lda1_000, bow1_000, feature_dict1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el45091105317080247020025307\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el45091105317080247020025307_data = {\"mdsDat\": {\"x\": [0.002111554210032669, -0.013223065720243933, -0.0003852393294649939, -0.014545282355675355, 0.026042033195351616], \"y\": [-0.013181938916466126, 0.010235825531490623, 0.019699109429317553, -0.014956749784845218, -0.0017962462594968468], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [27.063215255737305, 21.68592643737793, 18.866207122802734, 17.827550888061523, 14.557097434997559]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [338.0, 700.0, 622.0, 186.0, 502.0, 400.0, 167.0, 376.0, 271.0, 361.0, 207.0, 79.0, 192.0, 155.0, 115.0, 269.0, 543.0, 151.0, 105.0, 150.0, 69.0, 184.0, 149.0, 162.0, 141.0, 497.0, 76.0, 64.0, 134.0, 265.0, 3.1968350410461426, 2.9754486083984375, 2.1993489265441895, 2.1988909244537354, 2.1983680725097656, 2.114518404006958, 2.1090517044067383, 2.0347583293914795, 3.2996044158935547, 2.6349260807037354, 1.922061562538147, 1.9112423658370972, 11.958518028259277, 1.8479098081588745, 3.0286903381347656, 1.8280853033065796, 3.6517653465270996, 2.391458511352539, 1.2001421451568604, 1.1998175382614136, 1.1997588872909546, 1.1996791362762451, 1.1996233463287354, 1.1996206045150757, 1.19961678981781, 1.1994404792785645, 1.1996248960494995, 1.199562668800354, 1.1994374990463257, 1.199339509010315, 1.7891870737075806, 4.711242198944092, 1.7928985357284546, 4.0638227462768555, 1.7895734310150146, 14.321924209594727, 1.7576134204864502, 3.9122440814971924, 120.48738098144531, 2.2703890800476074, 4.749140739440918, 3.2243659496307373, 71.58782196044922, 9.899292945861816, 7.350013256072998, 143.5442657470703, 99.38262939453125, 8.097872734069824, 24.463483810424805, 35.489532470703125, 168.02743530273438, 116.48139953613281, 84.28587341308594, 39.60594940185547, 28.43487548828125, 85.79936981201172, 70.76091003417969, 193.05221557617188, 120.98848724365234, 99.83087921142578, 65.17584228515625, 140.00653076171875, 55.92660140991211, 55.76496887207031, 51.150596618652344, 47.035987854003906, 37.69158935546875, 75.58793640136719, 24.62447738647461, 76.79484558105469, 80.4527816772461, 61.75288391113281, 133.903076171875, 51.79676055908203, 34.35371780395508, 81.80755615234375, 43.24064254760742, 51.69255065917969, 88.7611312866211, 50.02094268798828, 45.3592643737793, 46.889888763427734, 5.49065637588501, 2.634035587310791, 5.328561305999756, 2.546333074569702, 1.8918694257736206, 2.427591323852539, 1.7708454132080078, 1.7297484874725342, 4.048952102661133, 1.719040036201477, 2.3180694580078125, 1.7044233083724976, 1.7113524675369263, 1.674343466758728, 3.3246138095855713, 1.0882512331008911, 1.0882648229599, 1.0881468057632446, 1.088144063949585, 1.0881481170654297, 1.088099479675293, 1.088118553161621, 1.0879815816879272, 1.0880151987075806, 1.087683081626892, 1.0877223014831543, 1.0877166986465454, 1.0877748727798462, 1.0877026319503784, 1.0878138542175293, 1.6111923456192017, 3.7311549186706543, 4.157538414001465, 2.623424768447876, 3.0820882320404053, 2.076266288757324, 3.510582685470581, 9.816411972045898, 55.478675842285156, 11.181052207946777, 26.098064422607422, 2.9053616523742676, 2.008272886276245, 2.035379648208618, 1.5356179475784302, 15.15774154663086, 3.2540910243988037, 8.770691871643066, 52.20994567871094, 134.774169921875, 38.537681579589844, 96.08479309082031, 98.73918151855469, 78.71825408935547, 41.67264175415039, 86.15584564208984, 42.083335876464844, 136.88111877441406, 51.263938903808594, 36.692413330078125, 21.415258407592773, 26.79378890991211, 63.62718963623047, 32.35250473022461, 24.841678619384766, 32.50813293457031, 28.624433517456055, 20.270870208740234, 92.00431060791016, 55.579341888427734, 96.88297271728516, 40.85462951660156, 38.67906951904297, 38.258872985839844, 108.71286010742188, 54.95804214477539, 52.030941009521484, 37.86552429199219, 60.00326156616211, 68.30732727050781, 45.42510223388672, 61.978946685791016, 39.40923309326172, 45.49611282348633, 37.73043441772461, 1.8774420022964478, 1.8734301328659058, 1.8684841394424438, 1.8678888082504272, 3.0505213737487793, 1.755477786064148, 2.884568214416504, 1.6275168657302856, 1.5836107730865479, 1.6155624389648438, 2.1285953521728516, 2.1259844303131104, 1.0239521265029907, 1.0239455699920654, 1.0238016843795776, 1.0240404605865479, 1.0239217281341553, 1.023851752281189, 1.0237789154052734, 1.0236848592758179, 1.0238598585128784, 1.0237892866134644, 1.023767352104187, 1.023668885231018, 1.0235661268234253, 1.0236297845840454, 1.0235233306884766, 1.023456335067749, 1.0235563516616821, 1.0234659910202026, 1.0236088037490845, 1.5112330913543701, 3.425591468811035, 1.5095984935760498, 3.420203447341919, 1.481428623199463, 3.6699461936950684, 2.8026294708251953, 10.722193717956543, 1.8922640085220337, 1.9120724201202393, 186.72128295898438, 138.2135772705078, 3.076798677444458, 46.73928451538086, 26.202533721923828, 3.0054574012756348, 4.7893571853637695, 87.1003189086914, 39.319793701171875, 137.22540283203125, 109.76622772216797, 20.590801239013672, 28.922496795654297, 38.382911682128906, 37.478336334228516, 102.4408950805664, 26.991790771484375, 79.63581085205078, 43.03392028808594, 65.36415100097656, 37.66257858276367, 36.69022750854492, 38.17575454711914, 20.278398513793945, 18.013254165649414, 35.044395446777344, 49.438716888427734, 51.04505157470703, 32.7028923034668, 13.361983299255371, 49.293304443359375, 59.908775329589844, 34.78544235229492, 44.56782531738281, 29.36772346496582, 32.63895034790039, 45.082706451416016, 32.75046157836914, 48.2697868347168, 37.975547790527344, 31.857845306396484, 2.482102632522583, 1.714690089225769, 1.715217113494873, 2.242222547531128, 2.2315964698791504, 1.6548619270324707, 1.6060107946395874, 3.6398234367370605, 2.074986696243286, 3.5902280807495117, 0.9972660541534424, 0.9972188472747803, 0.9971129894256592, 0.9970752000808716, 0.9969826936721802, 0.9970822930335999, 0.9969100952148438, 0.9968694448471069, 0.9969191551208496, 0.9967495203018188, 0.9964680075645447, 0.9966657757759094, 0.9966660141944885, 0.9965841174125671, 0.9965190887451172, 0.9966274499893188, 0.9965185523033142, 0.9963745474815369, 0.9962880611419678, 0.9962553381919861, 1.4846662282943726, 2.36258864402771, 2.771388292312622, 1.8648008108139038, 1.8471734523773193, 3.0048840045928955, 1.873166561126709, 2.20348858833313, 1.8455432653427124, 27.916555404663086, 4.365096569061279, 2.9653961658477783, 160.12191772460938, 2.218632936477661, 61.52706527709961, 56.12620162963867, 3.6506659984588623, 2.187223434448242, 19.870027542114258, 124.98152923583984, 24.609106063842773, 22.216140747070312, 49.581790924072266, 96.80322265625, 21.9818058013916, 18.237953186035156, 13.381142616271973, 62.019378662109375, 18.657312393188477, 16.654006958007812, 75.06354522705078, 37.61678695678711, 33.77568435668945, 73.944580078125, 19.464622497558594, 67.0788803100586, 84.5916748046875, 40.27652359008789, 24.7330322265625, 38.642032623291016, 102.37152862548828, 36.515419006347656, 57.21015930175781, 31.986116409301758, 58.577606201171875, 34.02787399291992, 49.115509033203125, 31.2448673248291, 28.645652770996094, 41.79546356201172, 38.67123794555664, 31.427488327026367, 34.94704055786133, 34.38996887207031, 33.34241485595703, 33.67567443847656, 1.750255823135376, 1.6892133951187134, 1.48991858959198, 0.9561224579811096, 0.9560556411743164, 0.9561611413955688, 0.9560944437980652, 0.9560136795043945, 0.9559935927391052, 0.9559632539749146, 0.9558653831481934, 0.9556791186332703, 0.9556005597114563, 0.9555144309997559, 0.9555425643920898, 0.9555019736289978, 0.9550268054008484, 0.9552805423736572, 0.9551702737808228, 0.9550695419311523, 0.954889714717865, 0.9528109431266785, 0.9520248174667358, 0.9524841904640198, 0.95132976770401, 0.9510343074798584, 0.9498026967048645, 0.9474554061889648, 1.4220051765441895, 0.9378573298454285, 3.7457754611968994, 56.52865219116211, 47.86456298828125, 1.3460701704025269, 1.7168172597885132, 87.46195983886719, 7.836106777191162, 1.7220343351364136, 1.7084614038467407, 76.9991455078125, 1.6981087923049927, 69.29823303222656, 56.7642936706543, 3.871915340423584, 26.415367126464844, 27.814619064331055, 53.72723388671875, 2.0094969272613525, 109.25141143798828, 1.9507874250411987, 17.047252655029297, 17.754987716674805, 18.390504837036133, 14.528619766235352, 28.626995086669922, 31.079912185668945, 18.300575256347656, 16.62042999267578, 19.503135681152344, 21.959768295288086, 51.8825798034668, 27.006011962890625, 13.863736152648926, 28.12826156616211, 13.60073471069336, 26.3759765625, 30.009859085083008, 58.462303161621094, 24.522415161132812, 55.689979553222656, 35.71651077270508, 24.132137298583984, 33.467735290527344, 38.73064041137695, 54.73699951171875, 50.69816207885742, 42.842681884765625, 28.392366409301758, 23.065473556518555, 25.660764694213867, 24.95818328857422], \"Term\": [\"service\", \"food\", \"good\", \"us\", \"get\", \"order\", \"wait\", \"come\", \"make\", \"time\", \"also\", \"wasnt\", \"chicken\", \"nice\", \"pretty\", \"back\", \"place\", \"meal\", \"burger\", \"pizza\", \"customer\", \"love\", \"fry\", \"take\", \"didnt\", \"go\", \"amaze\", \"happy\", \"want\", \"well\", \"jws\", \"tibetan\", \"melissa\", \"felon\", \"yesenia\", \"windsor\", \"weak\", \"80s\", \"buddy\", \"marsala\", \"gratuity\", \"brat\", \"dumpling\", \"panera\", \"tartare\", \"cavatappi\", \"wonton\", \"community\", \"exaggeration\", \"microbrewery\", \"critique\", \"mist\", \"terminate\", \"theft\", \"4oz\", \"premium\", \"michelle\", \"mgmt\", \"fraud\", \"badmouth\", \"bellini\", \"momo\", \"22\", \"hide\", \"ge\", \"vegan\", \"master\", \"impossible\", \"well\", \"stuck\", \"kung\", \"porch\", \"best\", \"creamy\", \"medium\", \"great\", \"make\", \"meatball\", \"thai\", \"roll\", \"place\", \"time\", \"restaurant\", \"could\", \"new\", \"try\", \"also\", \"food\", \"order\", \"like\", \"chicken\", \"go\", \"im\", \"always\", \"think\", \"ive\", \"find\", \"would\", \"top\", \"back\", \"one\", \"really\", \"good\", \"even\", \"flavor\", \"come\", \"table\", \"us\", \"get\", \"give\", \"price\", \"service\", \"dimsum\", \"speed\", \"obh\", \"rid\", \"thumb\", \"prince\", \"client\", \"nondairy\", \"cheesy\", \"douche\", \"fuck\", \"pacific\", \"creek\", \"innovative\", \"crown\", \"decorurban\", \"cuteyou\", \"slowspotty\", \"tepenyaki\", \"terriblei\", \"caring\", \"eric\", \"horrible3\", \"1lb\", \"palacio\", \"serviceseating\", \"reorder\", \"employement\", \"recycle\", \"meatballsmmmmmmmmmm\", \"princess\", \"quaint\", \"el\", \"soba\", \"par\", \"gf\", \"vodka\", \"japanese\", \"meal\", \"bbq\", \"happy\", \"lake\", \"importantly\", \"account\", \"drape\", \"ramen\", \"everywhere\", \"simple\", \"say\", \"get\", \"bar\", \"time\", \"come\", \"one\", \"little\", \"service\", \"delicious\", \"good\", \"love\", \"star\", \"beer\", \"sandwich\", \"try\", \"side\", \"since\", \"first\", \"bad\", \"full\", \"go\", \"would\", \"place\", \"take\", \"drink\", \"pizza\", \"food\", \"back\", \"restaurant\", \"eat\", \"like\", \"order\", \"really\", \"great\", \"menu\", \"well\", \"chicken\", \"kb\", \"rok\", \"veteran\", \"ha\", \"unlimited\", \"kyoto\", \"earlier\", \"juan\", \"earthy\", \"howie\", \"vinnies\", \"canadian\", \"monroeville\", \"waaay\", \"eu\", \"kazushi\", \"yetyoure\", \"dal\", \"plater\", \"manchurian\", \"boyger\", \"tney\", \"mama\", \"flooda\", \"lane\", \"800\", \"quicker\", \"spotless\", \"11am\", \"masala\", \"tara\", \"alley\", \"stomach\", \"lols\", \"favor\", \"marlin\", \"affordable\", \"fashion\", \"mexican\", \"rye\", \"oreganos\", \"food\", \"get\", \"foot\", \"pizza\", \"amaze\", \"sale\", \"guacamole\", \"service\", \"want\", \"good\", \"go\", \"quality\", \"people\", \"fry\", \"think\", \"place\", \"need\", \"great\", \"give\", \"like\", \"nice\", \"drink\", \"im\", \"customer\", \"inside\", \"price\", \"would\", \"try\", \"didnt\", \"quick\", \"back\", \"come\", \"menu\", \"restaurant\", \"definitely\", \"always\", \"one\", \"wait\", \"order\", \"make\", \"really\", \"lap\", \"dauphine\", \"cheung\", \"miami\", \"frites\", \"bestie\", \"amelies\", \"biryani\", \"joeys\", \"edible\", \"nacl\", \"toke\", \"benedicto\", \"600\", \"h2o\", \"robin\", \"gardein\", \"findable\", \"rccf\", \"300ish\", \"courtney\", \"texting\", \"digger\", \"potatoe\", \"fundido\", \"gathering\", \"venture\", \"vic\", \"prepped\", \"specials\", \"cure\", \"farm\", \"waffle\", \"mmm\", \"yelpers\", \"garden\", \"divine\", \"disinterested\", \"twist\", \"wasnt\", \"benedict\", \"office\", \"good\", \"devil\", \"also\", \"chicken\", \"meatloaf\", \"charcoal\", \"potato\", \"place\", \"small\", \"big\", \"really\", \"go\", \"two\", \"pork\", \"10\", \"one\", \"steak\", \"dessert\", \"order\", \"take\", \"didnt\", \"great\", \"egg\", \"time\", \"get\", \"love\", \"lunch\", \"best\", \"food\", \"even\", \"like\", \"drink\", \"come\", \"menu\", \"service\", \"nice\", \"little\", \"make\", \"would\", \"give\", \"try\", \"back\", \"restaurant\", \"well\", \"j\", \"legged\", \"apologized\", \"fruitsthey\", \"functionand\", \"nikki\", \"meme\", \"barrage\", \"appetizerslots\", \"girly\", \"gent\", \"claws\", \"informatively\", \"shutter\", \"rightthis\", \"edo\", \"unusual\", \"windfields\", \"roi\", \"vegiterian\", \"schwarma\", \"barista\", \"chi\", \"teacher\", \"confidence\", \"needs\", \"nitpick\", \"peppered\", \"sumptuous\", \"vogue\", \"bagel\", \"us\", \"wait\", \"abt\", \"ppl\", \"order\", \"min\", \"expertly\", \"minor\", \"come\", \"quarter\", \"service\", \"make\", \"told\", \"burger\", \"pretty\", \"back\", \"fly\", \"food\", \"filipino\", \"3\", \"area\", \"tell\", \"bring\", \"table\", \"nice\", \"2\", \"customer\", \"roll\", \"night\", \"time\", \"fry\", \"home\", \"take\", \"option\", \"meal\", \"love\", \"go\", \"sauce\", \"get\", \"well\", \"dont\", \"restaurant\", \"like\", \"good\", \"place\", \"great\", \"would\", \"didnt\", \"try\", \"one\"], \"Total\": [338.0, 700.0, 622.0, 186.0, 502.0, 400.0, 167.0, 376.0, 271.0, 361.0, 207.0, 79.0, 192.0, 155.0, 115.0, 269.0, 543.0, 151.0, 105.0, 150.0, 69.0, 184.0, 149.0, 162.0, 141.0, 497.0, 76.0, 64.0, 134.0, 265.0, 3.8776135444641113, 3.833832263946533, 2.8790621757507324, 2.878870964050293, 2.878436803817749, 2.8651809692382812, 2.8656320571899414, 2.8512866497039795, 4.713747024536133, 3.801306962966919, 2.8525843620300293, 2.8512372970581055, 17.905433654785156, 2.837892770767212, 4.65486478805542, 2.8169286251068115, 5.642722129821777, 3.739126205444336, 1.8792213201522827, 1.8789485692977905, 1.878912329673767, 1.8789132833480835, 1.878871202468872, 1.8789169788360596, 1.8789830207824707, 1.8787262439727783, 1.8790313005447388, 1.878963589668274, 1.878774642944336, 1.8786569833755493, 2.809983253479004, 7.492345809936523, 2.8238933086395264, 6.612093925476074, 2.84049129486084, 25.23775291442871, 2.801330089569092, 6.504817962646484, 265.1940612792969, 3.7222044467926025, 8.338462829589844, 5.541116237640381, 177.5450897216797, 19.31652069091797, 13.858100891113281, 401.9462890625, 271.79534912109375, 15.755139350891113, 56.600528717041016, 87.93938446044922, 543.031005859375, 361.0868225097656, 247.69479370117188, 103.45226287841797, 69.75274658203125, 261.07940673828125, 207.78668212890625, 700.1093139648438, 400.09112548828125, 321.13909912109375, 192.7138671875, 497.0426025390625, 161.40408325195312, 162.86865234375, 149.01168823242188, 134.23846435546875, 101.75684356689453, 247.6696014404297, 60.09392166137695, 269.16339111328125, 291.2313232421875, 209.4342041015625, 622.8684692382812, 169.09417724609375, 95.32463836669922, 376.0322570800781, 138.03912353515625, 186.90908813476562, 502.0305480957031, 181.1865997314453, 151.76116943359375, 338.559814453125, 7.110955238342285, 3.578120231628418, 7.262027740478516, 3.5770866870880127, 2.6849007606506348, 3.547039031982422, 2.674323081970215, 2.6816983222961426, 6.290464878082275, 2.6741647720336914, 3.647857427597046, 2.6847527027130127, 2.7129716873168945, 2.6614882946014404, 5.305234909057617, 1.7859764099121094, 1.7860084772109985, 1.785893201828003, 1.7859561443328857, 1.7859694957733154, 1.7859246730804443, 1.7859991788864136, 1.7858562469482422, 1.7859469652175903, 1.785498023033142, 1.785629153251648, 1.7857667207717896, 1.7858772277832031, 1.7858269214630127, 1.7860130071640015, 2.6495213508605957, 6.314618110656738, 7.11317253112793, 4.440467357635498, 5.311511516571045, 3.585672378540039, 6.389808654785156, 20.436017990112305, 151.9482421875, 24.686731338500977, 64.89474487304688, 5.396002769470215, 3.566117763519287, 3.6253297328948975, 2.6394753456115723, 35.735538482666016, 6.213449478149414, 19.820579528808594, 162.10903930664062, 502.0305480957031, 116.0568618774414, 361.0868225097656, 376.0322570800781, 291.2313232421875, 134.34971618652344, 338.559814453125, 142.355712890625, 622.8684692382812, 184.71412658691406, 123.082763671875, 62.498416900634766, 83.86681365966797, 261.07940673828125, 110.14187622070312, 77.90001678466797, 112.3137435913086, 95.66207122802734, 60.950408935546875, 497.0426025390625, 247.6696014404297, 543.031005859375, 162.56396484375, 150.89822387695312, 150.20867919921875, 700.1093139648438, 269.16339111328125, 247.69479370117188, 151.2852325439453, 321.13909912109375, 400.09112548828125, 209.4342041015625, 401.9462890625, 168.88858032226562, 265.1940612792969, 192.7138671875, 2.586526870727539, 2.5858912467956543, 2.5851521492004395, 2.5852348804473877, 4.269433975219727, 2.5878992080688477, 4.353326797485352, 2.601397752761841, 2.5779638290405273, 2.6305296421051025, 3.496629238128662, 3.4925343990325928, 1.7323510646820068, 1.732353925704956, 1.7322508096694946, 1.7326747179031372, 1.7325713634490967, 1.7324658632278442, 1.732478380203247, 1.7323774099349976, 1.7326990365982056, 1.7325806617736816, 1.7325639724731445, 1.7324209213256836, 1.7322843074798584, 1.7324305772781372, 1.7322996854782104, 1.7322745323181152, 1.7324451208114624, 1.7323131561279297, 1.7325570583343506, 2.5825958251953125, 6.0788726806640625, 2.598745346069336, 6.249094486236572, 2.581817388534546, 6.996070384979248, 5.220035076141357, 24.35895347595215, 3.476309299468994, 3.5315544605255127, 700.1093139648438, 502.0305480957031, 6.166364669799805, 150.20867919921875, 76.68460845947266, 6.111419677734375, 10.596872329711914, 338.559814453125, 134.5157928466797, 622.8684692382812, 497.0426025390625, 66.5129165649414, 103.4267349243164, 149.16424560546875, 149.01168823242188, 543.031005859375, 98.13935852050781, 401.9462890625, 181.1865997314453, 321.13909912109375, 155.88121032714844, 150.89822387695312, 161.40408325195312, 69.87395477294922, 59.617919921875, 151.76116943359375, 247.6696014404297, 261.07940673828125, 141.3845672607422, 40.374290466308594, 269.16339111328125, 376.0322570800781, 168.88858032226562, 247.69479370117188, 134.47213745117188, 162.86865234375, 291.2313232421875, 167.472412109375, 400.09112548828125, 271.79534912109375, 209.4342041015625, 3.4081966876983643, 2.5367727279663086, 2.563699960708618, 3.3837788105010986, 3.392489433288574, 2.5335872173309326, 2.586500644683838, 6.003113746643066, 3.484835147857666, 6.066145896911621, 1.7102025747299194, 1.710249423980713, 1.7101659774780273, 1.7101072072982788, 1.7099970579147339, 1.710209608078003, 1.7099601030349731, 1.7099865674972534, 1.7101950645446777, 1.7099289894104004, 1.7096030712127686, 1.7099978923797607, 1.7100203037261963, 1.7099016904830933, 1.7098528146743774, 1.710073471069336, 1.7099418640136719, 1.7100080251693726, 1.709947109222412, 1.7099473476409912, 2.578535556793213, 4.371278762817383, 5.225249767303467, 3.4610776901245117, 3.4265642166137695, 5.987119674682617, 3.504369020462036, 4.228812217712402, 3.4589900970458984, 79.92514038085938, 9.353434562683105, 6.015801429748535, 622.8684692382812, 4.302403926849365, 207.78668212890625, 192.7138671875, 7.8203229904174805, 4.293560028076172, 58.02812957763672, 543.031005859375, 81.83915710449219, 76.609130859375, 209.4342041015625, 497.0426025390625, 76.50259399414062, 60.751869201660156, 41.48518371582031, 291.2313232421875, 63.67033767700195, 55.928340911865234, 400.09112548828125, 162.56396484375, 141.3845672607422, 401.9462890625, 68.69434356689453, 361.0868225097656, 502.0305480957031, 184.71412658691406, 95.70428466796875, 177.5450897216797, 700.1093139648438, 169.09417724609375, 321.13909912109375, 150.89822387695312, 376.0322570800781, 168.88858032226562, 338.559814453125, 155.88121032714844, 134.34971618652344, 271.79534912109375, 247.6696014404297, 181.1865997314453, 261.07940673828125, 269.16339111328125, 247.69479370117188, 265.1940612792969, 2.473508834838867, 2.483795404434204, 2.5039215087890625, 1.6762112379074097, 1.6761152744293213, 1.6763226985931396, 1.6762300729751587, 1.6761422157287598, 1.6761153936386108, 1.6760969161987305, 1.6762397289276123, 1.6761205196380615, 1.6760938167572021, 1.675981879234314, 1.6761527061462402, 1.6762754917144775, 1.6757781505584717, 1.6762714385986328, 1.6761624813079834, 1.6763312816619873, 1.676140546798706, 1.676598072052002, 1.6762425899505615, 1.677168846130371, 1.6762018203735352, 1.6765618324279785, 1.6763877868652344, 1.6765350103378296, 2.540358066558838, 1.6773974895477295, 6.806491851806641, 186.90908813476562, 167.472412109375, 2.5291037559509277, 3.38967227935791, 400.09112548828125, 21.3203067779541, 3.423370838165283, 3.4130711555480957, 376.0322570800781, 3.3965413570404053, 338.559814453125, 271.79534912109375, 9.546323776245117, 105.7045669555664, 115.60556030273438, 269.16339111328125, 4.293635368347168, 700.1093139648438, 4.170384407043457, 68.7210693359375, 73.04525756835938, 76.52814483642578, 57.04930877685547, 138.03912353515625, 155.88121032714844, 78.78314971923828, 69.87395477294922, 87.93938446044922, 109.10550689697266, 361.0868225097656, 149.16424560546875, 60.54437255859375, 162.56396484375, 58.968544006347656, 151.9482421875, 184.71412658691406, 497.0426025390625, 142.41665649414062, 502.0305480957031, 265.1940612792969, 140.4808807373047, 247.69479370117188, 321.13909912109375, 622.8684692382812, 543.031005859375, 401.9462890625, 247.6696014404297, 141.3845672607422, 261.07940673828125, 291.2313232421875], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1138999462127686, 1.0535000562667847, 1.0377000570297241, 1.037500023841858, 1.037500023841858, 1.0032000541687012, 1.0003999471664429, 0.9696000218391418, 0.9502999782562256, 0.940500020980835, 0.9121999740600586, 0.9070000052452087, 0.9032999873161316, 0.878000020980835, 0.8772000074386597, 0.8745999932289124, 0.8718000054359436, 0.8600000143051147, 0.8586000204086304, 0.8585000038146973, 0.8583999872207642, 0.8583999872207642, 0.858299970626831, 0.858299970626831, 0.858299970626831, 0.858299970626831, 0.8582000136375427, 0.8582000136375427, 0.8582000136375427, 0.8582000136375427, 0.8555999994277954, 0.8431000113487244, 0.8526999950408936, 0.8202000260353088, 0.8450000286102295, 0.7404000163078308, 0.8409000039100647, 0.7986000180244446, 0.5181000232696533, 0.8126000165939331, 0.7440999746322632, 0.765500009059906, 0.3986999988555908, 0.6384999752044678, 0.6728000044822693, 0.27730000019073486, 0.30090001225471497, 0.6413999795913696, 0.4681999981403351, 0.39959999918937683, 0.1340000033378601, 0.17560000717639923, 0.2290000021457672, 0.34689998626708984, 0.4097000062465668, 0.19419999420642853, 0.2298000007867813, 0.018699999898672104, 0.11100000143051147, 0.13860000669956207, 0.22290000319480896, 0.03999999910593033, 0.24709999561309814, 0.23520000278949738, 0.23770000040531158, 0.2583000063896179, 0.31380000710487366, 0.12020000070333481, 0.4147999882698059, 0.052799999713897705, 0.020500000566244125, 0.08569999784231186, -0.23019999265670776, 0.12389999628067017, 0.2863999903202057, -0.218299999833107, 0.1462000012397766, 0.021700000390410423, -0.42570000886917114, 0.019899999722838402, 0.09929999709129333, -0.6699000000953674, 1.2698999643325806, 1.2222000360488892, 1.2188999652862549, 1.188599944114685, 1.1784000396728516, 1.1492999792099, 1.1162999868392944, 1.090000033378601, 1.0879000425338745, 1.0865999460220337, 1.0750999450683594, 1.0741000175476074, 1.0677000284194946, 1.065000057220459, 1.0612000226974487, 1.0331000089645386, 1.0331000089645386, 1.0331000089645386, 1.0329999923706055, 1.0329999923706055, 1.0329999923706055, 1.0329999923706055, 1.0328999757766724, 1.0328999757766724, 1.0328999757766724, 1.0327999591827393, 1.0326999425888062, 1.0326999425888062, 1.0326999425888062, 1.0326999425888062, 1.0311000347137451, 1.0024000406265259, 0.9915000200271606, 1.0022000074386597, 0.9842000007629395, 0.9821000099182129, 0.9296000003814697, 0.7953000068664551, 0.5210000276565552, 0.7365000247955322, 0.6176000237464905, 0.9093999862670898, 0.9542999863624573, 0.951200008392334, 0.9868999719619751, 0.6708999872207642, 0.8816999793052673, 0.7131999731063843, 0.3955000042915344, 0.2134000062942505, 0.4260999858379364, 0.2046000063419342, 0.19130000472068787, 0.22030000388622284, 0.3578999936580658, 0.1599999964237213, 0.30979999899864197, 0.013299999758601189, 0.2467000037431717, 0.3181999921798706, 0.45750001072883606, 0.3874000012874603, 0.11670000106096268, 0.3034000098705292, 0.3856000006198883, 0.28870001435279846, 0.32190001010894775, 0.4275999963283539, -0.1582999974489212, 0.03420000150799751, -0.19519999623298645, 0.14749999344348907, 0.1671999990940094, 0.16089999675750732, -0.33399999141693115, -0.06019999831914902, -0.03189999982714653, 0.14339999854564667, -0.14900000393390656, -0.23919999599456787, 0.00019999999494757503, -0.3409999907016754, 0.07329999655485153, -0.23430000245571136, -0.10220000147819519, 1.3473999500274658, 1.3454999923706055, 1.3430999517440796, 1.3428000211715698, 1.3315999507904053, 1.2797000408172607, 1.2561999559402466, 1.198799967765808, 1.1805000305175781, 1.180299997329712, 1.1714999675750732, 1.1713999509811401, 1.1419999599456787, 1.1419999599456787, 1.1418999433517456, 1.1418999433517456, 1.141800045967102, 1.141800045967102, 1.141700029373169, 1.141700029373169, 1.141700029373169, 1.141700029373169, 1.141700029373169, 1.141700029373169, 1.1416000127792358, 1.1416000127792358, 1.1416000127792358, 1.1414999961853027, 1.1414999961853027, 1.1414999961853027, 1.1414999961853027, 1.1318999528884888, 1.0943000316619873, 1.1246000528335571, 1.0650999546051025, 1.1123000383377075, 1.0226000547409058, 1.0458999872207642, 0.8471999764442444, 1.0595999956130981, 1.0542000532150269, 0.34619998931884766, 0.37790000438690186, 0.972599983215332, 0.5004000067710876, 0.593999981880188, 0.9581000208854675, 0.8736000061035156, 0.3102000057697296, 0.43779999017715454, 0.1551000028848648, 0.1574999988079071, 0.4952000081539154, 0.3935999870300293, 0.31040000915527344, 0.2874999940395355, -9.999999747378752e-05, 0.37689998745918274, 0.048900000751018524, 0.23029999434947968, 0.07590000331401825, 0.24740000069141388, 0.25369998812675476, 0.22609999775886536, 0.43070000410079956, 0.4708999991416931, 0.2020999938249588, 0.05640000104904175, 0.0357000008225441, 0.2037999927997589, 0.5619999766349792, -0.02969999983906746, -0.16910000145435333, 0.08780000358819962, -0.04740000143647194, 0.14630000293254852, 0.06040000170469284, -0.19779999554157257, 0.03590000048279762, -0.4471000134944916, -0.3003000020980835, -0.21529999375343323, 1.4072999954223633, 1.332800030708313, 1.3224999904632568, 1.3128999471664429, 1.3056000471115112, 1.2984999418258667, 1.2479000091552734, 1.2240999937057495, 1.2059999704360962, 1.1999000310897827, 1.1850999593734741, 1.184999942779541, 1.1849000453948975, 1.1849000453948975, 1.1849000453948975, 1.1849000453948975, 1.1849000453948975, 1.1848000288009644, 1.1847000122070312, 1.1847000122070312, 1.1845999956130981, 1.1845999956130981, 1.1845999956130981, 1.1845999956130981, 1.184499979019165, 1.184499979019165, 1.184499979019165, 1.1842999458312988, 1.1842000484466553, 1.1842000484466553, 1.1723999977111816, 1.1090999841690063, 1.0902999639511108, 1.1059999465942383, 1.1065000295639038, 1.035099983215332, 1.0980000495910645, 1.0724999904632568, 1.0961999893188477, 0.6725999712944031, 0.9623000025749207, 1.0169999599456787, 0.3659999966621399, 1.0621000528335571, 0.5073999762535095, 0.49079999327659607, 0.9625999927520752, 1.0499000549316406, 0.6527000069618225, 0.25540000200271606, 0.5228000283241272, 0.48649999499320984, 0.28360000252723694, 0.08839999884366989, 0.4772999882698059, 0.5210999846458435, 0.5928999781608582, 0.1777999997138977, 0.4968999922275543, 0.5130000114440918, 0.051100000739097595, 0.26080000400543213, 0.29269999265670776, 0.03139999881386757, 0.4634000062942505, 0.041200000792741776, -0.05640000104904175, 0.2013999968767166, 0.37130001187324524, 0.19949999451637268, -0.19820000231266022, 0.19169999659061432, -0.000699999975040555, 0.17309999465942383, -0.13490000367164612, 0.12240000069141388, -0.2061000019311905, 0.11720000207424164, 0.17900000512599945, -0.1477999985218048, -0.13259999454021454, -0.027400000020861626, -0.2865999937057495, -0.33309999108314514, -0.2809000015258789, -0.3393000066280365, 1.5812000036239624, 1.541599988937378, 1.4079999923706055, 1.3657000064849854, 1.3657000064849854, 1.3657000064849854, 1.3655999898910522, 1.3655999898910522, 1.3655999898910522, 1.3655999898910522, 1.365399956703186, 1.3653000593185425, 1.3652000427246094, 1.3652000427246094, 1.3651000261306763, 1.3650000095367432, 1.364799976348877, 1.364799976348877, 1.3646999597549438, 1.3645000457763672, 1.364400029182434, 1.3619999885559082, 1.3614000082015991, 1.361299991607666, 1.360700011253357, 1.3601000308990479, 1.3588999509811401, 1.3564000129699707, 1.3468999862670898, 1.3457000255584717, 1.329800009727478, 0.7311999797821045, 0.6746000051498413, 1.2963999509811401, 1.2467999458312988, 0.4065999984741211, 0.9261999726295471, 1.2400000095367432, 1.2351000308990479, 0.34119999408721924, 1.2338000535964966, 0.3407999873161316, 0.36090001463890076, 1.0247000455856323, 0.5404000282287598, 0.5024999976158142, 0.3156999945640564, 1.167799949645996, 0.06949999928474426, 1.1672999858856201, 0.5329999923706055, 0.5127000212669373, 0.5012999773025513, 0.5593000054359436, 0.3538999855518341, 0.31459999084472656, 0.4672999978065491, 0.4909999966621399, 0.42100000381469727, 0.3240000009536743, -0.013000000268220901, 0.21809999644756317, 0.453000009059906, 0.1728000044822693, 0.4602000117301941, 0.17599999904632568, 0.10980000346899033, -0.21320000290870667, 0.1678999960422516, -0.2718000113964081, -0.07779999822378159, 0.1656000018119812, -0.07450000196695328, -0.1881999969482422, -0.5047000050544739, -0.4442000091075897, -0.3116999864578247, -0.23890000581741333, 0.11389999836683273, -0.3928000032901764, -0.5297999978065491], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.469900131225586, -8.541600227355957, -8.843899726867676, -8.844099998474121, -8.844300270080566, -8.883199691772461, -8.8858003616333, -8.921699523925781, -8.438199996948242, -8.663200378417969, -8.978599548339844, -8.984299659729004, -7.150599956512451, -9.017999649047852, -8.523900032043457, -9.028800010681152, -8.336799621582031, -8.760100364685059, -9.449600219726562, -9.449899673461914, -9.449899673461914, -9.449999809265137, -9.449999809265137, -9.449999809265137, -9.449999809265137, -9.450200080871582, -9.449999809265137, -9.45009994506836, -9.450200080871582, -9.450300216674805, -9.050299644470215, -8.082099914550781, -9.048199653625488, -8.229900360107422, -9.050100326538086, -6.970200061798096, -9.068099975585938, -8.267900466918945, -4.8404998779296875, -8.812100410461426, -8.07409954071045, -8.461299896240234, -5.361100196838379, -7.339600086212158, -7.63730001449585, -4.66540002822876, -5.033100128173828, -7.54040002822876, -6.434899806976318, -6.06279993057251, -4.507900238037109, -4.874300003051758, -5.197800159454346, -5.953100204467773, -6.28439998626709, -5.179999828338623, -5.372700214385986, -4.369100093841553, -4.836299896240234, -5.028600215911865, -5.454999923706055, -4.690400123596191, -5.607999801635742, -5.610899925231934, -5.697299957275391, -5.781099796295166, -6.002600193023682, -5.306700229644775, -6.428299903869629, -5.290900230407715, -5.2444000244140625, -5.508900165557861, -4.734899997711182, -5.684700012207031, -6.095300197601318, -5.227700233459473, -5.865300178527832, -5.686699867248535, -5.146100044250488, -5.719600200653076, -5.817399978637695, -5.784200191497803, -7.707499980926514, -8.442000389099121, -7.737400054931641, -8.475899696350098, -8.77299976348877, -8.523599624633789, -8.839099884033203, -8.862600326538086, -8.012100219726562, -8.868800163269043, -8.56980037689209, -8.877300262451172, -8.873200416564941, -8.895099639892578, -8.209199905395508, -9.326000213623047, -9.325900077819824, -9.32610034942627, -9.32610034942627, -9.32610034942627, -9.32610034942627, -9.32610034942627, -9.326199531555176, -9.326199531555176, -9.326499938964844, -9.326399803161621, -9.326399803161621, -9.326399803161621, -9.326499938964844, -9.326399803161621, -8.933600425720215, -8.093799591064453, -7.985599994659424, -8.446000099182129, -8.284899711608887, -8.680000305175781, -8.15470027923584, -7.126500129699707, -5.394499778747559, -6.996300220489502, -6.14870023727417, -8.343999862670898, -8.713299751281738, -8.699799537658691, -8.981599807739258, -6.691999912261963, -8.230600357055664, -7.239099979400635, -5.4552998542785645, -4.506899833679199, -5.758900165557861, -4.845300197601318, -4.817999839782715, -5.0447001457214355, -5.680699825286865, -4.954400062561035, -5.670899868011475, -4.491399765014648, -5.473499774932861, -5.808000087738037, -6.346399784088135, -6.122399806976318, -5.257500171661377, -5.933800220489502, -6.197999954223633, -5.928999900817871, -6.056300163269043, -6.401299953460693, -4.888700008392334, -5.3927001953125, -4.836999893188477, -5.700500011444092, -5.755199909210205, -5.766200065612793, -4.721799850463867, -5.4039998054504395, -5.458700180053711, -5.776500225067139, -5.316100120544434, -5.186500072479248, -5.5945000648498535, -5.283699989318848, -5.736499786376953, -5.592899799346924, -5.780099868774414, -8.641300201416016, -8.643500328063965, -8.646100044250488, -8.646400451660156, -8.155900001525879, -8.708499908447266, -8.211899757385254, -8.784199714660645, -8.81149959564209, -8.791600227355957, -8.515800476074219, -8.517000198364258, -9.247599601745605, -9.247599601745605, -9.247699737548828, -9.2475004196167, -9.247599601745605, -9.247699737548828, -9.247699737548828, -9.24779987335205, -9.247699737548828, -9.247699737548828, -9.247699737548828, -9.24779987335205, -9.247900009155273, -9.247900009155273, -9.248000144958496, -9.248100280761719, -9.248000144958496, -9.248000144958496, -9.247900009155273, -8.85830020904541, -8.039999961853027, -8.859399795532227, -8.041500091552734, -8.878199577331543, -7.971099853515625, -8.240699768066406, -6.898900032043457, -8.633500099182129, -8.623100280761719, -4.041600227355957, -4.342400074005127, -8.147299766540527, -5.426700115203857, -6.00540018081665, -8.17080020904541, -7.704800128936768, -4.804200172424316, -5.5995001792907715, -4.349599838256836, -4.57289981842041, -6.246399879455566, -5.906599998474121, -5.623600006103516, -5.647500038146973, -4.642000198364258, -5.9756999015808105, -4.893799781799316, -5.509200096130371, -5.091300010681152, -5.642600059509277, -5.668700218200684, -5.629000186920166, -6.26170015335083, -6.380099773406982, -5.714600086212158, -5.370500087738037, -5.338500022888184, -5.78380012512207, -6.678800106048584, -5.3734002113342285, -5.178400039672852, -5.7220001220703125, -5.4741997718811035, -5.891300201416016, -5.785699844360352, -5.462699890136719, -5.782299995422363, -5.394400119781494, -5.634300231933594, -5.809999942779541, -8.305500030517578, -8.675399780273438, -8.675100326538086, -8.407099723815918, -8.41189956665039, -8.71090030670166, -8.740900039672852, -7.922699928283691, -8.484700202941895, -7.936399936676025, -9.217300415039062, -9.217399597167969, -9.217499732971191, -9.217499732971191, -9.217599868774414, -9.217499732971191, -9.217700004577637, -9.217700004577637, -9.217700004577637, -9.217900276184082, -9.218099594116211, -9.218000411987305, -9.218000411987305, -9.218000411987305, -9.218099594116211, -9.218000411987305, -9.218099594116211, -9.218199729919434, -9.218299865722656, -9.218400001525879, -8.8193998336792, -8.354900360107422, -8.195300102233887, -8.591500282287598, -8.60099983215332, -8.114399909973145, -8.586999893188477, -8.424599647521973, -8.601799964904785, -5.88539981842041, -7.741000175476074, -8.127599716186523, -4.138700008392334, -8.417699813842773, -5.095099925994873, -5.186999797821045, -7.9197001457214355, -8.432000160217285, -6.225399971008301, -4.38640022277832, -6.011499881744385, -6.113800048828125, -5.310999870300293, -4.641900062561035, -6.1244001388549805, -6.311100006103516, -6.620800018310547, -5.087200164794922, -6.288400173187256, -6.4019999504089355, -4.896299839019775, -5.587200164794922, -5.694900035858154, -4.911300182342529, -6.245999813079834, -5.008699893951416, -4.776800155639648, -5.518799781799316, -6.006499767303467, -5.560299873352051, -4.585999965667725, -5.6168999671936035, -5.167900085449219, -5.749300003051758, -5.1442999839782715, -5.687399864196777, -5.320400238037109, -5.772799968719482, -5.859600067138672, -5.481800079345703, -5.559500217437744, -5.766900062561035, -5.660799980163574, -5.676799774169922, -5.707799911499023, -5.697800159454346, -8.452199935913086, -8.487700462341309, -8.613200187683105, -9.05679988861084, -9.056900024414062, -9.05679988861084, -9.05679988861084, -9.056900024414062, -9.056900024414062, -9.057000160217285, -9.057100296020508, -9.057299613952637, -9.05739974975586, -9.05739974975586, -9.05739974975586, -9.057499885559082, -9.057999610900879, -9.057700157165527, -9.05780029296875, -9.057900428771973, -9.058099746704102, -9.06029987335205, -9.061100006103516, -9.060600280761719, -9.061800003051758, -9.062199592590332, -9.063400268554688, -9.065899848937988, -8.659899711608887, -9.07610034942627, -7.691299915313721, -4.977200031280518, -5.143599987030029, -8.714799880981445, -8.471500396728516, -4.5406999588012695, -6.953199863433838, -8.468400001525879, -8.476400375366211, -4.6682000160217285, -8.482399940490723, -4.773499965667725, -4.9730000495910645, -7.658199787139893, -5.73799991607666, -5.686399936676025, -5.0279998779296875, -8.31410026550293, -4.318299770355225, -8.343700408935547, -6.176000118255615, -6.135300159454346, -6.100100040435791, -6.3358001708984375, -5.657599925994873, -5.575399875640869, -6.105000019073486, -6.201300144195557, -6.041399955749512, -5.922699928283691, -5.063000202178955, -5.71589994430542, -6.382699966430664, -5.67519998550415, -6.401800155639648, -5.739500045776367, -5.610400199890137, -4.943600177764893, -5.812399864196777, -4.992099761962891, -5.436299800872803, -5.828400135040283, -5.501399993896484, -5.355299949645996, -5.009399890899658, -5.086100101470947, -5.25439977645874, -5.665800094604492, -5.873600006103516, -5.767000198364258, -5.7947001457214355]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 3, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 4, 1, 4, 3, 1, 2, 5, 1, 2, 1, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 4, 5, 1, 2, 3, 4, 5, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 2, 4, 3, 1, 1, 2, 3, 4, 5, 1, 5, 1, 2, 3, 4, 5, 1, 3, 2, 1, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 1, 2, 3, 4, 5, 1, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 2, 1, 2, 3, 5, 4, 1, 2, 3, 4, 5, 2, 3, 4, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 1, 2, 3, 4, 5, 4, 2, 5, 2, 4, 5, 1, 4, 1, 2, 3, 4, 5, 2, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 3, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 2, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 1, 5, 1, 4, 1, 2, 3, 4, 1, 3, 1, 3, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 5, 4, 4, 2, 3, 4, 5, 4, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 4, 3, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 2, 1, 3, 1, 2, 3, 4, 5, 2, 1, 3, 4, 5, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 4, 3, 1, 3, 3, 1, 2, 3, 4, 3, 1, 2, 5, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 3, 3, 1, 3, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 2, 4, 1, 2, 3, 4, 5, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 1, 1, 1, 2, 3, 4, 5, 1, 5, 1, 1, 4, 1, 2, 3, 5, 3, 4, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 2, 1, 2, 4, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 2, 2, 1, 2, 3, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 2, 1, 2, 3, 5, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 2, 2, 1, 2, 3, 4, 5, 2, 5, 4, 5, 3, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 2, 3, 4, 4, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 5, 5, 1, 2, 3, 4, 5, 2, 1, 2, 4, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 2, 1, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 3, 4, 1, 3, 1, 2, 3, 5, 3, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 5, 1, 1, 2, 4, 1, 2, 3, 4, 5, 2, 3, 4, 1, 3], \"Freq\": [0.19283993542194366, 0.12052495777606964, 0.19283993542194366, 0.3133648931980133, 0.19283993542194366, 0.5772188901901245, 0.5599270462989807, 0.279247522354126, 0.22847525775432587, 0.139623761177063, 0.11423762887716293, 0.22847525775432587, 0.7082420587539673, 0.1600673645734787, 0.2910315692424774, 0.1746189296245575, 0.13096420466899872, 0.24737682938575745, 0.5848196148872375, 0.5322027802467346, 0.5847586393356323, 0.5772237181663513, 0.7014377117156982, 0.39539697766304016, 0.39539697766304016, 0.2758369743824005, 0.551673948764801, 0.14293737709522247, 0.5717495083808899, 0.14293737709522247, 0.14293737709522247, 0.7744146585464478, 0.3416965901851654, 0.13956621289253235, 0.12031570076942444, 0.2983829379081726, 0.10106518864631653, 0.34383535385131836, 0.15963783860206604, 0.20261725783348083, 0.17805759608745575, 0.11665841937065125, 0.28688937425613403, 0.09128298610448837, 0.33905109763145447, 0.1304042637348175, 0.16952554881572723, 0.7732455134391785, 0.39937353134155273, 0.5966176390647888, 0.27380284667015076, 0.21904228627681732, 0.095830999314785, 0.1642817109823227, 0.24642257392406464, 0.2860715985298157, 0.20433685183525085, 0.1820455640554428, 0.1263173222541809, 0.20062163472175598, 0.21952274441719055, 0.3031504452228546, 0.20906928181648254, 0.1254415661096573, 0.1463484913110733, 0.5322951674461365, 0.14691856503486633, 0.14691856503486633, 0.14691856503486633, 0.5876742601394653, 0.20679518580436707, 0.3360421657562256, 0.18956224620342255, 0.12063052505254745, 0.14647991955280304, 0.5964458584785461, 0.5966081023216248, 0.32406070828437805, 0.4455834925174713, 0.040507588535547256, 0.12152276933193207, 0.040507588535547256, 0.24000607430934906, 0.3360085189342499, 0.11200283467769623, 0.25600647926330566, 0.04800121486186981, 0.7117480039596558, 0.10691259801387787, 0.10691259801387787, 0.10691259801387787, 0.4276503920555115, 0.21382519602775574, 0.584738552570343, 0.40553078055381775, 0.12391218543052673, 0.15207403898239136, 0.21966250240802765, 0.10138269513845444, 0.7893945574760437, 0.13053274154663086, 0.2871720492839813, 0.18274584412574768, 0.2871720492839813, 0.11747946590185165, 0.3331604301929474, 0.6663208603858948, 0.5771342515945435, 0.7014498710632324, 0.2103443592786789, 0.28045913577079773, 0.15775826573371887, 0.08764348179101944, 0.2629304528236389, 0.6364363431930542, 0.21214544773101807, 0.1892065852880478, 0.1702859252691269, 0.17974625527858734, 0.2081272453069687, 0.24596856534481049, 0.28632503747940063, 0.5726500749588013, 0.5599340200424194, 0.7099931240081787, 0.23290695250034332, 0.23290695250034332, 0.46581390500068665, 0.23290695250034332, 0.15897075831890106, 0.6358830332756042, 0.15897075831890106, 0.7801225185394287, 0.5965723395347595, 0.3372876048088074, 0.1971835196018219, 0.10378080606460571, 0.2905862629413605, 0.07264656573534012, 0.5966157913208008, 0.7478528022766113, 0.21806639432907104, 0.26327529549598694, 0.15956078469753265, 0.15690143406391144, 0.20476967096328735, 0.5348843336105347, 0.26744216680526733, 0.5965868830680847, 0.38665175437927246, 0.15466070175170898, 0.18365958333015442, 0.14499440789222717, 0.13532811403274536, 0.5849310755729675, 0.5176915526390076, 0.15530747175216675, 0.051769156008958817, 0.15530747175216675, 0.10353831201791763, 0.7371990084648132, 0.5322228074073792, 0.5654792189598083, 0.18849307298660278, 0.18849307298660278, 0.38781702518463135, 0.11449187248945236, 0.24329523742198944, 0.28622967004776, 0.11449187248945236, 0.24329523742198944, 0.5599077343940735, 0.5772119760513306, 0.788403332233429, 0.5599178075790405, 0.25284048914909363, 0.20822156965732574, 0.2156580537557602, 0.1859121173620224, 0.12642024457454681, 0.29503557085990906, 0.29503557085990906, 0.11239450424909592, 0.18264107406139374, 0.119419164955616, 0.28608036041259766, 0.1966802477836609, 0.12516015768051147, 0.303960382938385, 0.08940011262893677, 0.2324281930923462, 0.2324281930923462, 0.4648563861846924, 0.21218723058700562, 0.15560397505760193, 0.2334059625864029, 0.24047885835170746, 0.1626768708229065, 0.5847883820533752, 0.7031404376029968, 0.14062808454036713, 0.23647302389144897, 0.47294604778289795, 0.23647302389144897, 0.2853580713272095, 0.570716142654419, 0.2918546497821808, 0.14948654174804688, 0.20643378794193268, 0.18507856130599976, 0.1708417534828186, 0.7478970885276794, 0.7577263712882996, 0.3788631856441498, 0.17892855405807495, 0.2584523558616638, 0.24519838392734528, 0.21206346154212952, 0.11265871673822403, 0.6701875925064087, 0.11169793456792831, 0.05584896728396416, 0.05584896728396416, 0.11169793456792831, 0.22970938682556152, 0.6891281604766846, 0.7758060693740845, 0.27101126313209534, 0.25118115544319153, 0.19830091297626495, 0.15864072740077972, 0.12559057772159576, 0.16484931111335754, 0.16484931111335754, 0.6593972444534302, 0.5965606570243835, 0.2765875458717346, 0.14557239413261414, 0.1601296365261078, 0.2765875458717346, 0.14557239413261414, 0.14058424532413483, 0.5623369812965393, 0.14058424532413483, 0.14058424532413483, 0.5599489212036133, 0.5599106550216675, 0.5772836208343506, 0.30752092599868774, 0.1951575130224228, 0.14784660935401917, 0.2188129723072052, 0.13601887226104736, 0.1609411984682083, 0.48282358050346375, 0.1609411984682083, 0.1609411984682083, 0.5321353077888489, 0.29210975766181946, 0.5842195153236389, 0.22876600921154022, 0.45753201842308044, 0.19156959652900696, 0.19156959652900696, 0.5747087597846985, 0.19156959652900696, 0.3200463652610779, 0.4800695478916168, 0.6947168111801147, 0.4795720875263214, 0.4795720875263214, 0.3734392523765564, 0.20637433230876923, 0.16706493496894836, 0.09827348589897156, 0.15723757445812225, 0.5847998857498169, 0.17807260155677795, 0.293819785118103, 0.21368712186813354, 0.16026534140110016, 0.16026534140110016, 0.3566758930683136, 0.1783379465341568, 0.16784746944904327, 0.18882840871810913, 0.1049046739935875, 0.5772269368171692, 0.2329028695821762, 0.2329028695821762, 0.2329028695821762, 0.4658057391643524, 0.27567124366760254, 0.15568996965885162, 0.26710113883018494, 0.14569152891635895, 0.15568996965885162, 0.32434019446372986, 0.486510306596756, 0.16217009723186493, 0.5322617888450623, 0.5895375609397888, 0.5965835452079773, 0.20782460272312164, 0.16760048270225525, 0.2547527253627777, 0.18771255016326904, 0.18100851774215698, 0.2741335332393646, 0.5482670664787292, 0.26250848174095154, 0.32813560962677, 0.18047459423542023, 0.09844068437814713, 0.11484746634960175, 0.5966176986694336, 0.5848456621170044, 0.5848089456558228, 0.16702522337436676, 0.16702522337436676, 0.5010756850242615, 0.16702522337436676, 0.5847702026367188, 0.7041035294532776, 0.3520517647266388, 0.5965733528137207, 0.17728005349636078, 0.26890793442726135, 0.2748836576938629, 0.16931240260601044, 0.11154699325561523, 0.27888771891593933, 0.5577754378318787, 0.5966241955757141, 0.2759585976600647, 0.19869019091129303, 0.23732438683509827, 0.17109432816505432, 0.1159026101231575, 0.28166601061820984, 0.18509480357170105, 0.22130899131298065, 0.19515429437160492, 0.11669019609689713, 0.2151336967945099, 0.2199501246213913, 0.2199501246213913, 0.25687605142593384, 0.0883011445403099, 0.7011185884475708, 0.35825681686401367, 0.15424945950508118, 0.19903156161308289, 0.18410420417785645, 0.10697946697473526, 0.18873493373394012, 0.18873493373394012, 0.4718373417854309, 0.09436746686697006, 0.5847963094711304, 0.7736241221427917, 0.12327654659748077, 0.4006487727165222, 0.18491481244564056, 0.09245740622282028, 0.18491481244564056, 0.6049520969390869, 0.15123802423477173, 0.15123802423477173, 0.26426899433135986, 0.16516812145709991, 0.1816849261522293, 0.16516812145709991, 0.23123537003993988, 0.5599554777145386, 0.38015156984329224, 0.7603031396865845, 0.3469552993774414, 0.19826015830039978, 0.23543395102024078, 0.14249949157238007, 0.07434756308794022, 0.5608339905738831, 0.6149288415908813, 0.15373221039772034, 0.15373221039772034, 0.15373221039772034, 0.5966253280639648, 0.7514592409133911, 0.21805524826049805, 0.15096132457256317, 0.30192264914512634, 0.1677348017692566, 0.15096132457256317, 0.35012319684028625, 0.2532806098461151, 0.11174144595861435, 0.16388745605945587, 0.11919087171554565, 0.8085679411888123, 0.14679963886737823, 0.4893321096897125, 0.14679963886737823, 0.19573284685611725, 0.04893321171402931, 0.28695762157440186, 0.5739152431488037, 0.7688174843788147, 0.7736717462539673, 0.5771423578262329, 0.7732376456260681, 0.5996308922767639, 0.11992618441581726, 0.11992618441581726, 0.23985236883163452, 0.7728276252746582, 0.1853223592042923, 0.5559670925140381, 0.1853223592042923, 0.5772724747657776, 0.5868206024169922, 0.805219292640686, 0.31139153242111206, 0.1868349313735962, 0.20240449905395508, 0.17749318480491638, 0.12144270539283752, 0.26051414012908936, 0.3126169741153717, 0.15630848705768585, 0.21585457026958466, 0.0595460869371891, 0.7696021199226379, 0.17865444719791412, 0.27610233426094055, 0.16241313517093658, 0.21655084192752838, 0.16241313517093658, 0.25077247619628906, 0.12538623809814453, 0.22987475991249084, 0.261221319437027, 0.12538623809814453, 0.36424463987350464, 0.132452592253685, 0.13981106877326965, 0.15452803671360016, 0.20971661806106567, 0.5771792531013489, 0.5772414207458496, 0.3873240649700165, 0.7892022728919983, 0.577262818813324, 0.7139465808868408, 0.19085446000099182, 0.3619653582572937, 0.15136733651161194, 0.11846139281988144, 0.17111089825630188, 0.507770836353302, 0.1269427090883255, 0.19041405618190765, 0.06347135454416275, 0.06347135454416275, 0.5599063634872437, 0.3836158812046051, 0.5114878416061401, 0.5051197409629822, 0.14431992173194885, 0.14431992173194885, 0.14431992173194885, 0.07215996086597443, 0.6946706771850586, 0.5965768098831177, 0.236842542886734, 0.23092147707939148, 0.20723721385002136, 0.20131616294384003, 0.118421271443367, 0.12315800040960312, 0.20526333153247833, 0.45157933235168457, 0.12315800040960312, 0.08210533112287521, 0.532208263874054, 0.2955275774002075, 0.591055154800415, 0.532189130783081, 0.5322125554084778, 0.14071092009544373, 0.14071092009544373, 0.14071092009544373, 0.1876145601272583, 0.3752291202545166, 0.2929912507534027, 0.5859825015068054, 0.5322225689888, 0.2889273464679718, 0.5778546929359436, 0.6673477292060852, 0.13346955180168152, 0.13346955180168152, 0.13346955180168152, 0.5772501826286316, 0.5847260355949402, 0.2445501983165741, 0.19360224902629852, 0.27511897683143616, 0.16303347051143646, 0.12227509915828705, 0.5964587926864624, 0.40141788125038147, 0.14336353540420532, 0.22938165068626404, 0.12902717292308807, 0.08601811528205872, 0.21169966459274292, 0.14754825830459595, 0.2437753677368164, 0.19886937737464905, 0.19886937737464905, 0.25663232803344727, 0.20163968205451965, 0.12831616401672363, 0.21080511808395386, 0.20163968205451965, 0.5965438485145569, 0.5965206623077393, 0.7457960247993469, 0.1377025842666626, 0.688512921333313, 0.1377025842666626, 0.16622889041900635, 0.16622889041900635, 0.49868667125701904, 0.16622889041900635, 0.2746957242488861, 0.2712620198726654, 0.154516339302063, 0.2128891944885254, 0.08584241569042206, 0.2713311016559601, 0.16958193480968475, 0.1526237428188324, 0.1526237428188324, 0.23741471767425537, 0.3024311065673828, 0.16996127367019653, 0.11997266858816147, 0.18745729327201843, 0.21745045483112335, 0.2831614315509796, 0.2831614315509796, 0.5663228631019592, 0.7449475526809692, 0.5600678324699402, 0.704748272895813, 0.5648109912872314, 0.18827033042907715, 0.18827033042907715, 0.24171699583530426, 0.21271096169948578, 0.28039172291755676, 0.12569284439086914, 0.13536152243614197, 0.5964683294296265, 0.23966658115386963, 0.25298139452934265, 0.31289803981781006, 0.07323145121335983, 0.11983329057693481, 0.30937460064888, 0.17862699925899506, 0.18783457577228546, 0.23018942773342133, 0.09391728788614273, 0.5772078037261963, 0.5414071679115295, 0.18046905100345612, 0.18046905100345612, 0.26336637139320374, 0.16460399329662323, 0.16460399329662323, 0.2962871789932251, 0.11522278934717178, 0.2584953308105469, 0.13786417245864868, 0.13786417245864868, 0.3446604311466217, 0.10339812934398651, 0.5848289728164673, 0.29501378536224365, 0.5900275707244873, 0.5322754979133606, 0.5848134160041809, 0.1643519550561905, 0.26815319061279297, 0.1643519550561905, 0.15570186078548431, 0.24220287799835205, 0.2965185344219208, 0.15814322233200073, 0.23062552511692047, 0.16473251581192017, 0.14496462047100067, 0.5638505816459656, 0.2819252908229828, 0.7548533082008362, 0.15836270153522491, 0.6334508061408997, 0.15836270153522491, 0.15836270153522491, 0.2405547797679901, 0.12027738988399506, 0.31572815775871277, 0.16538141667842865, 0.16538141667842865, 0.294417142868042, 0.588834285736084, 0.14860942959785461, 0.27245059609413147, 0.3219870924949646, 0.14860942959785461, 0.09907294809818268, 0.5772673487663269, 0.2238667756319046, 0.4197502136230469, 0.08395004272460938, 0.19588343799114227, 0.08395004272460938, 0.5847285985946655, 0.2960357069969177, 0.2148646116256714, 0.15279261767864227, 0.23873846232891083, 0.10027015209197998, 0.559964656829834, 0.5599835515022278, 0.3391270339488983, 0.20993578433990479, 0.18167519569396973, 0.13322848081588745, 0.13322848081588745, 0.8386713266372681, 0.5966043472290039, 0.5847236514091492, 0.5966008901596069, 0.7734277248382568, 0.39800143241882324, 0.1364576369524002, 0.1478291004896164, 0.09097175300121307, 0.22742938995361328, 0.2876614034175873, 0.5753228068351746, 0.16362810134887695, 0.16362810134887695, 0.49088430404663086, 0.16362810134887695, 0.2503970265388489, 0.32193902134895325, 0.2027023434638977, 0.11923667788505554, 0.10731300711631775, 0.3019309639930725, 0.23171447217464447, 0.14043301343917847, 0.1544763147830963, 0.17554126679897308, 0.20973537862300873, 0.3207717537879944, 0.18506062030792236, 0.1665545552968979, 0.11720506101846695, 0.5966086983680725, 0.13882333040237427, 0.2540171444416046, 0.2569708526134491, 0.14473070204257965, 0.20380446314811707, 0.5600267052650452, 0.5966651439666748, 0.1906631737947464, 0.2905343770980835, 0.20882157981395721, 0.181583970785141, 0.12710878252983093, 0.25226306915283203, 0.4540735185146332, 0.10090522468090057, 0.10090522468090057, 0.10090522468090057, 0.21822844445705414, 0.32092419266700745, 0.15404361486434937, 0.15404361486434937, 0.15404361486434937, 0.5599439144134521, 0.25660088658332825, 0.13440999388694763, 0.18328633904457092, 0.30547723174095154, 0.10997180640697479, 0.6756045818328857, 0.22520151734352112, 0.22520151734352112, 0.5848133563995361, 0.8384290933609009, 0.5772756934165955, 0.276236891746521, 0.3006107211112976, 0.12186921387910843, 0.1624922901391983, 0.14624306559562683, 0.21988260746002197, 0.17276491224765778, 0.12564720213413239, 0.29841211438179016, 0.18847081065177917, 0.16450418531894684, 0.16450418531894684, 0.4935125708580017, 0.16450418531894684, 0.5373160243034363, 0.26865801215171814, 0.3936452865600586, 0.31150588393211365, 0.224574014544487, 0.15213078260421753, 0.10142052173614502, 0.21008536219596863, 0.2214512974023819, 0.252208411693573, 0.12302849441766739, 0.23375414311885834, 0.17223989963531494, 0.5771815776824951, 0.6444870233535767, 0.21482901275157928, 0.5962429046630859, 0.32667720317840576, 0.20907340943813324, 0.10453670471906662, 0.10453670471906662, 0.23520758748054504, 0.5599241852760315, 0.5322344303131104, 0.5599199533462524, 0.5847960710525513, 0.4240243136882782, 0.2120121568441391, 0.12367375940084457, 0.10600607842206955, 0.12367375940084457, 0.5322214961051941, 0.34225502610206604, 0.16106119751930237, 0.2483026683330536, 0.15435031056404114, 0.08724147826433182, 0.744906485080719, 0.7825068235397339, 0.32125237584114075, 0.2658640444278717, 0.08308251202106476, 0.18555094301700592, 0.14400967955589294, 0.5771737098693848, 0.5847100615501404, 0.31425708532333374, 0.10475236922502518, 0.10475236922502518, 0.10475236922502518, 0.4190094769001007, 0.4160154461860657, 0.21632803976535797, 0.11648432910442352, 0.16640618443489075, 0.08320309221744537, 0.3294017016887665, 0.24513614177703857, 0.19534286856651306, 0.13405883312225342, 0.09958656132221222, 0.2891017198562622, 0.5782034397125244, 0.235286146402359, 0.235286146402359, 0.1045716181397438, 0.2875719368457794, 0.1437859684228897, 0.7026692628860474, 0.5967376828193665, 0.2782101333141327, 0.1284046769142151, 0.1498054563999176, 0.14445525407791138, 0.30496108531951904, 0.5547245144844055, 0.15849271416664124, 0.15849271416664124, 0.11886953562498093, 0.5965408086776733, 0.5848152041435242, 0.7736488580703735, 0.5847925543785095, 0.2859897017478943, 0.5719794034957886, 0.3129984140396118, 0.6259968280792236, 0.1564992070198059, 0.5961616039276123, 0.5772492289543152, 0.1913784146308899, 0.1913784146308899, 0.5741352438926697, 0.2507875859737396, 0.13733604550361633, 0.1970473825931549, 0.1253937929868698, 0.2866143584251404, 0.27506065368652344, 0.16354957222938538, 0.2899287939071655, 0.10407699644565582, 0.16354957222938538, 0.15014049410820007, 0.12511707842350006, 0.17516390979290009, 0.35032781958580017, 0.2001873254776001, 0.6979262828826904, 0.45249882340431213, 0.16968706250190735, 0.11312470585107803, 0.1282079964876175, 0.13574965298175812, 0.5965620875358582, 0.6980361938476562, 0.7088777422904968, 0.1772194355726242, 0.1772194355726242, 0.30686041712760925, 0.22610768675804138, 0.1978442221879959, 0.15746785700321198, 0.11305384337902069, 0.29183751344680786, 0.29183751344680786, 0.5836750268936157, 0.6948215961456299, 0.5771768093109131], \"Term\": [\"10\", \"10\", \"10\", \"10\", \"10\", \"11am\", \"1lb\", \"2\", \"2\", \"2\", \"2\", \"2\", \"22\", \"3\", \"3\", \"3\", \"3\", \"3\", \"300ish\", \"4oz\", \"600\", \"800\", \"80s\", \"abt\", \"abt\", \"account\", \"account\", \"affordable\", \"affordable\", \"affordable\", \"affordable\", \"alley\", \"also\", \"also\", \"also\", \"also\", \"also\", \"always\", \"always\", \"always\", \"always\", \"always\", \"amaze\", \"amaze\", \"amaze\", \"amaze\", \"amaze\", \"amelies\", \"apologized\", \"appetizerslots\", \"area\", \"area\", \"area\", \"area\", \"area\", \"back\", \"back\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"badmouth\", \"bagel\", \"bagel\", \"bagel\", \"bagel\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"barista\", \"barrage\", \"bbq\", \"bbq\", \"bbq\", \"bbq\", \"bbq\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"bellini\", \"benedict\", \"benedict\", \"benedict\", \"benedict\", \"benedict\", \"benedicto\", \"best\", \"best\", \"best\", \"best\", \"best\", \"bestie\", \"big\", \"big\", \"big\", \"big\", \"big\", \"biryani\", \"biryani\", \"boyger\", \"brat\", \"bring\", \"bring\", \"bring\", \"bring\", \"bring\", \"buddy\", \"buddy\", \"burger\", \"burger\", \"burger\", \"burger\", \"burger\", \"canadian\", \"canadian\", \"caring\", \"cavatappi\", \"charcoal\", \"charcoal\", \"charcoal\", \"charcoal\", \"cheesy\", \"cheesy\", \"cheesy\", \"cheung\", \"chi\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"claws\", \"client\", \"come\", \"come\", \"come\", \"come\", \"come\", \"community\", \"community\", \"confidence\", \"could\", \"could\", \"could\", \"could\", \"could\", \"courtney\", \"creamy\", \"creamy\", \"creamy\", \"creamy\", \"creamy\", \"creek\", \"critique\", \"crown\", \"crown\", \"crown\", \"cure\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"cuteyou\", \"dal\", \"dauphine\", \"decorurban\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"devil\", \"devil\", \"devil\", \"didnt\", \"didnt\", \"didnt\", \"didnt\", \"didnt\", \"digger\", \"dimsum\", \"dimsum\", \"disinterested\", \"disinterested\", \"disinterested\", \"divine\", \"divine\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"douche\", \"drape\", \"drape\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"dumpling\", \"dumpling\", \"dumpling\", \"dumpling\", \"dumpling\", \"earlier\", \"earlier\", \"earthy\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"edible\", \"edible\", \"edible\", \"edo\", \"egg\", \"egg\", \"egg\", \"egg\", \"egg\", \"el\", \"el\", \"el\", \"el\", \"employement\", \"eric\", \"eu\", \"even\", \"even\", \"even\", \"even\", \"even\", \"everywhere\", \"everywhere\", \"everywhere\", \"everywhere\", \"exaggeration\", \"expertly\", \"expertly\", \"farm\", \"farm\", \"fashion\", \"fashion\", \"fashion\", \"fashion\", \"favor\", \"favor\", \"felon\", \"filipino\", \"filipino\", \"find\", \"find\", \"find\", \"find\", \"find\", \"findable\", \"first\", \"first\", \"first\", \"first\", \"first\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flooda\", \"fly\", \"fly\", \"fly\", \"fly\", \"food\", \"food\", \"food\", \"food\", \"food\", \"foot\", \"foot\", \"foot\", \"fraud\", \"frites\", \"fruitsthey\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"fuck\", \"fuck\", \"full\", \"full\", \"full\", \"full\", \"full\", \"functionand\", \"fundido\", \"gardein\", \"garden\", \"garden\", \"garden\", \"garden\", \"gathering\", \"ge\", \"ge\", \"gent\", \"get\", \"get\", \"get\", \"get\", \"get\", \"gf\", \"gf\", \"girly\", \"give\", \"give\", \"give\", \"give\", \"give\", \"go\", \"go\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gratuity\", \"great\", \"great\", \"great\", \"great\", \"great\", \"guacamole\", \"guacamole\", \"guacamole\", \"guacamole\", \"h2o\", \"ha\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"hide\", \"hide\", \"hide\", \"home\", \"home\", \"home\", \"home\", \"home\", \"horrible3\", \"howie\", \"howie\", \"im\", \"im\", \"im\", \"im\", \"im\", \"importantly\", \"impossible\", \"impossible\", \"impossible\", \"impossible\", \"informatively\", \"innovative\", \"inside\", \"inside\", \"inside\", \"inside\", \"inside\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"j\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"joeys\", \"joeys\", \"juan\", \"jws\", \"kazushi\", \"kb\", \"kung\", \"kung\", \"kung\", \"kung\", \"kyoto\", \"lake\", \"lake\", \"lake\", \"lane\", \"lap\", \"legged\", \"like\", \"like\", \"like\", \"like\", \"like\", \"little\", \"little\", \"little\", \"little\", \"little\", \"lols\", \"love\", \"love\", \"love\", \"love\", \"love\", \"lunch\", \"lunch\", \"lunch\", \"lunch\", \"lunch\", \"make\", \"make\", \"make\", \"make\", \"make\", \"mama\", \"manchurian\", \"marlin\", \"marsala\", \"masala\", \"master\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"meatball\", \"meatball\", \"meatball\", \"meatball\", \"meatball\", \"meatballsmmmmmmmmmm\", \"meatloaf\", \"meatloaf\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"melissa\", \"meme\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"mexican\", \"mexican\", \"mexican\", \"mexican\", \"mexican\", \"mgmt\", \"miami\", \"miami\", \"michelle\", \"microbrewery\", \"min\", \"min\", \"min\", \"min\", \"min\", \"minor\", \"minor\", \"mist\", \"mmm\", \"mmm\", \"momo\", \"momo\", \"momo\", \"momo\", \"monroeville\", \"nacl\", \"need\", \"need\", \"need\", \"need\", \"need\", \"needs\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"night\", \"night\", \"night\", \"night\", \"night\", \"nikki\", \"nitpick\", \"nondairy\", \"obh\", \"obh\", \"obh\", \"office\", \"office\", \"office\", \"office\", \"one\", \"one\", \"one\", \"one\", \"one\", \"option\", \"option\", \"option\", \"option\", \"option\", \"order\", \"order\", \"order\", \"order\", \"order\", \"oreganos\", \"oreganos\", \"oreganos\", \"pacific\", \"palacio\", \"panera\", \"par\", \"par\", \"par\", \"people\", \"people\", \"people\", \"people\", \"people\", \"peppered\", \"pizza\", \"pizza\", \"pizza\", \"pizza\", \"pizza\", \"place\", \"place\", \"place\", \"place\", \"place\", \"plater\", \"porch\", \"porch\", \"porch\", \"pork\", \"pork\", \"pork\", \"pork\", \"pork\", \"potato\", \"potato\", \"potato\", \"potato\", \"potato\", \"potatoe\", \"ppl\", \"ppl\", \"premium\", \"prepped\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"price\", \"price\", \"price\", \"price\", \"price\", \"prince\", \"prince\", \"princess\", \"quaint\", \"quaint\", \"quaint\", \"quaint\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quarter\", \"quarter\", \"quick\", \"quick\", \"quick\", \"quick\", \"quick\", \"quicker\", \"ramen\", \"ramen\", \"ramen\", \"ramen\", \"ramen\", \"rccf\", \"really\", \"really\", \"really\", \"really\", \"really\", \"recycle\", \"reorder\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"rid\", \"rightthis\", \"robin\", \"roi\", \"rok\", \"roll\", \"roll\", \"roll\", \"roll\", \"roll\", \"rye\", \"rye\", \"sale\", \"sale\", \"sale\", \"sale\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"say\", \"say\", \"say\", \"say\", \"say\", \"schwarma\", \"service\", \"service\", \"service\", \"service\", \"service\", \"serviceseating\", \"shutter\", \"side\", \"side\", \"side\", \"side\", \"side\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"since\", \"since\", \"since\", \"since\", \"since\", \"slowspotty\", \"small\", \"small\", \"small\", \"small\", \"small\", \"soba\", \"soba\", \"soba\", \"specials\", \"speed\", \"spotless\", \"star\", \"star\", \"star\", \"star\", \"star\", \"steak\", \"steak\", \"steak\", \"steak\", \"steak\", \"stomach\", \"stomach\", \"stomach\", \"stomach\", \"stuck\", \"stuck\", \"sumptuous\", \"table\", \"table\", \"table\", \"table\", \"table\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tara\", \"tartare\", \"tartare\", \"teacher\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tepenyaki\", \"terminate\", \"terriblei\", \"texting\", \"thai\", \"thai\", \"thai\", \"thai\", \"thai\", \"theft\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thumb\", \"tibetan\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tney\", \"toke\", \"told\", \"told\", \"told\", \"told\", \"told\", \"top\", \"top\", \"top\", \"top\", \"top\", \"try\", \"try\", \"try\", \"try\", \"try\", \"twist\", \"twist\", \"two\", \"two\", \"two\", \"two\", \"two\", \"unlimited\", \"unusual\", \"us\", \"us\", \"us\", \"us\", \"us\", \"vegan\", \"vegan\", \"vegan\", \"vegan\", \"vegiterian\", \"venture\", \"veteran\", \"vic\", \"vinnies\", \"vinnies\", \"vodka\", \"vodka\", \"vodka\", \"vogue\", \"waaay\", \"waffle\", \"waffle\", \"waffle\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"want\", \"want\", \"want\", \"want\", \"want\", \"wasnt\", \"wasnt\", \"wasnt\", \"wasnt\", \"wasnt\", \"weak\", \"well\", \"well\", \"well\", \"well\", \"well\", \"windfields\", \"windsor\", \"wonton\", \"wonton\", \"wonton\", \"would\", \"would\", \"would\", \"would\", \"would\", \"yelpers\", \"yelpers\", \"yelpers\", \"yesenia\", \"yetyoure\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 2, 3, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el45091105317080247020025307\", ldavis_el45091105317080247020025307_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el45091105317080247020025307\", ldavis_el45091105317080247020025307_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el45091105317080247020025307\", ldavis_el45091105317080247020025307_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(vis_data1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model with Sample Size 10,000 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.16 s, sys: 316 ms, total: 1.47 s\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lda10_000 = LdaMulticore(corpus=bow10_000,\n",
    "                        id2word=id2word10_000,\n",
    "                        num_topics=5,\n",
    "                        workers=5,\n",
    "                        chunksize=2000,\n",
    "                        passes=1,\n",
    "                        batch=False,\n",
    "                        alpha='symmetric',\n",
    "                        eta=None,\n",
    "                        decay=0.5,\n",
    "                        offset=1.0,\n",
    "                        eval_every=None,\n",
    "                        iterations=50,\n",
    "                        gamma_threshold=0.001,\n",
    "                        minimum_probability=0.01,\n",
    "                        random_state=7\n",
    "#                         minimum_phi_value=0.01,\n",
    "#                         per_word_topics=False,\n",
    "#                         dtype=<class 'numpy.float32'>,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('food', 0.011527253),\n",
       "   ('good', 0.010965304),\n",
       "   ('place', 0.009626116),\n",
       "   ('order', 0.008371312),\n",
       "   ('like', 0.008136446),\n",
       "   ('go', 0.0076647433),\n",
       "   ('get', 0.0075358297),\n",
       "   ('great', 0.007339271),\n",
       "   ('one', 0.007181665),\n",
       "   ('make', 0.0070636)]),\n",
       " (1,\n",
       "  [('great', 0.0129324505),\n",
       "   ('food', 0.012647134),\n",
       "   ('place', 0.011795186),\n",
       "   ('service', 0.010658088),\n",
       "   ('come', 0.010007418),\n",
       "   ('good', 0.008636398),\n",
       "   ('like', 0.007939527),\n",
       "   ('get', 0.007889032),\n",
       "   ('go', 0.007082385),\n",
       "   ('order', 0.005842592)]),\n",
       " (2,\n",
       "  [('place', 0.01570077),\n",
       "   ('get', 0.015632696),\n",
       "   ('good', 0.015381798),\n",
       "   ('food', 0.013805398),\n",
       "   ('go', 0.012369021),\n",
       "   ('time', 0.010384177),\n",
       "   ('order', 0.009553988),\n",
       "   ('like', 0.0070229047),\n",
       "   ('great', 0.006190839),\n",
       "   ('try', 0.005451274)]),\n",
       " (3,\n",
       "  [('food', 0.015497324),\n",
       "   ('good', 0.009655477),\n",
       "   ('get', 0.009487278),\n",
       "   ('go', 0.008981699),\n",
       "   ('order', 0.008794666),\n",
       "   ('place', 0.008135009),\n",
       "   ('great', 0.007745277),\n",
       "   ('one', 0.0077106906),\n",
       "   ('service', 0.0075905817),\n",
       "   ('come', 0.0072685676)]),\n",
       " (4,\n",
       "  [('good', 0.01438865),\n",
       "   ('food', 0.010776515),\n",
       "   ('come', 0.010039635),\n",
       "   ('place', 0.008702849),\n",
       "   ('go', 0.007783372),\n",
       "   ('time', 0.006959997),\n",
       "   ('service', 0.0068489057),\n",
       "   ('really', 0.0064866124),\n",
       "   ('get', 0.006191029),\n",
       "   ('like', 0.0059428047)])]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda10_000.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.015497324, 'food'),\n",
       "   (0.009655477, 'good'),\n",
       "   (0.009487278, 'get'),\n",
       "   (0.008981699, 'go'),\n",
       "   (0.008794666, 'order'),\n",
       "   (0.008135009, 'place'),\n",
       "   (0.007745277, 'great'),\n",
       "   (0.0077106906, 'one'),\n",
       "   (0.0075905817, 'service'),\n",
       "   (0.0072685676, 'come'),\n",
       "   (0.006808387, 'would'),\n",
       "   (0.0062794867, 'love'),\n",
       "   (0.0052507725, 'time'),\n",
       "   (0.005134007, 'restaurant'),\n",
       "   (0.005067099, 'like'),\n",
       "   (0.005012744, 'try'),\n",
       "   (0.0047140564, 'take'),\n",
       "   (0.0045579616, 'back'),\n",
       "   (0.0045004115, 'really'),\n",
       "   (0.0044915625, 'say')],\n",
       "  -1.2604571573723808),\n",
       " ([(0.011527253, 'food'),\n",
       "   (0.010965304, 'good'),\n",
       "   (0.009626116, 'place'),\n",
       "   (0.008371312, 'order'),\n",
       "   (0.008136446, 'like'),\n",
       "   (0.0076647433, 'go'),\n",
       "   (0.0075358297, 'get'),\n",
       "   (0.007339271, 'great'),\n",
       "   (0.007181665, 'one'),\n",
       "   (0.0070636, 'make'),\n",
       "   (0.0067745172, 'time'),\n",
       "   (0.006141056, 'try'),\n",
       "   (0.0060359407, 'well'),\n",
       "   (0.0054661785, 'come'),\n",
       "   (0.0050809113, 'restaurant'),\n",
       "   (0.005049891, 'would'),\n",
       "   (0.0049441387, 'back'),\n",
       "   (0.0048457333, 'us'),\n",
       "   (0.0043483963, 'little'),\n",
       "   (0.0042001563, 'say')],\n",
       "  -1.2847889843053866),\n",
       " ([(0.0129324505, 'great'),\n",
       "   (0.012647134, 'food'),\n",
       "   (0.011795186, 'place'),\n",
       "   (0.010658088, 'service'),\n",
       "   (0.010007418, 'come'),\n",
       "   (0.008636398, 'good'),\n",
       "   (0.007939527, 'like'),\n",
       "   (0.007889032, 'get'),\n",
       "   (0.007082385, 'go'),\n",
       "   (0.005842592, 'order'),\n",
       "   (0.005784587, 'restaurant'),\n",
       "   (0.005658776, 'make'),\n",
       "   (0.005447849, 'back'),\n",
       "   (0.005279811, 'one'),\n",
       "   (0.0052730087, 'best'),\n",
       "   (0.005013887, 'us'),\n",
       "   (0.0047464515, 'time'),\n",
       "   (0.004717936, 'really'),\n",
       "   (0.0046982164, 'love'),\n",
       "   (0.004536139, 'nice')],\n",
       "  -1.3250887128529432),\n",
       " ([(0.01570077, 'place'),\n",
       "   (0.015632696, 'get'),\n",
       "   (0.015381798, 'good'),\n",
       "   (0.013805398, 'food'),\n",
       "   (0.012369021, 'go'),\n",
       "   (0.010384177, 'time'),\n",
       "   (0.009553988, 'order'),\n",
       "   (0.0070229047, 'like'),\n",
       "   (0.006190839, 'great'),\n",
       "   (0.005451274, 'try'),\n",
       "   (0.005448505, 'come'),\n",
       "   (0.0052650077, 'back'),\n",
       "   (0.0051435805, 'service'),\n",
       "   (0.00472495, 'chicken'),\n",
       "   (0.0044934363, 'make'),\n",
       "   (0.0041966895, 'really'),\n",
       "   (0.00419085, 'delicious'),\n",
       "   (0.004067209, 'take'),\n",
       "   (0.004041101, 'would'),\n",
       "   (0.004040797, 'look')],\n",
       "  -1.3315136429690595),\n",
       " ([(0.01438865, 'good'),\n",
       "   (0.010776515, 'food'),\n",
       "   (0.010039635, 'come'),\n",
       "   (0.008702849, 'place'),\n",
       "   (0.007783372, 'go'),\n",
       "   (0.006959997, 'time'),\n",
       "   (0.0068489057, 'service'),\n",
       "   (0.0064866124, 'really'),\n",
       "   (0.006191029, 'get'),\n",
       "   (0.0059428047, 'like'),\n",
       "   (0.005818012, 'order'),\n",
       "   (0.0053190636, 'back'),\n",
       "   (0.004890759, 'say'),\n",
       "   (0.0048266477, 'also'),\n",
       "   (0.0047626724, 'fry'),\n",
       "   (0.004649231, 'us'),\n",
       "   (0.004528997, 'great'),\n",
       "   (0.004430557, 'taste'),\n",
       "   (0.004323003, 'one'),\n",
       "   (0.0042061745, 'table')],\n",
       "  -1.3502775507116942)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda10_000.top_topics(bow10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 1.3 s, total: 21.5 s\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vis_data10_000 = pyLDAvis.gensim.prepare(lda10_000, bow10_000, feature_dict10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el4509178814264088748703146\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el4509178814264088748703146_data = {\"mdsDat\": {\"x\": [0.0005621055545722153, -0.0007673157895792822, 0.00846158543431139, 0.010471001481000107, -0.018727376680304438], \"y\": [-0.005250591580152162, -0.00501780551705199, -0.00992968390886919, 0.015801531259424902, 0.004396549746648441], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [24.403594970703125, 22.104421615600586, 21.746112823486328, 21.133438110351562, 10.612434387207031]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [6450.0, 4129.0, 3803.0, 3809.0, 6150.0, 1554.0, 2584.0, 1282.0, 1557.0, 2148.0, 5453.0, 1511.0, 5000.0, 865.0, 4536.0, 986.0, 2155.0, 1281.0, 1829.0, 1154.0, 557.0, 1736.0, 775.0, 2218.0, 3861.0, 1289.0, 7389.0, 1183.0, 1452.0, 2846.0, 5.057967662811279, 2.954915761947632, 2.1791412830352783, 2.178077459335327, 2.1773781776428223, 2.1772921085357666, 2.1752395629882812, 2.1746888160705566, 2.160303831100464, 2.1474437713623047, 2.1348729133605957, 2.115846633911133, 2.8330883979797363, 2.76662540435791, 4.1548237800598145, 2.0737662315368652, 2.0590226650238037, 2.0408668518066406, 2.053682804107666, 2.019118547439575, 2.020754098892212, 2.011791706085205, 2.013932466506958, 1.9696156978607178, 3.3283894062042236, 3.9401025772094727, 1.9624710083007812, 1.957886815071106, 1.950406789779663, 3.2084531784057617, 3.2283661365509033, 2.5515859127044678, 62.7696418762207, 6.461940765380859, 423.3429870605469, 861.2027587890625, 501.9067687988281, 14.244808197021484, 29.881967544555664, 300.2464904785156, 933.739013671875, 552.38330078125, 1057.48583984375, 2125.38671875, 97.7520751953125, 357.5777587890625, 442.78704833984375, 646.5111694335938, 109.68889617919922, 99.4704818725586, 285.0882873535156, 555.739013671875, 419.5369873046875, 342.11572265625, 245.91038513183594, 166.39791870117188, 291.8208312988281, 190.3947296142578, 210.5274200439453, 169.3294677734375, 1206.1480712890625, 1041.013427734375, 131.98306274414062, 161.1329345703125, 416.49493408203125, 538.9873046875, 307.15753173828125, 1231.798828125, 615.9971923828125, 1301.1365966796875, 704.10546875, 461.50177001953125, 996.8506469726562, 449.4284362792969, 1062.2291259765625, 1324.2042236328125, 687.474853515625, 482.9368896484375, 1115.6790771484375, 565.7310791015625, 617.2107543945312, 608.2350463867188, 625.103515625, 720.1193237304688, 694.9293212890625, 448.0211486816406, 456.6695861816406, 2.2719383239746094, 2.2636351585388184, 4.404072284698486, 2.175654649734497, 5.676082134246826, 2.137397289276123, 2.1167922019958496, 2.119055986404419, 2.08506178855896, 2.0241503715515137, 1.9740666151046753, 1.9835375547409058, 1.9632866382598877, 3.912276029586792, 1.9615803956985474, 1.9420008659362793, 1.940083622932434, 1.8784844875335693, 1.8559918403625488, 2.512434720993042, 1.894653081893921, 1.8869292736053467, 1.880666971206665, 3.7733049392700195, 1.2414129972457886, 1.241248369216919, 1.2411190271377563, 1.2406214475631714, 1.2407289743423462, 1.2403560876846313, 1.8571802377700806, 1.8504468202590942, 1.8455067873001099, 7.465155601501465, 2.4090123176574707, 4.100128173828125, 102.50238037109375, 2.9750733375549316, 120.58766174316406, 7.739731311798096, 27.376161575317383, 121.65364074707031, 91.83793640136719, 9.620794296264648, 7.116917610168457, 11.436723709106445, 540.1765747070312, 877.4708862304688, 271.86834716796875, 110.16168975830078, 316.6913146972656, 275.6796569824219, 169.09625244140625, 749.8106079101562, 457.00408935546875, 485.3904724121094, 207.68650817871094, 300.4801940917969, 392.3437194824219, 892.137451171875, 481.801025390625, 240.72959899902344, 762.8684692382812, 1010.7444458007812, 311.78314208984375, 601.9579467773438, 252.2255096435547, 353.06121826171875, 269.0262756347656, 406.2628173828125, 350.21868896484375, 1039.9205322265625, 499.83197021484375, 1362.157470703125, 1431.9652099609375, 339.2983093261719, 631.1727294921875, 841.559814453125, 1195.7977294921875, 911.7159423828125, 627.3192749023438, 952.1475219726562, 521.7615356445312, 936.13330078125, 614.1822509765625, 497.1845703125, 520.39892578125, 451.2315368652344, 679.0322875976562, 506.9867248535156, 432.11260986328125, 459.4997863769531, 3.849804401397705, 2.8379197120666504, 2.061652421951294, 2.0590527057647705, 2.059187889099121, 2.0596489906311035, 2.0596115589141846, 2.057828187942505, 3.41648530960083, 2.7135655879974365, 2.709454298019409, 2.008681535720825, 3.233337879180908, 2.600377321243286, 2.578481435775757, 1.8967328071594238, 1.8873915672302246, 1.8818494081497192, 1.8739299774169922, 1.8259990215301514, 1.8249355554580688, 1.7953424453735352, 1.8241662979125977, 1.7746937274932861, 2.4215636253356934, 1.798914909362793, 1.7709273099899292, 2.995979070663452, 2.391132354736328, 4.205715656280518, 2.374443769454956, 18.363481521606445, 25.786148071289062, 4.004610061645508, 1910.482177734375, 56.384254455566406, 1269.05712890625, 1918.8016357421875, 36.13642501831055, 1511.626220703125, 1879.81982421875, 222.35609436035156, 493.8284606933594, 403.0206604003906, 79.64554595947266, 465.16802978515625, 1167.5992431640625, 512.1665649414062, 187.6731414794922, 577.4392700195312, 290.2948913574219, 204.18272399902344, 252.19305419921875, 1687.1668701171875, 350.3518981933594, 292.40924072265625, 372.5545349121094, 154.13265991210938, 141.96475219726562, 110.6976318359375, 666.203857421875, 470.14453125, 858.2738647460938, 497.05633544921875, 290.87353515625, 415.6588439941406, 643.4400634765625, 353.7765197753906, 330.931640625, 380.83544921875, 549.1458129882812, 756.5865478515625, 512.8801879882812, 665.8654174804688, 628.6004028320312, 492.71875, 453.6131591796875, 409.666259765625, 493.8656311035156, 442.2658386230469, 457.8653259277344, 383.2269287109375, 3.893794536590576, 3.101571559906006, 3.09853458404541, 3.096558094024658, 2.13356876373291, 2.131981372833252, 2.1311943531036377, 2.131927967071533, 2.130180835723877, 2.129239082336426, 2.1283533573150635, 2.12548828125, 2.0894994735717773, 2.0780723094940186, 2.0790634155273438, 2.079503297805786, 2.070852279663086, 2.057732582092285, 2.0439233779907227, 3.9060802459716797, 3.2413032054901123, 1.9084924459457397, 2.5261728763580322, 2.521446943283081, 1.8620045185089111, 2.4520609378814697, 1.8418465852737427, 4.885013580322266, 3.6208362579345703, 1.7914189100265503, 3.4542324542999268, 3.5052011013031006, 1535.954833984375, 104.14905548095703, 1265.83447265625, 25.38140869140625, 626.2620849609375, 114.55487823486328, 143.2699432373047, 401.02789306640625, 301.1553649902344, 490.1158142089844, 1188.5560302734375, 45.26567077636719, 538.745849609375, 157.66455078125, 242.87132263183594, 299.88568115234375, 452.8370056152344, 595.48681640625, 687.0209350585938, 378.465576171875, 1400.884765625, 480.2082824707031, 942.957763671875, 169.19505310058594, 386.093017578125, 125.05175018310547, 212.34815979003906, 1502.0684814453125, 672.0786743164062, 557.9954223632812, 233.7370147705078, 647.0274047851562, 301.7510986328125, 560.3374633789062, 936.9606323242188, 1025.723388671875, 360.28485107421875, 841.1571655273438, 627.0699462890625, 383.0851135253906, 693.9100341796875, 475.8265380859375, 563.7241821289062, 396.26043701171875, 461.03143310546875, 444.1590576171875, 396.3912353515625, 391.2402648925781, 1.7459628582000732, 1.6708940267562866, 1.582938313484192, 2.102257251739502, 2.546154737472534, 2.4718434810638428, 1.4784960746765137, 1.447069525718689, 0.9556549787521362, 0.9555855393409729, 0.9554860591888428, 0.9552783370018005, 0.9551589488983154, 0.9551423788070679, 0.9550870656967163, 0.9550144672393799, 0.9554794430732727, 0.9548393487930298, 0.9549331068992615, 0.9550812840461731, 0.9551558494567871, 0.955066442489624, 0.9548367857933044, 0.9549567699432373, 0.9545345306396484, 0.9548709392547607, 0.9548472762107849, 0.9549838900566101, 0.9548813700675964, 0.9548280835151672, 1.4212095737457275, 1.4219989776611328, 1.3918952941894531, 1.3816111087799072, 131.07070922851562, 1.3546407222747803, 284.0489501953125, 171.8015899658203, 264.2413635253906, 598.7705078125, 858.1486206054688, 113.66329193115234, 223.52676391601562, 144.30856323242188, 386.8658752441406, 1.6998754739761353, 1.345322608947754, 250.85903930664062, 164.37057495117188, 149.98605346679688, 38.785011291503906, 195.02847290039062, 291.6881103515625, 241.24566650390625, 111.4046630859375, 119.47618103027344, 175.74911499023438, 287.8644714355469, 277.2832336425781, 173.68292236328125, 237.42799377441406, 415.0988464355469, 408.4732971191406, 157.04574584960938, 210.30564880371094, 178.96975708007812, 163.8939666748047, 642.718505859375, 317.23248291015625, 464.2054748535156, 519.043701171875, 354.4328308105469, 346.9901123046875, 369.2370910644531, 204.13665771484375, 257.8267517089844, 270.1123962402344, 220.62770080566406, 208.87815856933594, 187.49952697753906, 199.11769104003906, 196.90524291992188], \"Term\": [\"good\", \"come\", \"service\", \"time\", \"place\", \"fry\", \"really\", \"burger\", \"table\", \"us\", \"get\", \"taste\", \"go\", \"rice\", \"great\", \"minute\", \"say\", \"salad\", \"even\", \"bar\", \"item\", \"price\", \"customer\", \"also\", \"like\", \"ask\", \"food\", \"never\", \"sauce\", \"back\", \"rink\", \"tomo\", \"stuffrageous\", \"letchon\", \"lobsicle\", \"asana\", \"chicharrones\", \"ebc\", \"shooter\", \"comedian\", \"440\", \"wikki\", \"mod\", \"pierogis\", \"potatoe\", \"freely\", \"posting\", \"chinise\", \"ofcourse\", \"raspados\", \"smokehouse\", \"marzen\", \"greatthe\", \"shaniac\", \"phil\", \"tortas\", \"aquatic\", \"qf\", \"whosh\", \"gordita\", \"justeat\", \"sopes\", \"japanese\", \"speedy\", \"everything\", \"love\", \"burger\", \"theater\", \"cookie\", \"excellent\", \"would\", \"didnt\", \"one\", \"food\", \"mac\", \"see\", \"cheese\", \"take\", \"store\", \"soft\", \"review\", \"give\", \"fresh\", \"flavor\", \"sweet\", \"bite\", \"beer\", \"clean\", \"start\", \"decent\", \"order\", \"service\", \"strip\", \"everyone\", \"know\", \"wait\", \"wasnt\", \"go\", \"say\", \"get\", \"restaurant\", \"fry\", \"come\", \"taste\", \"great\", \"good\", \"try\", \"price\", \"place\", \"also\", \"really\", \"well\", \"back\", \"time\", \"like\", \"dont\", \"make\", \"do\\u00f1ia\", \"jamie\", \"bebe\", \"sharma\", \"pellet\", \"yesenia\", \"nichols\", \"coffeetea\", \"osaka\", \"sarku\", \"1900\", \"creamsicle\", \"pamelas\", \"indonesia\", \"afghani\", \"goan\", \"clayton\", \"seamus\", \"raquel\", \"allot\", \"bac\", \"sorrell\", \"overabundance\", \"longhorn\", \"battercoated\", \"orderingstopping\", \"jap\", \"allegro\", \"chine\", \"michale\", \"bothered\", \"nickys\", \"monsoon\", \"kofta\", \"maiden\", \"exaggeration\", \"broth\", \"chore\", \"pho\", \"tendon\", \"wonton\", \"curry\", \"empty\", \"boar\", \"fettuccine\", \"alfredo\", \"little\", \"make\", \"soup\", \"late\", \"sure\", \"though\", \"noodle\", \"well\", \"sauce\", \"table\", \"spicy\", \"tell\", \"thing\", \"one\", \"im\", \"sushi\", \"try\", \"like\", \"minute\", \"us\", \"leave\", \"find\", \"seat\", \"think\", \"could\", \"order\", \"even\", \"good\", \"food\", \"night\", \"restaurant\", \"time\", \"place\", \"great\", \"would\", \"go\", \"say\", \"get\", \"back\", \"chicken\", \"also\", \"best\", \"come\", \"really\", \"wait\", \"service\", \"woodlawn\", \"pattersons\", \"mto\", \"ua\", \"wholey\", \"myvegas\", \"un\", \"hashups\", \"darren\", \"lookout\", \"latelier\", \"burning\", \"sumo\", \"fukumimi\", \"independence\", \"javon\", \"drill\", \"weck\", \"sisig\", \"bestie\", \"greatttt\", \"kame\", \"desiree\", \"toxic\", \"dq\", \"catered\", \"dauphine\", \"definitively\", \"dupars\", \"lustre\", \"kibbee\", \"panini\", \"shawarma\", \"lolo\", \"get\", \"takeout\", \"time\", \"place\", \"credit\", \"go\", \"good\", \"coffee\", \"look\", \"dish\", \"waffle\", \"always\", \"order\", \"delicious\", \"stop\", \"chicken\", \"amaze\", \"youre\", \"every\", \"food\", \"people\", \"location\", \"pretty\", \"fast\", \"guy\", \"local\", \"try\", \"menu\", \"like\", \"take\", \"recommend\", \"im\", \"back\", \"know\", \"meal\", \"little\", \"make\", \"great\", \"really\", \"come\", \"service\", \"well\", \"also\", \"even\", \"would\", \"love\", \"one\", \"price\", \"acia\", \"lindsay\", \"soergels\", \"badass\", \"inglewood\", \"santana\", \"christine\", \"placegreat\", \"zoyo\", \"distant\", \"brandi\", \"ginas\", \"poppa\", \"murder\", \"glenshaw\", \"jojos\", \"manwhich\", \"noddle\", \"elenas\", \"oreganos\", \"alicia\", \"ddeokbokki\", \"bingo\", \"rok\", \"brannon\", \"yadda\", \"sl\", \"joeys\", \"throughly\", \"bagolac\", \"pinball\", \"salon\", \"great\", \"game\", \"service\", \"philly\", \"best\", \"bartender\", \"wonderful\", \"bar\", \"area\", \"ive\", \"come\", \"affordable\", \"nice\", \"din\", \"awesome\", \"ever\", \"always\", \"us\", \"restaurant\", \"salad\", \"place\", \"eat\", \"like\", \"wine\", \"friendly\", \"cake\", \"atmosphere\", \"food\", \"make\", \"love\", \"many\", \"back\", \"lunch\", \"really\", \"get\", \"good\", \"pizza\", \"go\", \"one\", \"drink\", \"order\", \"well\", \"time\", \"even\", \"try\", \"would\", \"chicken\", \"also\", \"devein\", \"sexually\", \"tgifridays\", \"venice\", \"petes\", \"tabouleh\", \"strudel\", \"wcs\", \"revenge\", \"attending\", \"horseshoe\", \"misswhich\", \"backis\", \"monsterburgers\", \"steinberg\", \"porkys\", \"existant\", \"col\", \"custardfreaking\", \"liquior\", \"arcos\", \"fontana\", \"stronglly\", \"semilocal\", \"messaged\", \"lobbythe\", \"silvionis\", \"magoo\", \"visittypically\", \"recgonize\", \"multitasker\", \"akita\", \"loeufrier\", \"parchment\", \"item\", \"whitby\", \"fry\", \"rice\", \"taste\", \"come\", \"good\", \"shrimp\", \"burger\", \"customer\", \"really\", \"maharaja\", \"milo\", \"table\", \"minute\", \"around\", \"unfortunately\", \"salad\", \"say\", \"price\", \"taco\", \"5\", \"server\", \"also\", \"us\", \"never\", \"even\", \"time\", \"service\", \"bad\", \"drink\", \"ask\", \"bar\", \"food\", \"back\", \"go\", \"place\", \"like\", \"order\", \"get\", \"menu\", \"one\", \"great\", \"would\", \"make\", \"dont\", \"restaurant\", \"well\"], \"Total\": [6450.0, 4129.0, 3803.0, 3809.0, 6150.0, 1554.0, 2584.0, 1282.0, 1557.0, 2148.0, 5453.0, 1511.0, 5000.0, 865.0, 4536.0, 986.0, 2155.0, 1281.0, 1829.0, 1154.0, 557.0, 1736.0, 775.0, 2218.0, 3861.0, 1289.0, 7389.0, 1183.0, 1452.0, 2846.0, 5.887250900268555, 3.9108974933624268, 2.930312156677246, 2.9299216270446777, 2.930135488510132, 2.930311918258667, 2.9281036853790283, 2.9309287071228027, 2.929309606552124, 2.928466558456421, 2.9266011714935303, 2.9223432540893555, 3.916032552719116, 3.8483774662017822, 5.844169616699219, 2.930495262145996, 2.924769878387451, 2.9023334980010986, 2.9286489486694336, 2.8992042541503906, 2.9181270599365234, 2.9194934368133545, 2.9278526306152344, 2.88843035697937, 4.896450996398926, 5.82752799987793, 2.916447877883911, 2.9255905151367188, 2.917966365814209, 4.818129062652588, 4.888927936553955, 3.8465137481689453, 122.94766998291016, 10.511704444885254, 1012.6094360351562, 2223.912841796875, 1282.05224609375, 25.246753692626953, 57.74945068359375, 753.8583984375, 2719.710693359375, 1538.9219970703125, 3292.38525390625, 7389.3056640625, 224.060546875, 997.2635498046875, 1287.8934326171875, 2001.7620849609375, 261.0778503417969, 234.03338623046875, 792.4137573242188, 1722.7586669921875, 1250.5283203125, 988.669921875, 676.3138427734375, 440.6167297363281, 850.6488037109375, 519.3892822265625, 583.8320922851562, 453.76031494140625, 4454.568359375, 3803.42138671875, 339.97991943359375, 430.2971496582031, 1333.4287109375, 1823.630126953125, 928.0765380859375, 5000.93505859375, 2155.404052734375, 5453.94970703125, 2570.631591796875, 1554.9810791015625, 4129.07470703125, 1511.12353515625, 4536.5986328125, 6450.0537109375, 2770.189453125, 1736.810791015625, 6150.20703125, 2218.847900390625, 2584.281005859375, 2523.49609375, 2846.985595703125, 3809.559326171875, 3861.33837890625, 1667.602294921875, 2764.2431640625, 3.015532970428467, 3.013235092163086, 5.948643684387207, 2.9910056591033936, 7.877982139587402, 3.005258321762085, 2.982485294342041, 2.990858554840088, 3.0071122646331787, 2.9896185398101807, 2.9532408714294434, 3.003901481628418, 2.987333059310913, 5.9714250564575195, 2.9992337226867676, 2.98480224609375, 2.9923577308654785, 2.931185483932495, 2.93178129196167, 3.974271297454834, 2.9989500045776367, 2.9876415729522705, 2.988593339920044, 5.999488353729248, 1.9835877418518066, 1.9833871126174927, 1.9832044839859009, 1.9829856157302856, 1.9831591844558716, 1.9826562404632568, 2.9756412506103516, 2.965183973312378, 2.971914768218994, 12.687209129333496, 3.935302972793579, 6.8650922775268555, 210.0496826171875, 4.928062915802002, 268.0458679199219, 13.891051292419434, 54.96035385131836, 283.266845703125, 210.6962432861328, 17.784570693969727, 12.794569969177246, 21.59601402282715, 1571.46044921875, 2764.2431640625, 771.0634765625, 279.4837341308594, 932.3190307617188, 802.77490234375, 464.29644775390625, 2523.49609375, 1452.865478515625, 1557.195068359375, 601.0740966796875, 923.6475830078125, 1256.020751953125, 3292.38525390625, 1622.1748046875, 723.9874267578125, 2770.189453125, 3861.33837890625, 986.6636962890625, 2148.623291015625, 778.966796875, 1171.150146484375, 849.8871459960938, 1399.826171875, 1177.9873046875, 4454.568359375, 1829.133544921875, 6450.0537109375, 7389.3056640625, 1141.253662109375, 2570.631591796875, 3809.559326171875, 6150.20703125, 4536.5986328125, 2719.710693359375, 5000.93505859375, 2155.404052734375, 5453.94970703125, 2846.985595703125, 2051.58935546875, 2218.847900390625, 1841.4512939453125, 4129.07470703125, 2584.281005859375, 1823.630126953125, 3803.42138671875, 4.6811017990112305, 3.764364004135132, 2.823831796646118, 2.8211843967437744, 2.821552038192749, 2.822244644165039, 2.822509288787842, 2.820915699005127, 4.727333068847656, 3.7589738368988037, 3.782419204711914, 2.827364206314087, 4.676530838012695, 3.7942373752593994, 3.788892984390259, 2.8268420696258545, 2.831695556640625, 2.840700387954712, 2.83889102935791, 2.7960712909698486, 2.8281660079956055, 2.7843494415283203, 2.8410494327545166, 2.784808397293091, 3.8065974712371826, 2.8297383785247803, 2.787203311920166, 4.724366664886475, 3.7883265018463135, 6.674884796142578, 3.781806468963623, 31.95436668395996, 46.735233306884766, 6.571191787719727, 5453.94970703125, 121.84736633300781, 3809.559326171875, 6150.20703125, 76.18357849121094, 5000.93505859375, 6450.0537109375, 608.473388671875, 1576.4932861328125, 1288.541259765625, 201.42213439941406, 1546.0689697265625, 4454.568359375, 1742.4241943359375, 546.2041625976562, 2051.58935546875, 918.7633056640625, 609.5521850585938, 782.7630004882812, 7389.3056640625, 1162.906494140625, 952.7496337890625, 1271.343505859375, 458.5205078125, 415.9698791503906, 309.43133544921875, 2770.189453125, 1819.503173828125, 3861.33837890625, 2001.7620849609375, 1029.5130615234375, 1622.1748046875, 2846.985595703125, 1333.4287109375, 1296.1641845703125, 1571.46044921875, 2764.2431640625, 4536.5986328125, 2584.281005859375, 4129.07470703125, 3803.42138671875, 2523.49609375, 2218.847900390625, 1829.133544921875, 2719.710693359375, 2223.912841796875, 3292.38525390625, 1736.810791015625, 4.800343036651611, 3.8576571941375732, 3.8547451496124268, 3.853346109390259, 2.889322519302368, 2.8877482414245605, 2.8869364261627197, 2.8896987438201904, 2.887331485748291, 2.8861961364746094, 2.8861398696899414, 2.884382724761963, 2.887863874435425, 2.888350486755371, 2.8908069133758545, 2.8924145698547363, 2.888200044631958, 2.88922119140625, 2.889559745788574, 5.764762878417969, 4.840234756469727, 2.8962748050689697, 3.8574864864349365, 3.863874912261963, 2.8926074504852295, 3.82344651222229, 2.877296209335327, 7.692507743835449, 5.706050395965576, 2.827644109725952, 5.607961177825928, 5.734642028808594, 4536.5986328125, 237.8870086669922, 3803.42138671875, 51.22737503051758, 1841.4512939453125, 277.79925537109375, 358.6099548339844, 1154.1085205078125, 835.3170166015625, 1477.2645263671875, 4129.07470703125, 99.54339599609375, 1757.044189453125, 429.00445556640625, 722.186279296875, 927.075927734375, 1546.0689697265625, 2148.623291015625, 2570.631591796875, 1281.6121826171875, 6150.20703125, 1721.825439453125, 3861.33837890625, 501.32708740234375, 1348.8997802734375, 354.3793029785156, 674.18017578125, 7389.3056640625, 2764.2431640625, 2223.912841796875, 771.294189453125, 2846.985595703125, 1087.4852294921875, 2584.281005859375, 5453.94970703125, 6450.0537109375, 1437.221923828125, 5000.93505859375, 3292.38525390625, 1597.659423828125, 4454.568359375, 2523.49609375, 3809.559326171875, 1829.133544921875, 2770.189453125, 2719.710693359375, 2051.58935546875, 2218.847900390625, 2.541609525680542, 2.5566248893737793, 2.570131778717041, 3.4199559688568115, 4.29293155670166, 4.337127685546875, 2.5973665714263916, 2.5946125984191895, 1.7455945014953613, 1.7454833984375, 1.7454758882522583, 1.745272159576416, 1.745098352432251, 1.7450799942016602, 1.7451127767562866, 1.7449913024902344, 1.7458643913269043, 1.744841456413269, 1.745072841644287, 1.7453439235687256, 1.745499610900879, 1.745361089706421, 1.7450073957443237, 1.745247721672058, 1.7445290088653564, 1.7452023029327393, 1.7451865673065186, 1.7454396486282349, 1.7452657222747803, 1.7451696395874023, 2.6122236251831055, 2.6196489334106445, 2.6168625354766846, 2.6257925033569336, 557.046875, 2.637542247772217, 1554.9810791015625, 865.3645629882812, 1511.12353515625, 4129.07470703125, 6450.0537109375, 557.7947387695312, 1282.05224609375, 775.087890625, 2584.281005859375, 3.5031347274780273, 2.6388938426971436, 1557.195068359375, 986.6636962890625, 895.735595703125, 168.80291748046875, 1281.6121826171875, 2155.404052734375, 1736.810791015625, 653.45263671875, 719.188720703125, 1173.782470703125, 2218.847900390625, 2148.623291015625, 1183.988525390625, 1829.133544921875, 3809.559326171875, 3803.42138671875, 1069.364990234375, 1597.659423828125, 1289.2945556640625, 1154.1085205078125, 7389.3056640625, 2846.985595703125, 5000.93505859375, 6150.20703125, 3861.33837890625, 4454.568359375, 5453.94970703125, 1819.503173828125, 3292.38525390625, 4536.5986328125, 2719.710693359375, 2764.2431640625, 1667.602294921875, 2570.631591796875, 2523.49609375], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2585999965667725, 1.1301000118255615, 1.114300012588501, 1.1138999462127686, 1.1134999990463257, 1.1133999824523926, 1.1131999492645264, 1.1119999885559082, 1.1059000492095947, 1.1002000570297241, 1.0950000286102295, 1.087499976158142, 1.0866999626159668, 1.080399990081787, 1.0693000555038452, 1.0645999908447266, 1.059499979019165, 1.0583000183105469, 1.0555000305175781, 1.048699975013733, 1.0429999828338623, 1.038100004196167, 1.0362999439239502, 1.0276000499725342, 1.024399995803833, 1.0190999507904053, 1.014299988746643, 1.0088000297546387, 1.007599949836731, 1.0038000345230103, 0.9954000115394592, 1.0, 0.7382000088691711, 0.9239000082015991, 0.5382999777793884, 0.4616999924182892, 0.4726000130176544, 0.838100016117096, 0.7516000270843506, 0.48980000615119934, 0.34139999747276306, 0.38580000400543213, 0.27469998598098755, 0.16439999639987946, 0.5809999704360962, 0.384799987077713, 0.34279999136924744, 0.2802000045776367, 0.5432999730110168, 0.5547999739646912, 0.3882000148296356, 0.2791000008583069, 0.3183000087738037, 0.3492000102996826, 0.3986999988555908, 0.436599999666214, 0.34060001373291016, 0.40689998865127563, 0.3903999924659729, 0.424699991941452, 0.1039000004529953, 0.11469999700784683, 0.4641999900341034, 0.42820000648498535, 0.2468000054359436, 0.1914999932050705, 0.30469998717308044, 0.00930000003427267, 0.15790000557899475, -0.022700000554323196, 0.11550000309944153, 0.195700004696846, -0.01080000028014183, 0.19779999554157257, -0.0414000004529953, -0.1728000044822693, 0.01679999940097332, 0.13050000369548798, -0.29660001397132874, 0.043800000101327896, -0.02160000056028366, -0.012400000356137753, -0.10570000112056732, -0.25540000200271606, -0.304500013589859, 0.09610000252723694, -0.39010000228881836, 1.226199984550476, 1.2233999967575073, 1.208799958229065, 1.191100001335144, 1.1815999746322632, 1.1685999631881714, 1.1664999723434448, 1.1648000478744507, 1.1432000398635864, 1.1194000244140625, 1.106600046157837, 1.0944000482559204, 1.0895999670028687, 1.0865000486373901, 1.0848000049591064, 1.0795999765396118, 1.0760999917984009, 1.0644999742507935, 1.0521999597549438, 1.0507999658584595, 1.0501999855041504, 1.0499000549316406, 1.0462000370025635, 1.045699954032898, 1.0406999588012695, 1.0406999588012695, 1.0406999588012695, 1.0404000282287598, 1.0404000282287598, 1.0404000282287598, 1.0379999876022339, 1.0378999710083008, 1.0328999757766724, 0.9789999723434448, 1.0185999870300293, 0.9940000176429749, 0.7918999791145325, 1.004699945449829, 0.7106000185012817, 0.9244999885559082, 0.8125, 0.6642000079154968, 0.6790000200271606, 0.8949999809265137, 0.9228000044822693, 0.8737000226974487, 0.4415000081062317, 0.3619000017642975, 0.4668999910354614, 0.5784000158309937, 0.4296000003814697, 0.4406000077724457, 0.4993000030517578, 0.29580000042915344, 0.35280001163482666, 0.34369999170303345, 0.44670000672340393, 0.3864000141620636, 0.3458000123500824, 0.20360000431537628, 0.2953999936580658, 0.4083000123500824, 0.21979999542236328, 0.16910000145435333, 0.35740000009536743, 0.2370000034570694, 0.3817000091075897, 0.31029999256134033, 0.35910001397132874, 0.27230000495910645, 0.2964000105857849, 0.05460000038146973, 0.21209999918937683, -0.04560000076889992, -0.1316000074148178, 0.2964000105857849, 0.10509999841451645, -0.0006000000284984708, -0.1282999962568283, -0.09520000219345093, 0.04259999841451645, -0.1492999941110611, 0.0908999964594841, -0.25290000438690186, -0.024299999698996544, 0.09200000017881393, 0.05920000001788139, 0.1031000018119812, -0.2957000136375427, -0.1193000003695488, 0.06949999928474426, -0.6040999889373779, 1.330199956893921, 1.2431999444961548, 1.2110999822616577, 1.210800051689148, 1.210800051689148, 1.2107000350952148, 1.2106000185012817, 1.2102999687194824, 1.2009999752044678, 1.1999000310897827, 1.1921000480651855, 1.183899998664856, 1.1567000150680542, 1.1478999853134155, 1.1409000158309937, 1.1267000436782837, 1.1201000213623047, 1.1138999462127686, 1.1103999614715576, 1.0995999574661255, 1.0877000093460083, 1.086899995803833, 1.0827000141143799, 1.0751999616622925, 1.0734000205993652, 1.072700023651123, 1.0721999406814575, 1.0702999830245972, 1.065600037574768, 1.0637999773025513, 1.0602999925613403, 0.9718000292778015, 0.9311000108718872, 1.030500054359436, 0.47679999470710754, 0.7552000284194946, 0.42649999260902405, 0.3610000014305115, 0.7799000144004822, 0.3292999863624573, 0.29280000925064087, 0.51910001039505, 0.36500000953674316, 0.3634999990463257, 0.5978999733924866, 0.3246999979019165, 0.1868000030517578, 0.30140000581741333, 0.45739999413490295, 0.257999986410141, 0.3736000061035156, 0.4320000112056732, 0.39309999346733093, 0.04879999905824661, 0.32600000500679016, 0.34450000524520874, 0.29829999804496765, 0.43549999594688416, 0.4507000148296356, 0.49779999256134033, 0.1006999984383583, 0.17249999940395355, 0.021900000050663948, 0.13269999623298645, 0.26179999113082886, 0.16410000622272491, 0.03849999979138374, 0.1988999992609024, 0.16050000488758087, 0.10830000042915344, -0.09040000289678574, -0.2653999924659729, -0.09139999747276306, -0.29899999499320984, -0.274399995803833, -0.10769999772310257, -0.061799999326467514, 0.029500000178813934, -0.18029999732971191, -0.08940000087022781, -0.4471000134944916, 0.014600000344216824, 1.3450000286102295, 1.3361999988555908, 1.3358999490737915, 1.3357000350952148, 1.251099944114685, 1.2509000301361084, 1.2508000135421753, 1.2502000331878662, 1.2502000331878662, 1.250100016593933, 1.2496999502182007, 1.2489999532699585, 1.2307000160217285, 1.225100040435791, 1.2246999740600586, 1.2244000434875488, 1.22160005569458, 1.214900016784668, 1.2080999612808228, 1.1650999784469604, 1.1533000469207764, 1.1371999979019165, 1.13100004196167, 1.127500057220459, 1.113800048828125, 1.1101000308990479, 1.108199954032898, 1.1002000570297241, 1.0994999408721924, 1.0979000329971313, 1.069700002670288, 1.062000036239624, 0.47130000591278076, 0.7282999753952026, 0.45410001277923584, 0.8521000146865845, 0.4758000075817108, 0.6685000061988831, 0.6367999911308289, 0.49729999899864197, 0.5340999960899353, 0.45100000500679016, 0.3089999854564667, 0.7663000226020813, 0.37220001220703125, 0.5533000230789185, 0.46459999680519104, 0.42570000886917114, 0.3264000117778778, 0.2711000144481659, 0.23479999601840973, 0.3346000015735626, 0.07490000128746033, 0.2773999869823456, 0.1446000039577484, 0.46810001134872437, 0.30329999327659607, 0.5127000212669373, 0.39899998903274536, -0.03889999911189079, 0.14020000398159027, 0.17159999907016754, 0.36039999127388, 0.07270000129938126, 0.27230000495910645, 0.025699999183416367, -0.2071000039577484, -0.28439998626708984, 0.17069999873638153, -0.22830000519752502, -0.10400000214576721, 0.12630000710487366, -0.3050000071525574, -0.11400000005960464, -0.3564000129699707, 0.024800000712275505, -0.23890000581741333, -0.25780001282691956, -0.08969999849796295, -0.181099995970726, 1.8676999807357788, 1.8178000450134277, 1.7584999799728394, 1.756500005722046, 1.7208000421524048, 1.680899977684021, 1.6797000169754028, 1.6591999530792236, 1.6406999826431274, 1.6406999826431274, 1.6405999660491943, 1.6404999494552612, 1.6404999494552612, 1.6404000520706177, 1.6404000520706177, 1.6404000520706177, 1.6404000520706177, 1.6403000354766846, 1.6402000188827515, 1.6402000188827515, 1.6402000188827515, 1.6402000188827515, 1.6402000188827515, 1.6402000188827515, 1.6401000022888184, 1.6401000022888184, 1.6401000022888184, 1.6401000022888184, 1.6401000022888184, 1.6401000022888184, 1.6345000267028809, 1.632200002670288, 1.611799955368042, 1.6009999513626099, 0.7961999773979187, 1.576799988746643, 0.5430999994277954, 0.6262999773025513, 0.49939998984336853, 0.31220000982284546, 0.22609999775886536, 0.652400016784668, 0.4964999854564667, 0.5620999932289124, 0.3440000116825104, 1.5199999809265137, 1.5693999528884888, 0.4174000024795532, 0.45089998841285706, 0.4560000002384186, 0.7724000215530396, 0.36039999127388, 0.24310000240802765, 0.26919999718666077, 0.4740000069141388, 0.4481000006198883, 0.3441999852657318, 0.20090000331401825, 0.1956000030040741, 0.3237000107765198, 0.2013999968767166, 0.026399999856948853, 0.011900000274181366, 0.3249000012874603, 0.21539999544620514, 0.2685000002384186, 0.2912999987602234, -0.1988999992609024, 0.04879999905824661, -0.1339000016450882, -0.22910000383853912, -0.14509999752044678, -0.3091999888420105, -0.4494999945163727, 0.05559999868273735, -0.30390000343322754, -0.578000009059906, -0.2687000036239624, -0.33959999680519104, 0.05779999867081642, -0.3149000108242035, -0.3075000047683716], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -10.207799911499023, -10.74530029296875, -11.04990005493164, -11.050399780273438, -11.050700187683105, -11.050700187683105, -11.0516996383667, -11.051899909973145, -11.058500289916992, -11.064499855041504, -11.07040023803711, -11.079299926757812, -10.787400245666504, -10.811200141906738, -10.404500007629395, -11.09939956665039, -11.106599807739258, -11.115400314331055, -11.109199523925781, -11.126099586486816, -11.125300407409668, -11.129799842834473, -11.128700256347656, -11.151000022888184, -10.626299858093262, -10.457599639892578, -11.154600143432617, -11.156900405883789, -11.160799980163574, -10.663000106811523, -10.656800270080566, -10.89210033416748, -7.689300060272217, -9.962900161743164, -5.780600070953369, -5.070499897003174, -5.610400199890137, -9.17240047454834, -8.431500434875488, -6.124199867248535, -4.98960018157959, -5.514599800109863, -4.865099906921387, -4.167099952697754, -7.246399879455566, -5.949399948120117, -5.7357001304626465, -5.3572001457214355, -7.131100177764893, -7.228899955749512, -6.176000118255615, -5.508500099182129, -5.789599895477295, -5.993599891662598, -6.323800086975098, -6.714399814605713, -6.152699947357178, -6.579699993133545, -6.4791998863220215, -6.697000026702881, -4.73360013961792, -4.880799770355225, -6.946100234985352, -6.746600151062012, -5.796899795532227, -5.539100170135498, -6.101399898529053, -4.712600231170654, -5.405600070953369, -4.657800197601318, -5.271900177001953, -5.694300174713135, -4.924200057983398, -5.720799922943115, -4.8607001304626465, -4.640200138092041, -5.29580020904541, -5.648900032043457, -4.811600208282471, -5.490699768066406, -5.403600215911865, -5.4182000160217285, -5.390900135040283, -5.2494001388549805, -5.284999847412109, -5.723999977111816, -5.704800128936768, -10.909199714660645, -10.9128999710083, -10.247300148010254, -10.952500343322754, -9.993599891662598, -10.97029972076416, -10.979900360107422, -10.978899955749512, -10.994999885559082, -11.024700164794922, -11.049699783325195, -11.045000076293945, -11.05519962310791, -10.365699768066406, -11.056099891662598, -11.066100120544434, -11.067099571228027, -11.09939956665039, -11.11139965057373, -10.808600425720215, -11.090800285339355, -11.094900131225586, -11.098199844360352, -10.401900291442871, -11.51360034942627, -11.513699531555176, -11.513799667358398, -11.514200210571289, -11.514100074768066, -11.514399528503418, -11.110799789428711, -11.114399909973145, -11.11709976196289, -9.719599723815918, -10.850600242614746, -10.31879997253418, -7.099999904632568, -10.639599800109863, -6.9375, -9.683500289916992, -8.42020034790039, -6.928699970245361, -7.209799766540527, -9.465900421142578, -9.767399787902832, -9.293000221252441, -5.437900066375732, -4.9527997970581055, -6.124499797821045, -7.027900218963623, -5.97189998626709, -6.110599994659424, -6.599400043487549, -5.110000133514404, -5.605199813842773, -5.544899940490723, -6.393799781799316, -6.024499893188477, -5.757699966430664, -4.936200141906738, -5.552299976348877, -6.246200084686279, -5.092800140380859, -4.811399936676025, -5.987500190734863, -5.329699993133545, -6.19950008392334, -5.8632001876831055, -6.135000228881836, -5.722799777984619, -5.871300220489502, -4.782899856567383, -5.515600204467773, -4.513000011444092, -4.4629998207092285, -5.9029998779296875, -5.282299995422363, -4.99459981918335, -4.6433000564575195, -4.9145002365112305, -5.288400173187256, -4.871099948883057, -5.472599983215332, -4.8881001472473145, -5.309599876403809, -5.520899772644043, -5.475200176239014, -5.6178998947143555, -5.209199905395508, -5.501399993896484, -5.661200046539307, -5.599699974060059, -10.365500450134277, -10.67039966583252, -10.989999771118164, -10.991299629211426, -10.99120044708252, -10.991000175476074, -10.991000175476074, -10.991900444030762, -10.48490047454834, -10.715200424194336, -10.71679973602295, -11.015999794006348, -10.539999961853027, -10.757800102233887, -10.766300201416016, -11.073399543762207, -11.078300476074219, -11.081199645996094, -11.08549976348877, -11.11139965057373, -11.112000465393066, -11.128299713134766, -11.11240005493164, -11.139900207519531, -10.829099655151367, -11.126299858093262, -11.142000198364258, -10.61620044708252, -10.841699600219727, -10.277099609375, -10.848699569702148, -8.803099632263184, -8.463700294494629, -10.32610034942627, -4.158400058746338, -7.681300163269043, -4.567500114440918, -4.1539998054504395, -8.126199722290039, -4.392600059509277, -4.174600124359131, -6.309199810028076, -5.511300086975098, -5.7144999504089355, -7.335899829864502, -5.571100234985352, -4.6508002281188965, -5.474899768829346, -6.478799819946289, -5.354899883270264, -6.042600154876709, -6.394499778747559, -6.183300018310547, -4.282700061798096, -5.854599952697754, -6.035299777984619, -5.793099880218506, -6.6757001876831055, -6.757900238037109, -7.006700038909912, -5.211900234222412, -5.560500144958496, -4.958600044250488, -5.504799842834473, -6.040599822998047, -5.683599948883057, -5.246699810028076, -5.844799995422363, -5.911600112915039, -5.771100044250488, -5.405099868774414, -5.084700107574463, -5.473499774932861, -5.212399959564209, -5.269999980926514, -5.513599872589111, -5.59630012512207, -5.698200225830078, -5.511199951171875, -5.621600151062012, -5.586900234222412, -5.764900207519531, -10.325499534606934, -10.553000450134277, -10.553999900817871, -10.55459976196289, -10.92710018157959, -10.927900314331055, -10.928199768066406, -10.927900314331055, -10.92870044708252, -10.929200172424316, -10.92959976196289, -10.930899620056152, -10.947999954223633, -10.953499794006348, -10.95300006866455, -10.952799797058105, -10.956999778747559, -10.963299751281738, -10.970100402832031, -10.322400093078613, -10.508899688720703, -11.038599967956543, -10.758199691772461, -10.760100364685059, -11.063300132751465, -10.788000106811523, -11.074199676513672, -10.098799705505371, -10.398200035095215, -11.101900100708008, -10.445300102233887, -10.430700302124023, -4.3480000495910645, -7.039100170135498, -4.541399955749512, -8.450900077819824, -5.245200157165527, -6.943900108337402, -6.720200061798096, -5.690899848937988, -5.97730016708374, -5.490300178527832, -4.604400157928467, -7.872399806976318, -5.395699977874756, -6.624499797821045, -6.192399978637695, -5.981500148773193, -5.569399833679199, -5.295499801635742, -5.152599811553955, -5.748799800872803, -4.440100193023682, -5.510700225830078, -4.835899829864502, -6.553899765014648, -5.728799819946289, -6.856200218200684, -6.326700210571289, -4.370299816131592, -5.174499988555908, -5.360599994659424, -6.2307000160217285, -5.212500095367432, -5.975299835205078, -5.356400012969971, -4.842299938201904, -4.751800060272217, -5.797999858856201, -4.950099945068359, -5.243899822235107, -5.736700057983398, -5.142600059509277, -5.519899845123291, -5.350399971008301, -5.702899932861328, -5.551499843597412, -5.588699817657471, -5.702499866485596, -5.71560001373291, -10.438799858093262, -10.48270034790039, -10.536800384521484, -10.253100395202637, -10.06149959564209, -10.091099739074707, -10.60509967803955, -10.62660026550293, -11.041500091552734, -11.041500091552734, -11.041600227355957, -11.041799545288086, -11.041999816894531, -11.041999816894531, -11.041999816894531, -11.042099952697754, -11.041600227355957, -11.0423002243042, -11.042200088500977, -11.042099952697754, -11.041999816894531, -11.042099952697754, -11.0423002243042, -11.042200088500977, -11.04259967803955, -11.0423002243042, -11.0423002243042, -11.042200088500977, -11.0423002243042, -11.0423002243042, -10.644599914550781, -10.644000053405762, -10.665399551391602, -10.672800064086914, -6.1203999519348145, -10.69260025024414, -5.34689998626709, -5.849800109863281, -5.4191999435424805, -4.601200103759766, -4.241300106048584, -6.262899875640869, -5.586599826812744, -6.024099826812744, -5.038000106811523, -10.465499877929688, -10.69950008392334, -5.471199989318848, -5.894000053405762, -5.985599994659424, -7.338099956512451, -5.722899913787842, -5.320400238037109, -5.510300159454346, -6.282899856567383, -6.2129998207092285, -5.827000141143799, -5.333600044250488, -5.371099948883057, -5.838900089263916, -5.526199817657471, -4.967599868774414, -4.983699798583984, -5.939599990844727, -5.647500038146973, -5.808899879455566, -5.896900177001953, -4.530399799346924, -5.236499786376953, -4.855800151824951, -4.744100093841553, -5.1255998611450195, -5.1468000411987305, -5.084700107574463, -5.677299976348877, -5.44379997253418, -5.397299766540527, -5.599599838256836, -5.654300212860107, -5.76230001449585, -5.702199935913086, -5.713399887084961]}, \"token.table\": {\"Topic\": [2, 1, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 1, 4, 2, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 2, 1, 1, 2, 4, 4, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 2, 3, 3, 4, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 2, 2, 3, 3, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 3, 2, 3, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 2, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 4, 1, 2, 3, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 4, 5, 1, 2, 3, 4, 5, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 1, 2, 3, 4, 5, 5, 3, 5, 2, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 1, 5, 1, 2, 3, 4, 5, 5, 1, 2, 5, 3, 4, 5, 4, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 2, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 2, 2, 4, 2, 1, 2, 3, 4, 5, 5, 3, 2, 4, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 4, 5, 1, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 1, 4, 4, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 5, 3, 4, 1, 4, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 3, 4, 3, 1, 2, 3, 4, 5, 3, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 3, 1, 2, 3, 4, 5, 5, 3, 1, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 4, 2, 1, 2, 3, 4, 5, 4], \"Freq\": [0.6772221326828003, 0.6833865642547607, 0.19744469225406647, 0.18214969336986542, 0.17658786475658417, 0.2780911326408386, 0.16546422243118286, 0.8332737684249878, 0.20091739296913147, 0.1406421810388565, 0.1607339233160019, 0.4520641565322876, 0.05022934824228287, 0.6668369770050049, 0.3817305266857147, 0.09260968118906021, 0.5093532800674438, 0.23152419924736023, 0.09260968118906021, 0.046304840594530106, 0.20660154521465302, 0.6198046207427979, 0.5042901039123535, 0.7548553347587585, 0.25508734583854675, 0.2343558520078659, 0.2046106904745102, 0.17621757090091705, 0.1297970861196518, 0.1623472273349762, 0.19274689257144928, 0.30076277256011963, 0.2930011451244354, 0.051097333431243896, 0.20571130514144897, 0.13496403396129608, 0.31564170122146606, 0.2394523173570633, 0.1033998653292656, 0.6857657432556152, 0.5729019045829773, 0.2609787583351135, 0.12091217935085297, 0.16999533772468567, 0.36034223437309265, 0.08739197254180908, 0.2880314290523529, 0.1920209527015686, 0.16522732377052307, 0.18643894791603088, 0.1674601286649704, 0.6825212240219116, 0.29395920038223267, 0.2815493047237396, 0.11944516748189926, 0.16598224639892578, 0.138835608959198, 0.18244381248950958, 0.18392708897590637, 0.22842557728290558, 0.3144559860229492, 0.0904802605509758, 0.5729072093963623, 0.2658593952655792, 0.1024666354060173, 0.23539632558822632, 0.3364782929420471, 0.059541426599025726, 0.333450049161911, 0.666900098323822, 0.21953043341636658, 0.21566671133041382, 0.22585292160511017, 0.22725790739059448, 0.11134584248065948, 0.5730336308479309, 0.25155115127563477, 0.1973133534193039, 0.23471873998641968, 0.16925932466983795, 0.14681610465049744, 0.7785441279411316, 0.7073025703430176, 0.16636216640472412, 0.18715743720531464, 0.15683101117610931, 0.34745433926582336, 0.14210101962089539, 0.2087838500738144, 0.12239053845405579, 0.11879081279039383, 0.4139679968357086, 0.13318970799446106, 0.5041370391845703, 0.6724221706390381, 0.16810554265975952, 0.34326741099357605, 0.17280927300453186, 0.15047338604927063, 0.29036659002304077, 0.043496210128068924, 0.17268988490104675, 0.24491551518440247, 0.1623719334602356, 0.33994925022125244, 0.08037138730287552, 0.7152893543243408, 0.7777084708213806, 0.37674465775489807, 0.16113777458667755, 0.20425915718078613, 0.14525094628334045, 0.1112077608704567, 0.2249140590429306, 0.5622851252555847, 0.1124570295214653, 0.1124570295214653, 0.05622851476073265, 0.6721240282058716, 0.3360620141029358, 0.6929671168327332, 0.6914176940917969, 0.10473712533712387, 0.49036017060279846, 0.0952155664563179, 0.2189958095550537, 0.0952155664563179, 0.3915597200393677, 0.13883990049362183, 0.19265985488891602, 0.10217992216348648, 0.17471987009048462, 0.7073726058006287, 0.2088158130645752, 0.13826993107795715, 0.13544809818267822, 0.3527294099330902, 0.16366644203662872, 0.7067791223526001, 0.3439725637435913, 0.2158563733100891, 0.17237451672554016, 0.15373942255973816, 0.11336341500282288, 0.6830359101295471, 0.1959456503391266, 0.2422512024641037, 0.2812453806400299, 0.19302108883857727, 0.08724942803382874, 0.504245936870575, 0.6891006827354431, 0.20291948318481445, 0.6087584495544434, 0.20291948318481445, 0.6927759051322937, 0.6683692932128906, 0.3341846466064453, 0.3658142387866974, 0.1963844895362854, 0.16942976415157318, 0.18483246862888336, 0.08471488207578659, 0.23008401691913605, 0.1774933785200119, 0.3648475110530853, 0.1051812618970871, 0.12161583453416824, 0.6687043309211731, 0.5731179714202881, 0.241458460688591, 0.16444362699985504, 0.16129522025585175, 0.28795796632766724, 0.14506882429122925, 0.6829512715339661, 0.5194854736328125, 0.12121327221393585, 0.15584564208984375, 0.12121327221393585, 0.08658090978860855, 0.2818366587162018, 0.2971169650554657, 0.130731463432312, 0.1935504674911499, 0.09677523374557495, 0.6658008098602295, 0.18376663327217102, 0.14438806474208832, 0.47254279255867004, 0.1312618851661682, 0.0656309425830841, 0.17298176884651184, 0.4306893050670624, 0.1694515347480774, 0.15180033445358276, 0.07413504272699356, 0.5730419754981995, 0.24384331703186035, 0.12256674468517303, 0.15353097021579742, 0.2941601872444153, 0.18578538298606873, 0.2115357667207718, 0.6346072554588318, 0.7175651788711548, 0.6905422210693359, 0.3724433183670044, 0.18291595578193665, 0.20495401322841644, 0.13002459704875946, 0.11019033193588257, 0.6350057721138, 0.21166858077049255, 0.24506087601184845, 0.19914783537387848, 0.29384347796440125, 0.2123478353023529, 0.049356523901224136, 0.7039652466773987, 0.7869029641151428, 0.35869264602661133, 0.15985216200351715, 0.20338912308216095, 0.17739690840244293, 0.10071985423564911, 0.15151357650756836, 0.20046411454677582, 0.19114020466804504, 0.36829453706741333, 0.09090814739465714, 0.170735701918602, 0.23514963686466217, 0.3127567768096924, 0.21419569849967957, 0.0675182119011879, 0.6929535865783691, 0.2686491906642914, 0.2422640025615692, 0.1816979944705963, 0.1948905885219574, 0.11213704943656921, 0.663232684135437, 0.2627018094062805, 0.525403618812561, 0.706290602684021, 0.22220005095005035, 0.17713412642478943, 0.22908511757850647, 0.23972567915916443, 0.13144227862358093, 0.2639687955379486, 0.5279375910758972, 0.25612351298332214, 0.2363770455121994, 0.17365291714668274, 0.2787739038467407, 0.055754780769348145, 0.6823775768280029, 0.6921469569206238, 0.18510060012340546, 0.43664756417274475, 0.1423850804567337, 0.12814655900001526, 0.10916189104318619, 0.15635818243026733, 0.2733534574508667, 0.22414983808994293, 0.21649594604969025, 0.12956954538822174, 0.24593454599380493, 0.22759732604026794, 0.14561913907527924, 0.32359808683395386, 0.05716899782419205, 0.22228950262069702, 0.16096825897693634, 0.3219365179538727, 0.2312321811914444, 0.06387629359960556, 0.3741600513458252, 0.16035430133342743, 0.209157794713974, 0.1789461076259613, 0.07669118791818619, 0.4177326261997223, 0.15405742824077606, 0.17578347027301788, 0.16492044925689697, 0.08789173513650894, 0.5826578736305237, 0.14566446840763092, 0.14566446840763092, 0.14566446840763092, 0.39795270562171936, 0.17775221168994904, 0.13795694708824158, 0.19897635281085968, 0.0875495970249176, 0.5727821588516235, 0.23772110044956207, 0.14612214267253876, 0.3358628451824188, 0.21591182053089142, 0.06542782485485077, 0.07815815508365631, 0.5471071004867554, 0.07815815508365631, 0.15631631016731262, 0.07815815508365631, 0.2416428029537201, 0.30141308903694153, 0.22968873381614685, 0.14174100756645203, 0.08538614958524704, 0.34591928124427795, 0.24578475952148438, 0.1810513287782669, 0.16689088940620422, 0.05967613682150841, 0.5729473233222961, 0.2875777781009674, 0.19379357993602753, 0.22830291092395782, 0.20326672494411469, 0.08701764792203903, 0.6824784874916077, 0.3358580470085144, 0.23110231757164001, 0.18312260508537292, 0.1807236224412918, 0.06877093017101288, 0.26910820603370667, 0.15123435854911804, 0.19793909788131714, 0.28615912795066833, 0.09489215165376663, 0.2971097230911255, 0.14148081839084625, 0.2282986044883728, 0.1504841446876526, 0.18263888359069824, 0.2635575830936432, 0.7906727194786072, 0.18916544318199158, 0.134517639875412, 0.15973970293998718, 0.43718233704566956, 0.07566617429256439, 0.23854272067546844, 0.1716187447309494, 0.3502049148082733, 0.1718021035194397, 0.06765738874673843, 0.6933892369270325, 0.3227381706237793, 0.21012809872627258, 0.18052440881729126, 0.17878301441669464, 0.10738590359687805, 0.6918483376502991, 0.24635392427444458, 0.19036440551280975, 0.3023434579372406, 0.1681685447692871, 0.0927826464176178, 0.6700611710548401, 0.20526960492134094, 0.2111610323190689, 0.2914704382419586, 0.15906843543052673, 0.13302214443683624, 0.622648298740387, 0.20754943788051605, 0.23409609496593475, 0.2010316699743271, 0.16686511039733887, 0.3385796546936035, 0.05951595678925514, 0.6830945014953613, 0.7071720957756042, 0.15145327150821686, 0.2740583121776581, 0.3413708806037903, 0.12500904500484467, 0.10818091034889221, 0.7089896202087402, 0.5729096531867981, 0.1744571477174759, 0.2971319854259491, 0.25644585490226746, 0.21391035616397858, 0.05856335535645485, 0.7917879819869995, 0.66985684633255, 0.1674642115831375, 0.1674642115831375, 0.6922038197517395, 0.16515666246414185, 0.11848194897174835, 0.2692771553993225, 0.21362654864788055, 0.2351687252521515, 0.16313937306404114, 0.17870868742465973, 0.2409859597682953, 0.3316941559314728, 0.0859697088599205, 0.6637384295463562, 0.5042344331741333, 0.5124131441116333, 0.13827021420001984, 0.17080438137054443, 0.11386958509683609, 0.0650683343410492, 0.7075032591819763, 0.12999661266803741, 0.12999661266803741, 0.12999661266803741, 0.6499831080436707, 0.12999661266803741, 0.691463828086853, 0.6136314868927002, 0.2045438140630722, 0.7183006405830383, 0.5288478136062622, 0.31197768449783325, 0.20773513615131378, 0.26548099517822266, 0.12374114990234375, 0.09074351191520691, 0.07881954312324524, 0.5517367720603943, 0.15763908624649048, 0.07881954312324524, 0.15763908624649048, 0.0894506424665451, 0.393582820892334, 0.1789012849330902, 0.20752549171447754, 0.1323869526386261, 0.7931431531906128, 0.2618853747844696, 0.3235054314136505, 0.24006158113479614, 0.11553765833377838, 0.05905258283019066, 0.6826121211051941, 0.17998941242694855, 0.2618263065814972, 0.22220274806022644, 0.24421584606170654, 0.09167806059122086, 0.7776740789413452, 0.5729529857635498, 0.13936081528663635, 0.343629390001297, 0.24244962632656097, 0.19917777180671692, 0.07508938759565353, 0.5729994773864746, 0.6825622916221619, 0.21652622520923615, 0.13250112533569336, 0.3587225675582886, 0.22298969328403473, 0.06786642968654633, 0.18367889523506165, 0.17213337123394012, 0.30648136138916016, 0.2025715857744217, 0.13539758324623108, 0.3821370005607605, 0.15217939019203186, 0.6087175607681274, 0.15217939019203186, 0.15217939019203186, 0.16668088734149933, 0.6667235493659973, 0.16668088734149933, 0.21820580959320068, 0.21376557648181915, 0.31335368752479553, 0.1769750565290451, 0.07865558564662933, 0.7980901598930359, 0.3871554732322693, 0.12095797806978226, 0.19874879717826843, 0.25090911984443665, 0.04181818664073944, 0.20965802669525146, 0.16368038952350616, 0.22161220014095306, 0.2777049243450165, 0.12689827382564545, 0.14981532096862793, 0.5992612838745117, 0.14981532096862793, 0.437381774187088, 0.218690887093544, 0.08926158398389816, 0.12942929565906525, 0.12496621906757355, 0.5729215741157532, 0.28545862436294556, 0.5709172487258911, 0.5082200765609741, 0.16532553732395172, 0.3172658681869507, 0.19860771298408508, 0.24310451745986938, 0.07560840249061584, 0.6924728155136108, 0.18280962109565735, 0.23856008052825928, 0.2022574543952942, 0.3033861815929413, 0.07260523736476898, 0.685050368309021, 0.28777217864990234, 0.1813041865825653, 0.2553688883781433, 0.22373709082603455, 0.052462488412857056, 0.2286338359117508, 0.18851299583911896, 0.25831228494644165, 0.2121458202600479, 0.11211851984262466, 0.5732206106185913, 0.5043738484382629, 0.3789466619491577, 0.3789466619491577, 0.29391980171203613, 0.3162171542644501, 0.14695990085601807, 0.07702726125717163, 0.16621671617031097, 0.5729765295982361, 0.7660814523696899, 0.6729668378829956, 0.5730396509170532, 0.7082574963569641, 0.3828156292438507, 0.3828156292438507, 0.6924367547035217, 0.7086557745933533, 0.16216373443603516, 0.23226577043533325, 0.20101545751094818, 0.25760385394096375, 0.14696088433265686, 0.22879333794116974, 0.154236301779747, 0.20773524045944214, 0.30676519870758057, 0.10301391780376434, 0.6705816984176636, 0.6744943857192993, 0.21642865240573883, 0.29704177379608154, 0.18838931620121002, 0.18576063215732574, 0.11215735971927643, 0.6922280788421631, 0.14861194789409637, 0.3639915883541107, 0.2627631425857544, 0.15291954576969147, 0.07107528299093246, 0.6829087734222412, 0.32104384899139404, 0.27092820405960083, 0.13910886645317078, 0.19043943285942078, 0.07836263626813889, 0.2707332968711853, 0.2334681898355484, 0.26220273971557617, 0.1557951122522354, 0.0778975561261177, 0.5041880011558533, 0.17346766591072083, 0.6938706636428833, 0.6650899052619934, 0.6692111492156982, 0.3346055746078491, 0.6694934964179993, 0.09388387948274612, 0.06258925795555115, 0.5633032917976379, 0.15647312998771667, 0.09388387948274612, 0.38083741068840027, 0.7969473600387573, 0.7616163492202759, 0.12693606317043304, 0.12693606317043304, 0.18144193291664124, 0.1823018491268158, 0.30097001791000366, 0.2536747455596924, 0.08083195239305496, 0.23294104635715485, 0.23294104635715485, 0.6988231539726257, 0.6126886606216431, 0.20422954857349396, 0.29281219840049744, 0.05856243893504143, 0.11712487787008286, 0.4880203306674957, 0.03904162719845772, 0.1380360722541809, 0.4514152705669403, 0.1044597327709198, 0.2275729924440384, 0.07834479957818985, 0.7795493006706238, 0.17831791937351227, 0.534953773021698, 0.17831791937351227, 0.2483958750963211, 0.22265176475048065, 0.22613069415092468, 0.2504832446575165, 0.05287979543209076, 0.1814573109149933, 0.19446499645709991, 0.31202203035354614, 0.22779721021652222, 0.08438740670681, 0.6921136975288391, 0.6925534009933472, 0.5730687379837036, 0.6838144659996033, 0.6844428181648254, 0.17111070454120636, 0.2485559582710266, 0.17147214710712433, 0.2933904230594635, 0.1620333194732666, 0.12506455183029175, 0.27809593081474304, 0.22454950213432312, 0.22051912546157837, 0.13760854303836823, 0.13876007497310638, 0.6836226582527161, 0.6821791529655457, 0.6898444890975952, 0.2387511283159256, 0.1961860954761505, 0.19850783050060272, 0.2166946977376938, 0.14975151419639587, 0.5730102062225342, 0.29139989614486694, 0.17483994364738464, 0.2826578915119171, 0.13404394686222076, 0.1165599599480629, 0.27386265993118286, 0.2454649657011032, 0.13576430082321167, 0.2672494947910309, 0.07741288095712662, 0.5728707313537598, 0.3596605956554413, 0.20065274834632874, 0.23724979162216187, 0.1261966973543167, 0.07697998732328415, 0.15253686904907227, 0.13404755294322968, 0.271561861038208, 0.24382787942886353, 0.19876016676425934, 0.8492928147315979, 0.25880753993988037, 0.7764226794242859, 0.17790092527866364, 0.20755109190940857, 0.1669771820306778, 0.2949410080909729, 0.15215210616588593, 0.17437879741191864, 0.6975151896476746, 0.6925811767578125, 0.668981671333313, 0.2512276768684387, 0.3145507872104645, 0.13352923095226288, 0.17551521956920624, 0.12595798075199127, 0.285793274641037, 0.24218197166919708, 0.17490920424461365, 0.16191859543323517, 0.13547344505786896, 0.6823177933692932, 0.14590172469615936, 0.3165126144886017, 0.20120318233966827, 0.1870836615562439, 0.14825497567653656, 0.3589823246002197, 0.14038415253162384, 0.16043902933597565, 0.24366678297519684, 0.09726616740226746, 0.5729845762252808, 0.280290424823761, 0.19253993034362793, 0.1737971156835556, 0.2036152333021164, 0.14994260668754578, 0.2737009525299072, 0.12068081647157669, 0.1653774231672287, 0.33285820484161377, 0.10727183520793915, 0.7822813391685486, 0.6924176216125488, 0.6686714291572571, 0.12838280200958252, 0.14977993071079254, 0.5563254356384277, 0.12838280200958252, 0.04279426485300064, 0.6827547550201416, 0.2438172847032547, 0.16852077841758728, 0.22409677505493164, 0.15955689549446106, 0.2043762505054474, 0.5730046629905701, 0.7045004367828369, 0.695097029209137, 0.6853711009025574, 0.7782615423202515, 0.42301657795906067, 0.17091578245162964, 0.13673262298107147, 0.15382421016693115, 0.11536815762519836, 0.7799270153045654, 0.6694243550300598, 0.16081684827804565, 0.35275954008102417, 0.21009942889213562, 0.17897358536720276, 0.0972682535648346, 0.5707923173904419, 0.09513205289840698, 0.09513205289840698, 0.09513205289840698, 0.21960686147212982, 0.34604719281196594, 0.2079610526561737, 0.13642245531082153, 0.08983917534351349, 0.36140528321266174, 0.1610051989555359, 0.1524410843849182, 0.24322061240673065, 0.08221542090177536, 0.5730288624763489, 0.14463455975055695, 0.15195782482624054, 0.34419363737106323, 0.30940812826156616, 0.051262881606817245, 0.4213302731513977, 0.18385320901870728, 0.1340596228837967, 0.18385320901870728, 0.08043577522039413, 0.3882582187652588, 0.13824345171451569, 0.16765695810317993, 0.20295315980911255, 0.10588860511779785, 0.5730634927749634, 0.3850053548812866, 0.6825211644172668, 0.21383371949195862, 0.6415011882781982, 0.17483285069465637, 0.3400123715400696, 0.17912323772907257, 0.18234100937843323, 0.12334833294153214, 0.2707229256629944, 0.3328787088394165, 0.22514203190803528, 0.08149312436580658, 0.08978056162595749, 0.36373645067214966, 0.21439750492572784, 0.22918353974819183, 0.1404673308134079, 0.05175112187862396, 0.1367844045162201, 0.31145745515823364, 0.15733417868614197, 0.2331114560365677, 0.16118726134300232, 0.2305673360824585, 0.461134672164917, 0.2984148859977722, 0.1254872828722, 0.22495892643928528, 0.180579274892807, 0.16986693441867828, 0.3232152462005615, 0.1973261535167694, 0.24828125536441803, 0.13737896084785461, 0.09391725808382034, 0.17234677076339722, 0.21338170766830444, 0.4595913887023926, 0.0984838679432869, 0.04924193397164345, 0.29712989926338196, 0.1952189803123474, 0.17867499589920044, 0.15418991446495056, 0.174704447388649, 0.2035408318042755, 0.32479920983314514, 0.2197807878255844, 0.1721435785293579, 0.0790344700217247, 0.1439775824546814, 0.5759103298187256, 0.0719887912273407, 0.2159663736820221, 0.0719887912273407, 0.7781702280044556, 0.5545267462730408, 0.11882715672254562, 0.15843620896339417, 0.07921810448169708, 0.03960905224084854, 0.16878701746463776, 0.3120967447757721, 0.18152566254138947, 0.22531475126743317, 0.11225929111242294, 0.2043110877275467, 0.29003602266311646, 0.1878804713487625, 0.21359795331954956, 0.10358428955078125, 0.21425682306289673, 0.3438074588775635, 0.21301114559173584, 0.15820126235485077, 0.07100371271371841, 0.17525257170200348, 0.7010102868080139, 0.17525257170200348, 0.1889982372522354, 0.22102293372154236, 0.33310940861701965, 0.14804862439632416, 0.10893648117780685, 0.7670873403549194, 0.6863973736763, 0.171599343419075, 0.171599343419075, 0.7181822657585144, 0.2479974776506424, 0.27543243765830994, 0.2404167652130127, 0.16641460359096527, 0.06967032700777054, 0.7089220881462097, 0.7085893154144287, 0.21326646208763123, 0.1658739149570465, 0.25473493337631226, 0.14217764139175415, 0.2310386598110199, 0.20617853105068207, 0.2801794111728668, 0.10751070082187653, 0.27692151069641113, 0.12891976535320282, 0.29240143299102783, 0.5848028659820557, 0.572978675365448, 0.16383501887321472, 0.2780230641365051, 0.3971758186817169, 0.10922335088253021, 0.054611675441265106, 0.29556432366371155, 0.23689013719558716, 0.19082817435264587, 0.19795681536197662, 0.07896338403224945, 0.3307916820049286, 0.1680895835161209, 0.20256949961185455, 0.1842520385980606, 0.11421471834182739, 0.3854140043258667, 0.3854140043258667, 0.7040517330169678, 0.2409355789422989, 0.29720672965049744, 0.1953638792037964, 0.1886271983385086, 0.07806629687547684, 0.37914085388183594, 0.7088297605514526, 0.6854088306427002, 0.6843823194503784, 0.23138585686683655, 0.2034599781036377, 0.14760822057724, 0.33710527420043945, 0.07978823035955429, 0.1756783425807953, 0.1533699780702591, 0.21471796929836273, 0.39876192808151245, 0.058559443801641464, 0.054584801197052, 0.491263210773468, 0.109169602394104, 0.09097466617822647, 0.23653413355350494, 0.8544996976852417, 0.3434188663959503, 0.2305392324924469, 0.1816369593143463, 0.16325265169143677, 0.08125863969326019, 0.523088276386261, 0.6655002236366272, 0.15421156585216522, 0.19686584174633026, 0.33467191457748413, 0.19686584174633026, 0.1181195005774498, 0.6926811337471008], \"Term\": [\"1900\", \"440\", \"5\", \"5\", \"5\", \"5\", \"5\", \"acia\", \"affordable\", \"affordable\", \"affordable\", \"affordable\", \"affordable\", \"afghani\", \"akita\", \"alfredo\", \"alfredo\", \"alfredo\", \"alfredo\", \"alfredo\", \"alicia\", \"alicia\", \"allegro\", \"allot\", \"also\", \"also\", \"also\", \"also\", \"also\", \"always\", \"always\", \"always\", \"always\", \"always\", \"amaze\", \"amaze\", \"amaze\", \"amaze\", \"amaze\", \"aquatic\", \"arcos\", \"area\", \"area\", \"area\", \"area\", \"area\", \"around\", \"around\", \"around\", \"around\", \"around\", \"asana\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"atmosphere\", \"atmosphere\", \"atmosphere\", \"atmosphere\", \"atmosphere\", \"attending\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"bac\", \"bac\", \"back\", \"back\", \"back\", \"back\", \"back\", \"backis\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"badass\", \"bagolac\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bartender\", \"bartender\", \"bartender\", \"bartender\", \"bartender\", \"battercoated\", \"bebe\", \"bebe\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"best\", \"best\", \"best\", \"best\", \"best\", \"bestie\", \"bingo\", \"bite\", \"bite\", \"bite\", \"bite\", \"bite\", \"boar\", \"boar\", \"boar\", \"boar\", \"boar\", \"bothered\", \"bothered\", \"brandi\", \"brannon\", \"broth\", \"broth\", \"broth\", \"broth\", \"broth\", \"burger\", \"burger\", \"burger\", \"burger\", \"burger\", \"burning\", \"cake\", \"cake\", \"cake\", \"cake\", \"cake\", \"catered\", \"cheese\", \"cheese\", \"cheese\", \"cheese\", \"cheese\", \"chicharrones\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chine\", \"chinise\", \"chore\", \"chore\", \"chore\", \"christine\", \"clayton\", \"clayton\", \"clean\", \"clean\", \"clean\", \"clean\", \"clean\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffeetea\", \"col\", \"come\", \"come\", \"come\", \"come\", \"come\", \"comedian\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"could\", \"could\", \"could\", \"could\", \"could\", \"creamsicle\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"curry\", \"curry\", \"curry\", \"curry\", \"curry\", \"custardfreaking\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"darren\", \"darren\", \"dauphine\", \"ddeokbokki\", \"decent\", \"decent\", \"decent\", \"decent\", \"decent\", \"definitively\", \"definitively\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"desiree\", \"devein\", \"didnt\", \"didnt\", \"didnt\", \"didnt\", \"didnt\", \"din\", \"din\", \"din\", \"din\", \"din\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"distant\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"do\\u00f1ia\", \"dq\", \"dq\", \"drill\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"dupars\", \"dupars\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"ebc\", \"elenas\", \"empty\", \"empty\", \"empty\", \"empty\", \"empty\", \"even\", \"even\", \"even\", \"even\", \"even\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"every\", \"every\", \"every\", \"every\", \"every\", \"everyone\", \"everyone\", \"everyone\", \"everyone\", \"everyone\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything\", \"exaggeration\", \"exaggeration\", \"exaggeration\", \"exaggeration\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"existant\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fettuccine\", \"fettuccine\", \"fettuccine\", \"fettuccine\", \"fettuccine\", \"find\", \"find\", \"find\", \"find\", \"find\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"fontana\", \"food\", \"food\", \"food\", \"food\", \"food\", \"freely\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"fukumimi\", \"fukumimi\", \"game\", \"game\", \"game\", \"game\", \"game\", \"get\", \"get\", \"get\", \"get\", \"get\", \"ginas\", \"give\", \"give\", \"give\", \"give\", \"give\", \"glenshaw\", \"go\", \"go\", \"go\", \"go\", \"go\", \"goan\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gordita\", \"gordita\", \"great\", \"great\", \"great\", \"great\", \"great\", \"greatthe\", \"greatttt\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"hashups\", \"horseshoe\", \"im\", \"im\", \"im\", \"im\", \"im\", \"independence\", \"indonesia\", \"indonesia\", \"indonesia\", \"inglewood\", \"item\", \"item\", \"item\", \"item\", \"item\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"jamie\", \"jap\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"javon\", \"joeys\", \"joeys\", \"joeys\", \"joeys\", \"joeys\", \"jojos\", \"justeat\", \"justeat\", \"kame\", \"kibbee\", \"know\", \"know\", \"know\", \"know\", \"know\", \"kofta\", \"kofta\", \"kofta\", \"kofta\", \"kofta\", \"late\", \"late\", \"late\", \"late\", \"late\", \"latelier\", \"leave\", \"leave\", \"leave\", \"leave\", \"leave\", \"letchon\", \"like\", \"like\", \"like\", \"like\", \"like\", \"lindsay\", \"liquior\", \"little\", \"little\", \"little\", \"little\", \"little\", \"lobbythe\", \"lobsicle\", \"local\", \"local\", \"local\", \"local\", \"local\", \"location\", \"location\", \"location\", \"location\", \"location\", \"loeufrier\", \"lolo\", \"lolo\", \"lolo\", \"lolo\", \"longhorn\", \"longhorn\", \"longhorn\", \"look\", \"look\", \"look\", \"look\", \"look\", \"lookout\", \"love\", \"love\", \"love\", \"love\", \"love\", \"lunch\", \"lunch\", \"lunch\", \"lunch\", \"lunch\", \"lustre\", \"lustre\", \"lustre\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"magoo\", \"maharaja\", \"maharaja\", \"maiden\", \"make\", \"make\", \"make\", \"make\", \"make\", \"manwhich\", \"many\", \"many\", \"many\", \"many\", \"many\", \"marzen\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"messaged\", \"michale\", \"milo\", \"milo\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"misswhich\", \"mod\", \"monsoon\", \"monsterburgers\", \"mto\", \"multitasker\", \"multitasker\", \"murder\", \"myvegas\", \"never\", \"never\", \"never\", \"never\", \"never\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nichols\", \"nickys\", \"night\", \"night\", \"night\", \"night\", \"night\", \"noddle\", \"noodle\", \"noodle\", \"noodle\", \"noodle\", \"noodle\", \"ofcourse\", \"one\", \"one\", \"one\", \"one\", \"one\", \"order\", \"order\", \"order\", \"order\", \"order\", \"orderingstopping\", \"oreganos\", \"oreganos\", \"osaka\", \"overabundance\", \"overabundance\", \"pamelas\", \"panini\", \"panini\", \"panini\", \"panini\", \"panini\", \"parchment\", \"pattersons\", \"pellet\", \"pellet\", \"pellet\", \"people\", \"people\", \"people\", \"people\", \"people\", \"petes\", \"petes\", \"petes\", \"phil\", \"phil\", \"philly\", \"philly\", \"philly\", \"philly\", \"philly\", \"pho\", \"pho\", \"pho\", \"pho\", \"pho\", \"pierogis\", \"pinball\", \"pinball\", \"pinball\", \"pizza\", \"pizza\", \"pizza\", \"pizza\", \"pizza\", \"place\", \"place\", \"place\", \"place\", \"place\", \"placegreat\", \"poppa\", \"porkys\", \"posting\", \"potatoe\", \"potatoe\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"price\", \"price\", \"price\", \"price\", \"price\", \"qf\", \"raquel\", \"raspados\", \"really\", \"really\", \"really\", \"really\", \"really\", \"recgonize\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"revenge\", \"review\", \"review\", \"review\", \"review\", \"review\", \"rice\", \"rice\", \"rice\", \"rice\", \"rice\", \"rink\", \"rok\", \"rok\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"salon\", \"salon\", \"santana\", \"sarku\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"say\", \"say\", \"say\", \"say\", \"say\", \"seamus\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"see\", \"see\", \"see\", \"see\", \"see\", \"semilocal\", \"server\", \"server\", \"server\", \"server\", \"server\", \"service\", \"service\", \"service\", \"service\", \"service\", \"sexually\", \"shaniac\", \"sharma\", \"shawarma\", \"shawarma\", \"shawarma\", \"shawarma\", \"shawarma\", \"shooter\", \"shrimp\", \"shrimp\", \"shrimp\", \"shrimp\", \"shrimp\", \"silvionis\", \"sisig\", \"sl\", \"smokehouse\", \"soergels\", \"soft\", \"soft\", \"soft\", \"soft\", \"soft\", \"sopes\", \"sorrell\", \"soup\", \"soup\", \"soup\", \"soup\", \"soup\", \"speedy\", \"speedy\", \"speedy\", \"speedy\", \"spicy\", \"spicy\", \"spicy\", \"spicy\", \"spicy\", \"start\", \"start\", \"start\", \"start\", \"start\", \"steinberg\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"store\", \"store\", \"store\", \"store\", \"store\", \"strip\", \"strip\", \"strip\", \"strip\", \"strip\", \"stronglly\", \"strudel\", \"stuffrageous\", \"sumo\", \"sumo\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sweet\", \"sweet\", \"sweet\", \"sweet\", \"sweet\", \"table\", \"table\", \"table\", \"table\", \"table\", \"tabouleh\", \"tabouleh\", \"taco\", \"taco\", \"taco\", \"taco\", \"taco\", \"take\", \"take\", \"take\", \"take\", \"take\", \"takeout\", \"takeout\", \"takeout\", \"takeout\", \"takeout\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tendon\", \"tendon\", \"tendon\", \"tendon\", \"tendon\", \"tgifridays\", \"theater\", \"theater\", \"theater\", \"theater\", \"theater\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"though\", \"though\", \"though\", \"though\", \"though\", \"throughly\", \"throughly\", \"throughly\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tomo\", \"tortas\", \"tortas\", \"tortas\", \"toxic\", \"try\", \"try\", \"try\", \"try\", \"try\", \"ua\", \"un\", \"unfortunately\", \"unfortunately\", \"unfortunately\", \"unfortunately\", \"unfortunately\", \"us\", \"us\", \"us\", \"us\", \"us\", \"venice\", \"venice\", \"visittypically\", \"waffle\", \"waffle\", \"waffle\", \"waffle\", \"waffle\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"wasnt\", \"wasnt\", \"wasnt\", \"wasnt\", \"wasnt\", \"wcs\", \"wcs\", \"weck\", \"well\", \"well\", \"well\", \"well\", \"well\", \"whitby\", \"wholey\", \"whosh\", \"wikki\", \"wine\", \"wine\", \"wine\", \"wine\", \"wine\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonton\", \"wonton\", \"wonton\", \"wonton\", \"wonton\", \"woodlawn\", \"would\", \"would\", \"would\", \"would\", \"would\", \"yadda\", \"yesenia\", \"youre\", \"youre\", \"youre\", \"youre\", \"youre\", \"zoyo\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 1, 3, 2, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el4509178814264088748703146\", ldavis_el4509178814264088748703146_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el4509178814264088748703146\", ldavis_el4509178814264088748703146_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el4509178814264088748703146\", ldavis_el4509178814264088748703146_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(vis_data10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model with Sample Size 100,000 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 3.69 s, total: 28.1 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lda100_000 = LdaMulticore(corpus=bow100_000,\n",
    "                        id2word=id2word100_000,\n",
    "                        num_topics=5,\n",
    "                        workers=5,\n",
    "                        chunksize=2000,\n",
    "                        passes=1,\n",
    "                        batch=False,\n",
    "                        alpha='symmetric',\n",
    "                        eta=None,\n",
    "                        decay=0.5,\n",
    "                        offset=1.0,\n",
    "                        eval_every=None,\n",
    "                        iterations=50,\n",
    "                        gamma_threshold=0.001,\n",
    "                        minimum_probability=0.01,\n",
    "                        random_state=7\n",
    "#                         minimum_phi_value=0.01,\n",
    "#                         per_word_topics=False,\n",
    "#                         dtype=<class 'numpy.float32'>,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('food', 0.010301965),\n",
       "   ('like', 0.009722904),\n",
       "   ('good', 0.009251022),\n",
       "   ('order', 0.00888321),\n",
       "   ('place', 0.007920678),\n",
       "   ('come', 0.007660559),\n",
       "   ('restaurant', 0.0073490497),\n",
       "   ('one', 0.007248193),\n",
       "   ('try', 0.0070958445),\n",
       "   ('chicken', 0.006795262)]),\n",
       " (1,\n",
       "  [('go', 0.015959743),\n",
       "   ('place', 0.013255054),\n",
       "   ('order', 0.012505443),\n",
       "   ('get', 0.011103365),\n",
       "   ('time', 0.009243421),\n",
       "   ('good', 0.008764825),\n",
       "   ('food', 0.007326077),\n",
       "   ('back', 0.0072726062),\n",
       "   ('say', 0.0071426025),\n",
       "   ('like', 0.007008652)]),\n",
       " (2,\n",
       "  [('food', 0.022347925),\n",
       "   ('service', 0.013905951),\n",
       "   ('good', 0.011932319),\n",
       "   ('time', 0.010544484),\n",
       "   ('place', 0.009370003),\n",
       "   ('get', 0.009047121),\n",
       "   ('come', 0.008830168),\n",
       "   ('like', 0.008543043),\n",
       "   ('order', 0.007943083),\n",
       "   ('go', 0.007864215)]),\n",
       " (3,\n",
       "  [('good', 0.011593052),\n",
       "   ('place', 0.01114119),\n",
       "   ('food', 0.010100398),\n",
       "   ('fry', 0.010067904),\n",
       "   ('love', 0.009506537),\n",
       "   ('delicious', 0.0076631643),\n",
       "   ('burger', 0.007496068),\n",
       "   ('great', 0.0068189045),\n",
       "   ('really', 0.006469476),\n",
       "   ('like', 0.0062668854)]),\n",
       " (4,\n",
       "  [('food', 0.01760691),\n",
       "   ('good', 0.01679223),\n",
       "   ('great', 0.014988219),\n",
       "   ('get', 0.014139009),\n",
       "   ('place', 0.013486359),\n",
       "   ('come', 0.009202727),\n",
       "   ('go', 0.009009322),\n",
       "   ('service', 0.008096897),\n",
       "   ('order', 0.0071695964),\n",
       "   ('would', 0.00657204)])]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda100_000.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.015959743, 'go'),\n",
       "   (0.013255054, 'place'),\n",
       "   (0.012505443, 'order'),\n",
       "   (0.011103365, 'get'),\n",
       "   (0.009243421, 'time'),\n",
       "   (0.008764825, 'good'),\n",
       "   (0.007326077, 'food'),\n",
       "   (0.0072726062, 'back'),\n",
       "   (0.0071426025, 'say'),\n",
       "   (0.007008652, 'like'),\n",
       "   (0.0067377877, 'great'),\n",
       "   (0.0066067227, 'one'),\n",
       "   (0.006369754, 'take'),\n",
       "   (0.0063180914, 'would'),\n",
       "   (0.0062427013, 'come'),\n",
       "   (0.005867725, 'try'),\n",
       "   (0.0058167162, 'make'),\n",
       "   (0.005640407, 'wait'),\n",
       "   (0.0056004473, 'service'),\n",
       "   (0.0053679347, 'love')],\n",
       "  -1.2207290075104114),\n",
       " ([(0.010301965, 'food'),\n",
       "   (0.009722904, 'like'),\n",
       "   (0.009251022, 'good'),\n",
       "   (0.00888321, 'order'),\n",
       "   (0.007920678, 'place'),\n",
       "   (0.007660559, 'come'),\n",
       "   (0.0073490497, 'restaurant'),\n",
       "   (0.007248193, 'one'),\n",
       "   (0.0070958445, 'try'),\n",
       "   (0.006795262, 'chicken'),\n",
       "   (0.0067417813, 'make'),\n",
       "   (0.0066879326, 'get'),\n",
       "   (0.0057606213, 'well'),\n",
       "   (0.0056054113, 'go'),\n",
       "   (0.0055619352, 'dish'),\n",
       "   (0.0055553825, 'sauce'),\n",
       "   (0.005524473, 'time'),\n",
       "   (0.0047392016, 'taste'),\n",
       "   (0.0044361805, 'also'),\n",
       "   (0.004389773, 'best')],\n",
       "  -1.3713793131916772),\n",
       " ([(0.022347925, 'food'),\n",
       "   (0.013905951, 'service'),\n",
       "   (0.011932319, 'good'),\n",
       "   (0.010544484, 'time'),\n",
       "   (0.009370003, 'place'),\n",
       "   (0.009047121, 'get'),\n",
       "   (0.008830168, 'come'),\n",
       "   (0.008543043, 'like'),\n",
       "   (0.007943083, 'order'),\n",
       "   (0.007864215, 'go'),\n",
       "   (0.0065534962, 'one'),\n",
       "   (0.005971494, 'back'),\n",
       "   (0.0054335645, 'bad'),\n",
       "   (0.005313154, 'us'),\n",
       "   (0.005034437, 'great'),\n",
       "   (0.005013829, 'really'),\n",
       "   (0.0050104987, 'say'),\n",
       "   (0.005009589, 'well'),\n",
       "   (0.0047622602, 'ask'),\n",
       "   (0.004352859, 'ive')],\n",
       "  -1.3737894243807474),\n",
       " ([(0.01760691, 'food'),\n",
       "   (0.01679223, 'good'),\n",
       "   (0.014988219, 'great'),\n",
       "   (0.014139009, 'get'),\n",
       "   (0.013486359, 'place'),\n",
       "   (0.009202727, 'come'),\n",
       "   (0.009009322, 'go'),\n",
       "   (0.008096897, 'service'),\n",
       "   (0.0071695964, 'order'),\n",
       "   (0.00657204, 'would'),\n",
       "   (0.0062749903, 'price'),\n",
       "   (0.005855955, 'really'),\n",
       "   (0.0057790233, 'time'),\n",
       "   (0.0055167717, 'restaurant'),\n",
       "   (0.005387329, 'back'),\n",
       "   (0.005284762, 'pizza'),\n",
       "   (0.005209206, 'one'),\n",
       "   (0.0050664996, 'like'),\n",
       "   (0.0049806004, 'also'),\n",
       "   (0.004839085, 'menu')],\n",
       "  -1.378112544091129),\n",
       " ([(0.011593052, 'good'),\n",
       "   (0.01114119, 'place'),\n",
       "   (0.010100398, 'food'),\n",
       "   (0.010067904, 'fry'),\n",
       "   (0.009506537, 'love'),\n",
       "   (0.0076631643, 'delicious'),\n",
       "   (0.007496068, 'burger'),\n",
       "   (0.0068189045, 'great'),\n",
       "   (0.006469476, 'really'),\n",
       "   (0.0062668854, 'like'),\n",
       "   (0.0062428922, 'chicken'),\n",
       "   (0.005630777, 'make'),\n",
       "   (0.0054417853, 'best'),\n",
       "   (0.005358225, 'go'),\n",
       "   (0.005286052, 'cheese'),\n",
       "   (0.0049318145, 'service'),\n",
       "   (0.0049114143, 'sauce'),\n",
       "   (0.004844892, 'get'),\n",
       "   (0.004746983, 'nice'),\n",
       "   (0.004542216, 'order')],\n",
       "  -1.4215548928805255)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda100_000.top_topics(bow100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model with Sample Size 500,000 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 28s, sys: 44.7 s, total: 5min 13s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lda500_000 = LdaMulticore(corpus=bow500_000,\n",
    "                        id2word=id2word500_000,\n",
    "                        num_topics=5,\n",
    "                        workers=5,\n",
    "                        chunksize=2000,\n",
    "                        passes=1,\n",
    "                        batch=False,\n",
    "                        alpha='symmetric',\n",
    "                        eta=None,\n",
    "                        decay=0.5,\n",
    "                        offset=1.0,\n",
    "                        eval_every=None,\n",
    "                        iterations=50,\n",
    "                        gamma_threshold=0.001,\n",
    "                        minimum_probability=0.01,\n",
    "                        random_state=7\n",
    "#                         minimum_phi_value=0.01,\n",
    "#                         per_word_topics=False,\n",
    "#                         dtype=<class 'numpy.float32'>,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('good', 0.012664083),\n",
       "   ('food', 0.011850028),\n",
       "   ('place', 0.010963864),\n",
       "   ('dish', 0.009526698),\n",
       "   ('like', 0.008637138),\n",
       "   ('restaurant', 0.008554481),\n",
       "   ('chicken', 0.008153795),\n",
       "   ('order', 0.008004137),\n",
       "   ('get', 0.007243999),\n",
       "   ('try', 0.006849711)]),\n",
       " (1,\n",
       "  [('good', 0.01482968),\n",
       "   ('get', 0.010676769),\n",
       "   ('order', 0.009730199),\n",
       "   ('burger', 0.009706478),\n",
       "   ('fry', 0.009368991),\n",
       "   ('like', 0.00907249),\n",
       "   ('cheese', 0.008136843),\n",
       "   ('go', 0.0077758585),\n",
       "   ('come', 0.007360467),\n",
       "   ('chicken', 0.0068593426)]),\n",
       " (2,\n",
       "  [('get', 0.014071949),\n",
       "   ('go', 0.012654565),\n",
       "   ('pizza', 0.010407035),\n",
       "   ('place', 0.009489011),\n",
       "   ('order', 0.0092026265),\n",
       "   ('say', 0.008244008),\n",
       "   ('one', 0.00818839),\n",
       "   ('like', 0.007869867),\n",
       "   ('time', 0.0070830677),\n",
       "   ('food', 0.0067708977)]),\n",
       " (3,\n",
       "  [('food', 0.030980019),\n",
       "   ('service', 0.020788107),\n",
       "   ('time', 0.017565804),\n",
       "   ('great', 0.014718281),\n",
       "   ('come', 0.014308248),\n",
       "   ('order', 0.014292425),\n",
       "   ('good', 0.013576967),\n",
       "   ('go', 0.012996611),\n",
       "   ('place', 0.011202557),\n",
       "   ('get', 0.010887534)]),\n",
       " (4,\n",
       "  [('place', 0.021073477),\n",
       "   ('great', 0.021001229),\n",
       "   ('food', 0.016388262),\n",
       "   ('good', 0.015992481),\n",
       "   ('love', 0.008619233),\n",
       "   ('service', 0.0073167295),\n",
       "   ('go', 0.0070191417),\n",
       "   ('get', 0.006936872),\n",
       "   ('nice', 0.0069094705),\n",
       "   ('like', 0.006366591)])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda500_000.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.014071949, 'get'),\n",
       "   (0.012654565, 'go'),\n",
       "   (0.010407035, 'pizza'),\n",
       "   (0.009489011, 'place'),\n",
       "   (0.0092026265, 'order'),\n",
       "   (0.008244008, 'say'),\n",
       "   (0.00818839, 'one'),\n",
       "   (0.007869867, 'like'),\n",
       "   (0.0070830677, 'time'),\n",
       "   (0.0067708977, 'food'),\n",
       "   (0.00642899, 'would'),\n",
       "   (0.0060832435, 'take'),\n",
       "   (0.005997097, 'back'),\n",
       "   (0.0059471675, 'come'),\n",
       "   (0.0059353495, 'make'),\n",
       "   (0.005674911, 'dont'),\n",
       "   (0.005482478, 'ask'),\n",
       "   (0.005337891, 'us'),\n",
       "   (0.004909312, 'good'),\n",
       "   (0.0048892917, 'even')],\n",
       "  -1.2450687883807496),\n",
       " ([(0.01482968, 'good'),\n",
       "   (0.010676769, 'get'),\n",
       "   (0.009730199, 'order'),\n",
       "   (0.009706478, 'burger'),\n",
       "   (0.009368991, 'fry'),\n",
       "   (0.00907249, 'like'),\n",
       "   (0.008136843, 'cheese'),\n",
       "   (0.0077758585, 'go'),\n",
       "   (0.007360467, 'come'),\n",
       "   (0.0068593426, 'chicken'),\n",
       "   (0.006534986, 'place'),\n",
       "   (0.0063788146, 'food'),\n",
       "   (0.006268972, 'try'),\n",
       "   (0.006090687, 'sauce'),\n",
       "   (0.0058407905, 'taste'),\n",
       "   (0.0056140553, 'one'),\n",
       "   (0.005470538, 'salad'),\n",
       "   (0.0050745793, 'well'),\n",
       "   (0.0049920958, 'really'),\n",
       "   (0.004926455, 'make')],\n",
       "  -1.3552943026943098),\n",
       " ([(0.030980019, 'food'),\n",
       "   (0.020788107, 'service'),\n",
       "   (0.017565804, 'time'),\n",
       "   (0.014718281, 'great'),\n",
       "   (0.014308248, 'come'),\n",
       "   (0.014292425, 'order'),\n",
       "   (0.013576967, 'good'),\n",
       "   (0.012996611, 'go'),\n",
       "   (0.011202557, 'place'),\n",
       "   (0.010887534, 'get'),\n",
       "   (0.010626707, 'us'),\n",
       "   (0.009730888, 'wait'),\n",
       "   (0.009202537, 'back'),\n",
       "   (0.007932236, 'take'),\n",
       "   (0.007671642, 'server'),\n",
       "   (0.007078723, 'restaurant'),\n",
       "   (0.0061816927, 'table'),\n",
       "   (0.0060832985, 'drink'),\n",
       "   (0.0060192053, 'always'),\n",
       "   (0.005807447, 'would')],\n",
       "  -1.4370530402606654),\n",
       " ([(0.012664083, 'good'),\n",
       "   (0.011850028, 'food'),\n",
       "   (0.010963864, 'place'),\n",
       "   (0.009526698, 'dish'),\n",
       "   (0.008637138, 'like'),\n",
       "   (0.008554481, 'restaurant'),\n",
       "   (0.008153795, 'chicken'),\n",
       "   (0.008004137, 'order'),\n",
       "   (0.007243999, 'get'),\n",
       "   (0.006849711, 'try'),\n",
       "   (0.0067712474, 'come'),\n",
       "   (0.0065222746, 'sushi'),\n",
       "   (0.006437327, 'go'),\n",
       "   (0.0061567402, 'roll'),\n",
       "   (0.005996623, 'rice'),\n",
       "   (0.0058679827, 'one'),\n",
       "   (0.0055558616, 'taste'),\n",
       "   (0.0054218103, 'sauce'),\n",
       "   (0.005414816, 'really'),\n",
       "   (0.0054103085, 'also')],\n",
       "  -1.5588446605194908),\n",
       " ([(0.021073477, 'place'),\n",
       "   (0.021001229, 'great'),\n",
       "   (0.016388262, 'food'),\n",
       "   (0.015992481, 'good'),\n",
       "   (0.008619233, 'love'),\n",
       "   (0.0073167295, 'service'),\n",
       "   (0.0070191417, 'go'),\n",
       "   (0.006936872, 'get'),\n",
       "   (0.0069094705, 'nice'),\n",
       "   (0.006366591, 'like'),\n",
       "   (0.006353961, 'beer'),\n",
       "   (0.006340258, 'really'),\n",
       "   (0.006183523, 'friendly'),\n",
       "   (0.006152202, 'bar'),\n",
       "   (0.0060835704, 'menu'),\n",
       "   (0.005838334, 'breakfast'),\n",
       "   (0.0052701174, 'also'),\n",
       "   (0.0051353234, 'well'),\n",
       "   (0.0051317634, 'drink'),\n",
       "   (0.005121707, 'make')],\n",
       "  -1.6593722207075816)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda500_000.top_topics(bow500_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have developed some basic LDA models with various sample sizes, we can move to the extended modeling stage of the project. In the extended modeling stage, we will try to refine one or more LDA models by clarifying a method for evaluating the models and improving the models according to that standard of evaluation.\n",
    "\n",
    "**Ideas for Next Steps**\n",
    "* Develop method for evaluating the lda model results in line with the overarching project goals.\n",
    "* Use bigrams (or ngrams for some n) as features instead of individual tokens.\n",
    "* Use the filter_extremes method to remove infrequent and extremely frequent tokens (see [here](https://radimrehurek.com/gensim/corpora/dictionary.html)).\n",
    "* Try different variations for the lda model parameters (see below).\n",
    "* Construct visualizations of topic models using pyLDAvis, and explore the meaning of these visualizations.\n",
    "\n",
    "**Possible LDA Parameter Variations to Try**\n",
    "* *Sample Size*: [1_000, 10_000, 100_000]\n",
    "* *num_topics*: [5, 10, 25, 50, 100], default=100\n",
    "* *passes*: default=1\n",
    "* *alpha*: ['symmetric', 'auto'], default='symmetric'\n",
    "* *eta*: [None, 'auto'], default=None\n",
    "* *decay*: default=0.5\n",
    "* *offset*: default=1.0\n",
    "* *eval_every*: [None, 10], default=10\n",
    "* *iterations*: default=50\n",
    "* *gamma_threshold*: default=0.001\n",
    "* *minimum_probability*: [0.01, 0.1, 0.25, 0.5, 0.75, 0.9], default=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources Used for Guidance**\n",
    "* https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#sphx-glr-auto-examples-tutorials-run-lda-py\n",
    "* https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n",
    "* https://towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28\n",
    "* https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "* https://simonhessner.de/lemmatize-whole-sentences-with-python-and-nltks-wordnetlemmatizer/\n",
    "* https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "* https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "* https://kite.com/python/docs/nltk.pos_tag\n",
    "* https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "* https://datascience.blog.wzb.eu/2016/06/17/creating-a-sparse-document-term-matrix-for-topic-modeling-via-lda/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
