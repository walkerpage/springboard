{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modeling #\n",
    "\n",
    "We are approaching the problem as a supervised classification problem. We will begin by using Binomial Logistic Regression to construct a model since this algorithm typically performs well as a baseline and is regarded as a good starting point. For a helpful overview of Logistic Regression (as well as recommended additional reading) see [here](https://machinelearningmastery.com/logistic-regression-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data as DataFrame\n",
    "reviews = pd.read_csv('/Users/dwalkerpage/Documents/Data_Science/Springboard/Projects/springboard/Capstone_Projects/Capstone_Project_1/Capstone_Project_1_Data/restaurant_reviews_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clean Corpus of Terms ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4043449, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Reviews: 2644830\n",
      "Rounded Proportion of Positive Reviews: 0.65\n",
      "\n",
      "Number of Negative Reviews: 1398619\n",
      "Rounded Proportion of Negative Reviews: 0.35\n"
     ]
    }
   ],
   "source": [
    "# Identify the numbers and proprotions of the two categories (positive reviews and negative reviews) in our dataset\n",
    "\n",
    "print('Number of Positive Reviews:', len(reviews[reviews.stars > 3.0]))\n",
    "print('Rounded Proportion of Positive Reviews:', round(len(reviews[reviews.stars > 3.0])/len(reviews), 2))\n",
    "print()\n",
    "print('Number of Negative Reviews:', len(reviews[reviews.stars <= 3.0]))\n",
    "print('Rounded Proportion of Negative Reviews:', round(len(reviews[reviews.stars <= 3.0])/len(reviews), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preserve time and computational efficiency, we will work with a large sample of our dataset. Since we are working with a large and random sample, we can be reasonably confident that our results will also apply to the larger dataset. Determining the extent to which our results do in fact extend to the larger dataset could be a fruitful direction for future developments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take large sample from data\n",
    "reviews_sample = reviews.sample(n=1000000, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Reviews: 654478\n",
      "Rounded Proportion of Positive Reviews: 0.65\n",
      "\n",
      "Number of Negative Reviews: 345522\n",
      "Rounded Proportion of Negative Reviews: 0.35\n"
     ]
    }
   ],
   "source": [
    "# Identify the numbers and proprotions of the two categories (positive reviews and negative reviews) in\n",
    "# the sample of our dataset\n",
    "\n",
    "print('Number of Positive Reviews:', len(reviews_sample[reviews_sample.stars > 3.0]))\n",
    "print('Rounded Proportion of Positive Reviews:', round(len(reviews_sample[reviews_sample.stars > 3.0])/len(reviews_sample), 2))\n",
    "print()\n",
    "print('Number of Negative Reviews:', len(reviews_sample[reviews_sample.stars <= 3.0]))\n",
    "print('Rounded Proportion of Negative Reviews:', round(len(reviews_sample[reviews_sample.stars <= 3.0])/len(reviews_sample), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove punctuation from a string\n",
    "# From here: https://stackoverflow.com/questions/33047818/remove-punctuation-for-each-row-in-a-pandas-data-frame?noredirect=1&lq=1\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    '''Removes punctuation from a string'''\n",
    "    s = ''.join([i for i in s if i not in set(string.punctuation)])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLTK's list of stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove stop words, using NLTK's list of stop words\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    '''Removes stopwords from a string'''\n",
    "    s = ' '.join([word for word in s.split() if word not in stop_words])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to clean corpus of review data\n",
    "\n",
    "def clean_corpus(df):\n",
    "    '''Makes review text lowercase, removes punctuation, removes stop words'''\n",
    "    df['text'] = df['text'].str.lower().apply(remove_punctuation).apply(remove_stopwords)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 21s, sys: 2.09 s, total: 7min 23s\n",
      "Wall time: 7min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean corpus of terms in reviews\n",
    "\n",
    "clean_reviews_sample = reviews_sample.copy()\n",
    "clean_reviews_sample = clean_corpus(clean_reviews_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3229215</th>\n",
       "      <td>ja01cHy1xqUB9DQ1r1OYKQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great place!  Awesome atmosphere.  Had the att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54352</th>\n",
       "      <td>wJY74R0zAgjxvBf-d4gm9g</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Me and my fiancé came here for drinks. We were...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020909</th>\n",
       "      <td>r1k3JVOrfF4vJJUWJrF8uQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Their ad says everybody loves Showmars and I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286600</th>\n",
       "      <td>mOMeDQB8NjdBTTzKtikAYg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great place ! Great atmosphere! It's already m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3836550</th>\n",
       "      <td>XcQKsEUlh1W0R4iXbTA1Yg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This really is a perfect little eatery that I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    business_id  stars  \\\n",
       "3229215  ja01cHy1xqUB9DQ1r1OYKQ    5.0   \n",
       "54352    wJY74R0zAgjxvBf-d4gm9g    1.0   \n",
       "3020909  r1k3JVOrfF4vJJUWJrF8uQ    5.0   \n",
       "286600   mOMeDQB8NjdBTTzKtikAYg    5.0   \n",
       "3836550  XcQKsEUlh1W0R4iXbTA1Yg    5.0   \n",
       "\n",
       "                                                      text  \n",
       "3229215  Great place!  Awesome atmosphere.  Had the att...  \n",
       "54352    Me and my fiancé came here for drinks. We were...  \n",
       "3020909  Their ad says everybody loves Showmars and I w...  \n",
       "286600   Great place ! Great atmosphere! It's already m...  \n",
       "3836550  This really is a perfect little eatery that I ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews_sample.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ja01cHy1xqUB9DQ1r1OYKQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>great place awesome atmosphere attic chard fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wJY74R0zAgjxvBf-d4gm9g</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fiancé came drinks seated outside wasnt busy w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r1k3JVOrfF4vJJUWJrF8uQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>ad says everybody loves showmars wholeheartedl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mOMeDQB8NjdBTTzKtikAYg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>great place great atmosphere already second ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XcQKsEUlh1W0R4iXbTA1Yg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>really perfect little eatery love visiting hap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  stars  \\\n",
       "0  ja01cHy1xqUB9DQ1r1OYKQ    5.0   \n",
       "1  wJY74R0zAgjxvBf-d4gm9g    1.0   \n",
       "2  r1k3JVOrfF4vJJUWJrF8uQ    5.0   \n",
       "3  mOMeDQB8NjdBTTzKtikAYg    5.0   \n",
       "4  XcQKsEUlh1W0R4iXbTA1Yg    5.0   \n",
       "\n",
       "                                                text  \n",
       "0  great place awesome atmosphere attic chard fan...  \n",
       "1  fiancé came drinks seated outside wasnt busy w...  \n",
       "2  ad says everybody loves showmars wholeheartedl...  \n",
       "3  great place great atmosphere already second ti...  \n",
       "4  really perfect little eatery love visiting hap...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vectorize Review Text ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 s, sys: 52.9 ms, total: 24.1 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Add column of sentiment labels to DataFrame\n",
    "\n",
    "# Initialize empty column in DataFrame\n",
    "clean_reviews_sample['sentiment_label'] = np.nan\n",
    "\n",
    "# Add sentiment labels to the column\n",
    "for i in range(len(clean_reviews_sample)):\n",
    "    if clean_reviews_sample['stars'].iloc[i] >= 4.0:\n",
    "        clean_reviews_sample.at[i, 'sentiment_label'] = 1\n",
    "    else:\n",
    "        clean_reviews_sample.at[i, 'sentiment_label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews_sample.to_csv('clean_reviews_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews_sample = pd.read_csv('clean_reviews_sample.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ja01cHy1xqUB9DQ1r1OYKQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>great place awesome atmosphere attic chard fan...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wJY74R0zAgjxvBf-d4gm9g</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fiancé came drinks seated outside wasnt busy w...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r1k3JVOrfF4vJJUWJrF8uQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>ad says everybody loves showmars wholeheartedl...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mOMeDQB8NjdBTTzKtikAYg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>great place great atmosphere already second ti...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XcQKsEUlh1W0R4iXbTA1Yg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>really perfect little eatery love visiting hap...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  stars  \\\n",
       "0  ja01cHy1xqUB9DQ1r1OYKQ    5.0   \n",
       "1  wJY74R0zAgjxvBf-d4gm9g    1.0   \n",
       "2  r1k3JVOrfF4vJJUWJrF8uQ    5.0   \n",
       "3  mOMeDQB8NjdBTTzKtikAYg    5.0   \n",
       "4  XcQKsEUlh1W0R4iXbTA1Yg    5.0   \n",
       "\n",
       "                                                text  sentiment_label  \n",
       "0  great place awesome atmosphere attic chard fan...              1.0  \n",
       "1  fiancé came drinks seated outside wasnt busy w...              0.0  \n",
       "2  ad says everybody loves showmars wholeheartedl...              1.0  \n",
       "3  great place great atmosphere already second ti...              1.0  \n",
       "4  really perfect little eatery love visiting hap...              1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews_sample.drop(index=294610, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to construct x & y values for model construction\n",
    "\n",
    "def make_xy(clean_reviews_sample, vectorizer=None):  \n",
    "    '''Make x & y values for model construction'''\n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    x = vectorizer.fit_transform(clean_reviews_sample.text)\n",
    "    x = x.tocsc()  # some versions of sklearn return COO format\n",
    "    y = np.asarray([i for i in clean_reviews_sample['sentiment_label'].values])\n",
    "    return x, y, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.8 s, sys: 1.65 s, total: 44.5 s\n",
      "Wall time: 43.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x, y, vect = make_xy(clean_reviews_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorizer object for later use\n",
    "# See here: https://www.kaggle.com/mattwills8/fit-transform-and-save-tfidfvectorizer\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(vect, open('Yelp_Sentiment_CountVectorizer.sav', 'wb'))\n",
    "\n",
    "# To load vectorizer for future use input:\n",
    "# variable_name = pickle.load(open('Yelp_Sentiment_CountVectorizer.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to construct logistic regression models\n",
    "'''Performs train_test_split, construct LogisticRegression object, and fits the classifier.\n",
    "Allows for specification of stratify in train_test_split, and the values of the C and penalty\n",
    "parameters in the classifer object.'''\n",
    "\n",
    "def logistic_regression_model(features,\n",
    "                              labels,\n",
    "                              test_size=0.25,\n",
    "                              random_state=7,\n",
    "                              stratify=None,\n",
    "                              classifier=LogisticRegression,\n",
    "                              solver='liblinear',\n",
    "                              C=1.0,\n",
    "                              penalty='l2'):\n",
    "    # Split the data into a training and test set\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(features, labels, test_size=test_size, random_state=random_state, stratify=stratify)\n",
    "    \n",
    "    # Construct classifier object\n",
    "    logreg_clf = LogisticRegression(solver=solver, C=C, penalty=penalty)\n",
    "    \n",
    "    # Fit the model on the training data, which trains the model on the training data.\n",
    "    # xtrain data are the features that are being used for the classification.\n",
    "    # ytrain data are the labels used to classify data points.\n",
    "    logreg_clf.fit(xtrain, ytrain)\n",
    "    \n",
    "    return xtrain, xtest, ytrain, ytest, logreg_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Version 1: No Stratify in train_test_split + l2 penalty parameter in model object ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31min 16s, sys: 13.9 s, total: 31min 30s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xtrain1a, xtest1a, ytrain1a, ytest1a, logreg_clf1a = logistic_regression_model(x, y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy score is: 0.9276171428571428\n",
      "The test accuracy score is: 0.8953333333333333\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy scores for the training and testing data. Accuracy is the percentage of correct classifications made.\n",
    "# Predict involves using the model to classify the feature data (xtrain/xtest), which then generates predicted y-values/labels\n",
    "# and accuracy_score compares these predicted y-values against the label data.\n",
    "\n",
    "print('The training accuracy score is: {}'.format(accuracy_score(ytrain1a, logreg_clf1a.predict(xtrain1a))))\n",
    "print ('The test accuracy score is: {}'.format(accuracy_score(ytest1a, logreg_clf1a.predict(xtest1a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.86      0.89    241702\n",
      "         1.0       0.93      0.96      0.95    458298\n",
      "\n",
      "    accuracy                           0.93    700000\n",
      "   macro avg       0.93      0.91      0.92    700000\n",
      "weighted avg       0.93      0.93      0.93    700000\n",
      "\n",
      "\n",
      "Classification Report for Testing Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.81      0.84    103820\n",
      "         1.0       0.91      0.94      0.92    196180\n",
      "\n",
      "    accuracy                           0.90    300000\n",
      "   macro avg       0.89      0.88      0.88    300000\n",
      "weighted avg       0.89      0.90      0.89    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports with multiple performance metrics for training data and test data\n",
    "\n",
    "print('Classification Report for Training Data:')\n",
    "print(classification_report(ytrain1a, logreg_clf1a.predict(xtrain1a)))\n",
    "print()\n",
    "print ('Classification Report for Testing Data:')\n",
    "print(classification_report(ytest1a, logreg_clf1a.predict(xtest1a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs fairly well. Although there is only a 0.03 gap between the training and test accuracy, which indicates that the model is *not* overfitting, it would be interesting to see if we can narrow this gap even more and improve model performance. Accordingly, we will now see if we can tune the regularization parameter $C$ to improve the performance of the model. We will do this using Scikit-Learn's [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) tool. First, however, we will perform k-fold cross-validation on the basic model constructed above without parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate cross-validation score using a specified scoring function\n",
    "\n",
    "def cv_score(clf, x, y, score_func=accuracy_score):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(x): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit model on training data\n",
    "        result += score_func(y[test], clf.predict(x[test])) # evaluate score function on held-out data\n",
    "    return result / nfold # average the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 16min 24s, sys: 44.7 s, total: 2h 17min 9s\n",
      "Wall time: 22min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8938642857142858"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Compute cv_score on the basic logistic regression model used above without tuning the parameter\n",
    "\n",
    "score = cv_score(logreg_clf1a, xtrain1a, ytrain1a)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Scikit-Learn's GridSearchCV tool to find the optimal value for the model parameter\n",
    "\n",
    "#initialize grid of parameter values to search over\n",
    "Cs = np.power(10.0, np.arange(-5, 5))\n",
    "\n",
    "# set up hyperparameter grid for grid search\n",
    "param_grid = {'C': Cs}\n",
    "\n",
    "# instantiate logistic regression classifier: clf\n",
    "gridsearch_logreg1 = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate GridSearchCV object: clf_cv\n",
    "gridsearch_logreg1_cv = GridSearchCV(gridsearch_logreg1, param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17h 1min 11s, sys: 5min 26s, total: 17h 6min 37s\n",
      "Wall time: 2h 51min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='liblinear',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': array([1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02,\n",
       "       1.e+03, 1.e+04])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fit GridSearchCV object to training data\n",
    "gridsearch_logreg1_cv.fit(xtrain1a, ytrain1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameter: {'C': 0.1}\n",
      "Best score is 0.8972485714285714\n"
     ]
    }
   ],
   "source": [
    "# Print the tuned parameter and score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(gridsearch_logreg1_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(gridsearch_logreg1_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch found a value for $C$ with slightly better results than the default $C$ parameter in the LogisticRegression function. We can now use the value for $C$ provided by the GridSearch in our model construction to see how it influences our performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 49s, sys: 2.96 s, total: 4min 52s\n",
      "Wall time: 54.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xtrain1b, xtest1b, ytrain1b, ytest1b, logreg_clf1b = logistic_regression_model(x, y, test_size=0.3, random_state=7, C=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy score is: 0.9094328571428572\n",
      "The test accuracy score is: 0.8981866666666667\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy scores for the training and testing data.\n",
    "\n",
    "print('The training accuracy score is: {}'.format(accuracy_score(ytrain1b, logreg_clf1b.predict(xtrain1b))))\n",
    "print ('The test accuracy score is: {}'.format(accuracy_score(ytest1b, logreg_clf1b.predict(xtest1b))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, having tuned the $C$ parameter, the accuracy scores for the training and test data are even closer: .91 and .90, respectively, as opposed to .93 and .90 previously. We have thus improved the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will see if we can construct an even better Logistic Regression model by varying the stratify parameter in the train_test_split, and the penalty parameter in the classifier object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Version 2: No Stratify in train_test_split + l1 penalty parameter in model object ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 1.23 s, total: 32.3 s\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xtrain2a, xtest2a, ytrain2a, ytest2a, logreg_clf2a = logistic_regression_model(x, y, test_size=0.3, random_state=7, penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy score is: 0.91546\n",
      "The test accuracy score is: 0.8954766666666667\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy scores for the training and testing data.\n",
    "\n",
    "print('The training accuracy score is: {}'.format(accuracy_score(ytrain2a, logreg_clf2a.predict(xtrain2a))))\n",
    "print ('The test accuracy score is: {}'.format(accuracy_score(ytest2a, logreg_clf2a.predict(xtest2a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87    241702\n",
      "         1.0       0.92      0.95      0.94    458298\n",
      "\n",
      "    accuracy                           0.92    700000\n",
      "   macro avg       0.91      0.90      0.90    700000\n",
      "weighted avg       0.92      0.92      0.91    700000\n",
      "\n",
      "\n",
      "Classification Report for Testing Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.81      0.84    103820\n",
      "         1.0       0.90      0.94      0.92    196180\n",
      "\n",
      "    accuracy                           0.90    300000\n",
      "   macro avg       0.89      0.88      0.88    300000\n",
      "weighted avg       0.89      0.90      0.89    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports with multiple performance metrics for training data and test data\n",
    "\n",
    "print('Classification Report for Training Data:')\n",
    "print(classification_report(ytrain2a, logreg_clf2a.predict(xtrain2a)))\n",
    "print()\n",
    "print ('Classification Report for Testing Data:')\n",
    "print(classification_report(ytest2a, logreg_clf2a.predict(xtest2a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the model is slightly improved using the 'l1' penalty rather than the 'l2'. There is only a 0.02 gap between the training and test accuracy scores as opposed to a 0.03 gap. It will still be interesting to see if we can improve this performance by tuning the $C$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Scikit-Learn's GridSearchCV tool to find the optimal value for the model parameter\n",
    "\n",
    "#initialize grid of parameter values to search over\n",
    "Cs = np.power(10.0, np.arange(-5, 5))\n",
    "\n",
    "# set up hyperparameter grid for grid search\n",
    "param_grid = {'C': Cs}\n",
    "\n",
    "# instantiate logistic regression classifier: clf\n",
    "gridsearch_logreg2 = LogisticRegression(solver='liblinear', penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate GridSearchCV object: clf_cv\n",
    "gridsearch_logreg2_cv = GridSearchCV(gridsearch_logreg2, param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 20s, sys: 28.9 s, total: 18min 49s\n",
      "Wall time: 16min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l1',\n",
       "                                          random_state=None, solver='liblinear',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': array([1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02,\n",
       "       1.e+03, 1.e+04])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fit GridSearchCV object to training data\n",
    "gridsearch_logreg2_cv.fit(xtrain2a, ytrain2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameter: {'C': 0.1}\n",
      "Best score is 0.8959185714285715\n"
     ]
    }
   ],
   "source": [
    "# Print the tuned parameter and score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(gridsearch_logreg2_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(gridsearch_logreg2_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 s, sys: 2.54 s, total: 21.6 s\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xtrain2b, xtest2b, ytrain2b, ytest2b, logreg_clf2b = logistic_regression_model(x, y, test_size=0.3, random_state=7, C=0.1, penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy score is: 0.8997585714285714\n",
      "The test accuracy score is: 0.8964666666666666\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy scores for the training and testing data.\n",
    "\n",
    "print('The training accuracy score is: {}'.format(accuracy_score(ytrain2b, logreg_clf2b.predict(xtrain2b))))\n",
    "print ('The test accuracy score is: {}'.format(accuracy_score(ytest2b, logreg_clf2b.predict(xtest2b))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, having tuned the $C$ parameter, the accuracy scores for the training and test data are even closer, and in fact, almost equivalent. Both scores round to .90, and there is only about a .003 difference between them. This is the best model performance so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see how varying the 'stratify' parameter in the train_test_split affects model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Version 3: Stratify in train_test_split + l2 penalty parameter in model object ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34min 15s, sys: 27.4 s, total: 34min 43s\n",
      "Wall time: 6min 48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xtrain3a, xtest3a, ytrain3a, ytest3a, logreg_clf3a = logistic_regression_model(x, y, test_size=0.3, random_state=7, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy score is: 0.9259514285714285\n",
      "The test accuracy score is: 0.89606\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy scores for the training and testing data.\n",
    "\n",
    "print('The training accuracy score is: {}'.format(accuracy_score(ytrain3a, logreg_clf3a.predict(xtrain3a))))\n",
    "print ('The test accuracy score is: {}'.format(accuracy_score(ytest3a, logreg_clf3a.predict(xtest3a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.86      0.89    241865\n",
      "         1.0       0.93      0.96      0.94    458135\n",
      "\n",
      "    accuracy                           0.93    700000\n",
      "   macro avg       0.92      0.91      0.92    700000\n",
      "weighted avg       0.93      0.93      0.93    700000\n",
      "\n",
      "\n",
      "Classification Report for Testing Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.82      0.84    103657\n",
      "         1.0       0.91      0.94      0.92    196343\n",
      "\n",
      "    accuracy                           0.90    300000\n",
      "   macro avg       0.89      0.88      0.88    300000\n",
      "weighted avg       0.90      0.90      0.90    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports with multiple performance metrics for training data and test data\n",
    "\n",
    "print('Classification Report for Training Data:')\n",
    "print(classification_report(ytrain3a, logreg_clf3a.predict(xtrain3a)))\n",
    "print()\n",
    "print ('Classification Report for Testing Data:')\n",
    "print(classification_report(ytest3a, logreg_clf3a.predict(xtest3a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the model peformance is about the same as with the first version, with a 0.03 gap between the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Version 4: Stratify in train_test_split + l1 penalty parameter in model object ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 s, sys: 1.25 s, total: 30.6 s\n",
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xtrain4a, xtest4a, ytrain4a, ytest4a, logreg_clf4a = logistic_regression_model(x, y, test_size=0.3, random_state=7, stratify=y, penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy score is: 0.9152942857142857\n",
      "The test accuracy score is: 0.8960966666666667\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy scores for the training and testing data.\n",
    "\n",
    "print('The training accuracy score is: {}'.format(accuracy_score(ytrain4a, logreg_clf4a.predict(xtrain4a))))\n",
    "print ('The test accuracy score is: {}'.format(accuracy_score(ytest4a, logreg_clf4a.predict(xtest4a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87    241865\n",
      "         1.0       0.92      0.95      0.94    458135\n",
      "\n",
      "    accuracy                           0.92    700000\n",
      "   macro avg       0.91      0.90      0.90    700000\n",
      "weighted avg       0.91      0.92      0.91    700000\n",
      "\n",
      "\n",
      "Classification Report for Testing Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.81      0.84    103657\n",
      "         1.0       0.91      0.94      0.92    196343\n",
      "\n",
      "    accuracy                           0.90    300000\n",
      "   macro avg       0.89      0.88      0.88    300000\n",
      "weighted avg       0.90      0.90      0.90    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports with multiple performance metrics for training data and test data\n",
    "\n",
    "print('Classification Report for Training Data:')\n",
    "print(classification_report(ytrain4a, logreg_clf4a.predict(xtrain4a)))\n",
    "print()\n",
    "print ('Classification Report for Testing Data:')\n",
    "print(classification_report(ytest4a, logreg_clf4a.predict(xtest4a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we find that using 'l1' as the penalty value enables the model to perform better than using 'l2'. Let us see if tuning the $C$ parameter will improve performance more than Version 2 above when we used l1, but did not stratify the train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Scikit-Learn's GridSearchCV tool to find the optimal value for the model parameter\n",
    "\n",
    "#initialize grid of parameter values to search over\n",
    "Cs = np.power(10.0, np.arange(-5, 5))\n",
    "\n",
    "# set up hyperparameter grid for grid search\n",
    "param_grid = {'C': Cs}\n",
    "\n",
    "# instantiate logistic regression classifier: clf\n",
    "gridsearch_logreg4 = LogisticRegression(solver='liblinear', penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate GridSearchCV object: clf_cv\n",
    "gridsearch_logreg4_cv = GridSearchCV(gridsearch_logreg4, param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 35s, sys: 31.7 s, total: 20min 6s\n",
      "Wall time: 18min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l1',\n",
       "                                          random_state=None, solver='liblinear',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': array([1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02,\n",
       "       1.e+03, 1.e+04])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fit GridSearchCV object to training data\n",
    "gridsearch_logreg4_cv.fit(xtrain4a, ytrain4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameter: {'C': 0.1}\n",
      "Best score is 0.8954414285714286\n"
     ]
    }
   ],
   "source": [
    "# Print the tuned parameter and score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(gridsearch_logreg4_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(gridsearch_logreg4_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.3 s, sys: 1.29 s, total: 21.6 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xtrain4b, xtest4b, ytrain4b, ytest4b, logreg_clf4b = logistic_regression_model(x, y, test_size=0.3, random_state=7, stratify=y, C=0.1, penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy score is: 0.8995841422630604\n",
      "The test accuracy score is: 0.8967866666666666\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy scores for the training and testing data.\n",
    "\n",
    "print('The training accuracy score is: {}'.format(accuracy_score(ytrain4b, logreg_clf4b.predict(xtrain4b))))\n",
    "print ('The test accuracy score is: {}'.format(accuracy_score(ytest4b, logreg_clf4b.predict(xtest4b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy score is: 0.8997585714285714\n",
      "The test accuracy score is: 0.8964666666666666\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy scores for the training and testing data.\n",
    "\n",
    "print('The training accuracy score is: {}'.format(accuracy_score(ytrain2b, logreg_clf2b.predict(xtrain2b))))\n",
    "print ('The test accuracy score is: {}'.format(accuracy_score(ytest2b, logreg_clf2b.predict(xtest2b))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gap between the training and test accuracy scores is even smaller than the gap between the scores in Version 2 of our model. Version 4, therefore, performs the best of the variations we have tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.81      0.85    241865\n",
      "         1.0       0.90      0.95      0.93    458134\n",
      "\n",
      "    accuracy                           0.90    699999\n",
      "   macro avg       0.90      0.88      0.89    699999\n",
      "weighted avg       0.90      0.90      0.90    699999\n",
      "\n",
      "\n",
      "Classification Report for Testing Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.81      0.84    103656\n",
      "         1.0       0.90      0.95      0.92    196344\n",
      "\n",
      "    accuracy                           0.90    300000\n",
      "   macro avg       0.89      0.88      0.88    300000\n",
      "weighted avg       0.90      0.90      0.90    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports with multiple performance metrics for training data and test data\n",
    "\n",
    "print('Classification Report for Training Data:')\n",
    "print(classification_report(ytrain4b, logreg_clf4b.predict(xtrain4b)))\n",
    "print()\n",
    "print ('Classification Report for Testing Data:')\n",
    "print(classification_report(ytest4b, logreg_clf4b.predict(xtest4b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final tuned model for later use\n",
    "# See here: https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "# And here: https://www.geeksforgeeks.org/saving-a-machine-learning-model/\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = 'logreg_Yelp_sentiment_classifier.sav'\n",
    "\n",
    "pickle.dump(logreg_clf4b, open(filename, 'wb'))\n",
    "\n",
    "# To load model for future use input:\n",
    "# variable_name = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Possible Future Developments ##\n",
    "* Use stemming or lemmatization\n",
    "* Build a model where features are bigrams/pairs of words instead of individual words\n",
    "* Use a different vectorizer (e.g. tf-idf vectorizer, binary vectorizer)\n",
    "* Add additional features, such as length of review, to help with label prediction\n",
    "* Use different min_df and max_df values in the vectorization\n",
    "* Try doing train/test/split *before* doing vectorization\n",
    "* Use different algorithms to classify the reviews (e.g. Naive Bayes, tree-based such as RandomForest or XG Boost, SVMs, Neural Nets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
